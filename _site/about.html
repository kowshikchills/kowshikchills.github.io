<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta http-equiv="X-UA-Compatible" content="ie=edge">
    <title>About | KD Explains</title>

    <!-- Begin Jekyll SEO tag v2.5.0 -->
<title>About | KD Explains</title>
<meta name="generator" content="Jekyll v3.8.5" />
<meta property="og:title" content="About" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Joint Initiative by Kowshik and Dharani" />
<meta property="og:description" content="Joint Initiative by Kowshik and Dharani" />
<link rel="canonical" href="http://localhost:4000/kowshikchills.github.io/about.html" />
<meta property="og:url" content="http://localhost:4000/kowshikchills.github.io/about.html" />
<meta property="og:site_name" content="KD Explains" />
<script type="application/ld+json">
{"description":"Joint Initiative by Kowshik and Dharani","@type":"WebPage","url":"http://localhost:4000/kowshikchills.github.io/about.html","publisher":{"@type":"Organization","logo":{"@type":"ImageObject","url":"http://localhost:4000/kowshikchills.github.io/assets/images/title_pic.png"}},"headline":"About","@context":"http://schema.org"}</script>
<!-- End Jekyll SEO tag -->


    <link rel="shortcut icon" type="image/x-icon" href="/kowshikchills.github.io/assets/images/favicon.ico">

    <!-- Font Awesome Icons -->
    <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.3.1/css/all.css" integrity="sha384-mzrmE5qonljUremFsqc01SB46JvROS7bZs3IO2EmfFsd15uHvIt+Y8vEf7N7fWAU" crossorigin="anonymous">

    <!-- Google Fonts-->
    <link href="https://fonts.googleapis.com/css?family=Lora:400,400i,700" rel="stylesheet">

    <!-- Bootstrap Modified -->
    <link rel="stylesheet" href="/kowshikchills.github.io/assets/css/main.css">

    <!-- Theme Stylesheet -->
    <link rel="stylesheet" href="/kowshikchills.github.io/assets/css/theme.css">

    <!-- Jquery on header to make sure everything works, the rest  of the scripts in footer for fast loading -->
    <script
    src="https://code.jquery.com/jquery-3.3.1.min.js"
    integrity="sha256-FgpCb/KJQlLNfOu91ta32o/NMZxltwRo8QtmkMRdAu8="
    crossorigin="anonymous"></script>

    <!-- This goes before </head> closing tag, Google Analytics can be placed here --> 


</head>

<body class="">

    <!-- Navbar -->
    <nav id="MagicMenu" class="topnav navbar navbar-expand-lg navbar-light bg-white fixed-top">
    <div class="container">
        <a class="navbar-brand" href="/kowshikchills.github.io/index.html"><strong>KD Explains</strong></a>
        <button class="navbar-toggler collapsed" type="button" data-toggle="collapse" data-target="#navbarColor02" aria-controls="navbarColor02" aria-expanded="false" aria-label="Toggle navigation">
        <span class="navbar-toggler-icon"></span>
        </button>
        <div class="navbar-collapse collapse" id="navbarColor02" style="">
            <ul class="navbar-nav mr-auto d-flex align-items-center">
               <!--  Replace menu links here -->

<li class="nav-item">
<a class="nav-link" href="/kowshikchills.github.io/index.html">Home</a>
</li>
<li class="nav-item">
<a class="nav-link" href="/kowshikchills.github.io/categories.html">Categories</a>
</li>
<li class="nav-item">
<a class="nav-link" href="/kowshikchills.github.io/authors-list.html">Authors</a>
</li>
<li class="nav-item">
<a class="nav-link" href="/kowshikchills.github.io/about.html">About</a>
</li>
<li class="nav-item">
<a class="nav-link" href="/kowshikchills.github.io/contact.html">Write With Us</a>

            </ul>
            <ul class="navbar-nav ml-auto d-flex align-items-center">
                <script src="/kowshikchills.github.io/assets/js/lunr.js"></script>

<script>
$(function() {
    $("#lunrsearchresults").on('click', '#btnx', function () {
        $('#lunrsearchresults').hide( 1000 );
        $( "body" ).removeClass( "modal-open" );
    });
});
    

var documents = [{
    "id": 0,
    "url": "http://localhost:4000/kowshikchills.github.io/404/",
    "title": "",
    "body": " 404 Page not found :(  The requested page could not be found. "
    }, {
    "id": 1,
    "url": "http://localhost:4000/kowshikchills.github.io/about.html",
    "title": "About",
    "body": " Our Mission: Being avid readers of blogs and books, we wanted to come up with blogs that gives readers a great perspective with real world applications and get the gist that sticks to mind. Our intention is to untangle intricate subjects to sow enduring knowledge in the readers mind. Not only technical we also bring in various topics like history, new innovations, finance etc. , We really respect readers interests and whatever we write is solely our perspective. Let‚Äôs all try to improve the knowledge and make the world a better place to learn. Thank You! Kowshik Chilamkurthy: Hailed from IIT Madras. Data science has been my core interest since 5 years with 3 years of industry experience. I currently work in Algorithmic trading. In my experience I understood that with the soaring competition in data science day by day, one has to put an extensive effort to get accustomed with emerging technologies like Reinforcement learning, game theory etc. , These interesting and new topics are mostly in the form of research papers or journals. I found there is a need to bring the subject and its applications in a more approchable way. My quench for knowledge includes learning history and other intriguing topics. This inquisitiveness has led me to comprise them all in a lucid way. Hope you learn a new thing from every blog here. Don‚Äôt forget to share your feedback üòä Dharani Jonnagadda: Graduated from DAIICT, I have joined an analytics firm. I have 2+ experience in data analytics. I believe any data set has a lot of stories to unravel. In my experience when working with real word data sets, not all problems end with deploying a Machine learning model, a complete analysis is expected by any business. So I have sharpend my skills in ML, DL and Visualisation to facilitate the industry need. When I started learning ML and DL, I found existing blogs explain a topic on an over all level and I had to follow some or the other book to get to know the derivations and Math behind it later, which obviously consumed more time. I started to put my learnings in blogs explaining methodically with mathematical equations, derivations and little bit of code that comprehends a topic well. I also keep drawn to learn interesting things happening around the world and if I feel it‚Äôs something one needs to know, I keep adding them here. I hope every blog here gives you a good learning point. "
    }, {
    "id": 2,
    "url": "http://localhost:4000/kowshikchills.github.io/author-dharani-jonnalagadda.html",
    "title": "Dharani Jonnalagadda",
    "body": "                        {{page. title}} Follow:         {{ site. authors. dharani. site }}         {{ site. authors. dharani. bio }}                                   Posts by {{page. title}}:       {% assign posts = site. posts | where: author , dharani  %}      {% for post in posts %}      {% include main-loop-card. html %}      {% endfor %}  "
    }, {
    "id": 3,
    "url": "http://localhost:4000/kowshikchills.github.io/author-kowshik-chilamkurthy.html",
    "title": "Kowshik Chilamkurthy",
    "body": "                        {{page. title}} Follow:         {{ site. authors. kowshik. site }}         {{ site. authors. kowshik. bio }}                                   Posts by {{page. title}}:       {% assign posts = site. posts | where: author , kowshik  %}      {% for post in posts %}      {% include main-loop-card. html %}      {% endfor %}  "
    }, {
    "id": 4,
    "url": "http://localhost:4000/kowshikchills.github.io/authors-list.html",
    "title": "Authors",
    "body": "{{page. title}}:     {% for author in site. authors %}                                         {{ author[1]. name }} :       (View Posts)      {{ author[1]. bio }}                          &nbsp;       &nbsp;                                    {% endfor %}  "
    }, {
    "id": 5,
    "url": "http://localhost:4000/kowshikchills.github.io/categories.html",
    "title": "Categories",
    "body": "          Categories          {% for category in site. categories %}     {{ category[0] }}:           {% assign pages_list = category[1] %}    {% for post in pages_list %}    {% if post. title != null %}     {% if group == null or group == post. group %}           {% include main-loop-card. html %}     {% endif %}    {% endif %}    {% endfor %}    {% assign pages_list = nil %}    {% assign group = nil %}    {% endfor %}                  {% include sidebar-featured. html %}          "
    }, {
    "id": 6,
    "url": "http://localhost:4000/kowshikchills.github.io/contact.html",
    "title": "Contact",
    "body": "  Please send your message to {{site. name}}. We will reply as soon as possible!   "
    }, {
    "id": 7,
    "url": "http://localhost:4000/kowshikchills.github.io/",
    "title": "KD Explains",
    "body": "  {% if page. url ==  /  %}            {% assign latest_post = site. posts[0] %}          &lt;div class= topfirstimage  style= background-image: url({% if latest_post. image contains  ://  %}{{ latest_post. image }}{% else %} {{site. baseurl}}/{{ latest_post. image}}{% endif %}); height: 200px;  background-size: cover;  background-repeat: no-repeat; &gt;&lt;/div&gt;           {{ latest_post. title }}  :       {{ latest_post. excerpt | strip_html | strip_newlines | truncate: 136 }}               In         {% for category in latest_post. categories %}        {{ category }},         {% endfor %}                                {{ latest_post. date | date: '%b %d, %Y' }}                            {%- assign second_post = site. posts[1] -%}                        {% if second_post. image %}                         &lt;img class= w-100  src= {% if second_post. image contains  ://  %}{{ second_post. image }}{% else %}{{ second_post. image | absolute_url }}{% endif %}  alt= {{ second_post. title }} &gt;                        {% endif %}                                    {{ second_post. title }}          :                       In             {% for category in second_post. categories %}            {{ category }},             {% endfor %}                                                      {{ second_post. date | date: '%b %d, %Y' }}                                    {%- assign third_post = site. posts[2] -%}                        {% if third_post. image %}                         &lt;img class= w-100  src= {% if third_post. image contains  ://  %}{{ third_post. image }}{% else %}{{site. baseurl}}/{{ third_post. image }}{% endif %}  alt= {{ third_post. title }} &gt;                        {% endif %}                                    {{ third_post. title }}          :                       In             {% for category in third_post. categories %}            {{ category }},             {% endfor %}                                                      {{ third_post. date | date: '%b %d, %Y' }}                                    {%- assign fourth_post = site. posts[3] -%}                        {% if fourth_post. image %}                        &lt;img class= w-100  src= {% if fourth_post. image contains  ://  %}{{ fourth_post. image }}{% else %}{{site. baseurl}}/{{ fourth_post. image }}{% endif %}  alt= {{ fourth_post. title }} &gt;                        {% endif %}                                    {{ fourth_post. title }}          :                       In             {% for category in fourth_post. categories %}            {{ category }},             {% endfor %}                                                      {{ fourth_post. date | date: '%b %d, %Y' }}                                  {% for post in site. posts %} {% if post. tags contains  sticky  %}                    {{post. title}}                  {{ post. excerpt | strip_html | strip_newlines | truncate: 136 }}                 Read More            	             {% endif %}{% endfor %}  {% endif %}                All Stories:         {% for post in paginator. posts %}          {% include main-loop-card. html %}        {% endfor %}                   {% if paginator. total_pages &gt; 1 %}              {% if paginator. previous_page %}        &laquo; Prev       {% else %}        &laquo;       {% endif %}       {% for page in (1. . paginator. total_pages) %}        {% if page == paginator. page %}        {{ page }}        {% elsif page == 1 %}        {{ page }}        {% else %}        {{ page }}        {% endif %}       {% endfor %}       {% if paginator. next_page %}        Next &raquo;       {% else %}        &raquo;       {% endif %}            {% endif %}                     {% include sidebar-featured. html %}      "
    }, {
    "id": 8,
    "url": "http://localhost:4000/kowshikchills.github.io/Write-With-Us.html",
    "title": "Write With Us",
    "body": "‚Äù{{site. name}}‚Äù takes your privacy seriously. To better protect your privacy we provide this privacy policy notice explaining the way your personal information is collected and used. Collection of Routine Information: This website track basic information about their visitors. This information includes, but is not limited to, IP addresses, browser details, timestamps and referring pages. None of this information can personally identify specific visitor to this website. The information is tracked for routine administration and maintenance purposes. Cookies: Where necessary, this website uses cookies to store information about a visitor‚Äôs preferences and history in order to better serve the visitor and/or present the visitor with customized content. Advertisement and Other Third Parties: Advertising partners and other third parties may use cookies, scripts and/or web beacons to track visitor activities on this website in order to display advertisements and other useful information. Such tracking is done directly by the third parties through their own servers and is subject to their own privacy policies. This website has no access or control over these cookies, scripts and/or web beacons that may be used by third parties. Learn how to opt out of Google‚Äôs cookie usage. Links to Third Party Websites: We have included links on this website for your use and reference. We are not responsible for the privacy policies on these websites. You should be aware that the privacy policies of these websites may differ from our own. Security: The security of your personal information is important to us, but remember that no method of transmission over the Internet, or method of electronic storage, is 100% secure. While we strive to use commercially acceptable means to protect your personal information, we cannot guarantee its absolute security. Changes To This Privacy Policy: This Privacy Policy is effective and will remain in effect except with respect to any changes in its provisions in the future, which will be in effect immediately after being posted on this page. We reserve the right to update or change our Privacy Policy at any time and you should check this Privacy Policy periodically. If we make any material changes to this Privacy Policy, we will notify you either through the email address you have provided us, or by placing a prominent notice on our website. Contact Information: For any questions or concerns regarding the privacy policy, please contact us here. "
    }, {
    "id": 9,
    "url": "http://localhost:4000/kowshikchills.github.io/tags.html",
    "title": "Tags",
    "body": "          Tags          {% for tag in site. tags %}     {{ tag[0] }}:           {% assign pages_list = tag[1] %}    {% for post in pages_list %}    {% if post. title != null %}     {% if group == null or group == post. group %}           {% include main-loop-card. html %}     {% endif %}    {% endif %}    {% endfor %}    {% assign pages_list = nil %}    {% assign group = nil %}    {% endfor %}                  {% include sidebar-featured. html %}          "
    }, {
    "id": 10,
    "url": "http://localhost:4000/kowshikchills.github.io/robots.txt",
    "title": "",
    "body": "      Sitemap: {{ ‚Äúsitemap. xml‚Äù   absolute_url }}   "
    }, {
    "id": 11,
    "url": "http://localhost:4000/kowshikchills.github.io/page2/",
    "title": "KD Explains",
    "body": "  {% if page. url ==  /  %}            {% assign latest_post = site. posts[0] %}          &lt;div class= topfirstimage  style= background-image: url({% if latest_post. image contains  ://  %}{{ latest_post. image }}{% else %} {{site. baseurl}}/{{ latest_post. image}}{% endif %}); height: 200px;  background-size: cover;  background-repeat: no-repeat; &gt;&lt;/div&gt;           {{ latest_post. title }}  :       {{ latest_post. excerpt | strip_html | strip_newlines | truncate: 136 }}               In         {% for category in latest_post. categories %}        {{ category }},         {% endfor %}                                {{ latest_post. date | date: '%b %d, %Y' }}                            {%- assign second_post = site. posts[1] -%}                        {% if second_post. image %}                         &lt;img class= w-100  src= {% if second_post. image contains  ://  %}{{ second_post. image }}{% else %}{{ second_post. image | absolute_url }}{% endif %}  alt= {{ second_post. title }} &gt;                        {% endif %}                                    {{ second_post. title }}          :                       In             {% for category in second_post. categories %}            {{ category }},             {% endfor %}                                                      {{ second_post. date | date: '%b %d, %Y' }}                                    {%- assign third_post = site. posts[2] -%}                        {% if third_post. image %}                         &lt;img class= w-100  src= {% if third_post. image contains  ://  %}{{ third_post. image }}{% else %}{{site. baseurl}}/{{ third_post. image }}{% endif %}  alt= {{ third_post. title }} &gt;                        {% endif %}                                    {{ third_post. title }}          :                       In             {% for category in third_post. categories %}            {{ category }},             {% endfor %}                                                      {{ third_post. date | date: '%b %d, %Y' }}                                    {%- assign fourth_post = site. posts[3] -%}                        {% if fourth_post. image %}                        &lt;img class= w-100  src= {% if fourth_post. image contains  ://  %}{{ fourth_post. image }}{% else %}{{site. baseurl}}/{{ fourth_post. image }}{% endif %}  alt= {{ fourth_post. title }} &gt;                        {% endif %}                                    {{ fourth_post. title }}          :                       In             {% for category in fourth_post. categories %}            {{ category }},             {% endfor %}                                                      {{ fourth_post. date | date: '%b %d, %Y' }}                                  {% for post in site. posts %} {% if post. tags contains  sticky  %}                    {{post. title}}                  {{ post. excerpt | strip_html | strip_newlines | truncate: 136 }}                 Read More            	             {% endif %}{% endfor %}  {% endif %}                All Stories:         {% for post in paginator. posts %}          {% include main-loop-card. html %}        {% endfor %}                   {% if paginator. total_pages &gt; 1 %}              {% if paginator. previous_page %}        &laquo; Prev       {% else %}        &laquo;       {% endif %}       {% for page in (1. . paginator. total_pages) %}        {% if page == paginator. page %}        {{ page }}        {% elsif page == 1 %}        {{ page }}        {% else %}        {{ page }}        {% endif %}       {% endfor %}       {% if paginator. next_page %}        Next &raquo;       {% else %}        &raquo;       {% endif %}            {% endif %}                     {% include sidebar-featured. html %}      "
    }, {
    "id": 12,
    "url": "http://localhost:4000/kowshikchills.github.io/page3/",
    "title": "KD Explains",
    "body": "  {% if page. url ==  /  %}            {% assign latest_post = site. posts[0] %}          &lt;div class= topfirstimage  style= background-image: url({% if latest_post. image contains  ://  %}{{ latest_post. image }}{% else %} {{site. baseurl}}/{{ latest_post. image}}{% endif %}); height: 200px;  background-size: cover;  background-repeat: no-repeat; &gt;&lt;/div&gt;           {{ latest_post. title }}  :       {{ latest_post. excerpt | strip_html | strip_newlines | truncate: 136 }}               In         {% for category in latest_post. categories %}        {{ category }},         {% endfor %}                                {{ latest_post. date | date: '%b %d, %Y' }}                            {%- assign second_post = site. posts[1] -%}                        {% if second_post. image %}                         &lt;img class= w-100  src= {% if second_post. image contains  ://  %}{{ second_post. image }}{% else %}{{ second_post. image | absolute_url }}{% endif %}  alt= {{ second_post. title }} &gt;                        {% endif %}                                    {{ second_post. title }}          :                       In             {% for category in second_post. categories %}            {{ category }},             {% endfor %}                                                      {{ second_post. date | date: '%b %d, %Y' }}                                    {%- assign third_post = site. posts[2] -%}                        {% if third_post. image %}                         &lt;img class= w-100  src= {% if third_post. image contains  ://  %}{{ third_post. image }}{% else %}{{site. baseurl}}/{{ third_post. image }}{% endif %}  alt= {{ third_post. title }} &gt;                        {% endif %}                                    {{ third_post. title }}          :                       In             {% for category in third_post. categories %}            {{ category }},             {% endfor %}                                                      {{ third_post. date | date: '%b %d, %Y' }}                                    {%- assign fourth_post = site. posts[3] -%}                        {% if fourth_post. image %}                        &lt;img class= w-100  src= {% if fourth_post. image contains  ://  %}{{ fourth_post. image }}{% else %}{{site. baseurl}}/{{ fourth_post. image }}{% endif %}  alt= {{ fourth_post. title }} &gt;                        {% endif %}                                    {{ fourth_post. title }}          :                       In             {% for category in fourth_post. categories %}            {{ category }},             {% endfor %}                                                      {{ fourth_post. date | date: '%b %d, %Y' }}                                  {% for post in site. posts %} {% if post. tags contains  sticky  %}                    {{post. title}}                  {{ post. excerpt | strip_html | strip_newlines | truncate: 136 }}                 Read More            	             {% endif %}{% endfor %}  {% endif %}                All Stories:         {% for post in paginator. posts %}          {% include main-loop-card. html %}        {% endfor %}                   {% if paginator. total_pages &gt; 1 %}              {% if paginator. previous_page %}        &laquo; Prev       {% else %}        &laquo;       {% endif %}       {% for page in (1. . paginator. total_pages) %}        {% if page == paginator. page %}        {{ page }}        {% elsif page == 1 %}        {{ page }}        {% else %}        {{ page }}        {% endif %}       {% endfor %}       {% if paginator. next_page %}        Next &raquo;       {% else %}        &raquo;       {% endif %}            {% endif %}                     {% include sidebar-featured. html %}      "
    }, {
    "id": 13,
    "url": "http://localhost:4000/kowshikchills.github.io/KD-Explains-A-Joint-Initiative-by-Kowshik-and-Dharani/",
    "title": "KD Explains: A Joint Initiative by Kowshik and Dharani",
    "body": "2020/08/07 - Being avid readers of blogs and books, we wanted to come up with blogs that gives readers a great perspective with real world applications and get the gist that sticks to mind. Our intention is to untangle intricate subjects to sow enduring knowledge in the readers mind. Not only technical we also bring in various topics like history, new innovations, finance etc. , We really respect readers interests and whatever we write is solely our perspective. Let‚Äôs all try to improve the knowledge and make the world a better place to learn. Thank You! Kowshik Chilamkurthy: Hailed from IIT Madras. Data science has been my core interest since 5 years with 3 years of industry experience. I currently work in Algorithmic trading. In my experience I understood that with the soaring competition in data science day by day, one has to put an extensive effort to get accustomed with emerging technologies like Reinforcement learning, game theory etc. , These interesting and new topics are mostly in the form of research papers or journals. I found there is a need to bring the subject and its applications in a more approchable way. My quench for knowledge includes learning history and other intriguing topics. This inquisitiveness has led me to comprise them all in a lucid way. Hope you learn a new thing from every blog here. Don‚Äôt forget to share your feedback üòä Dharani Jonnagaladda: Graduated from DAIICT, I have joined an analytics firm. I have 2+ experience in data analytics. I believe any data set has a lot of stories to unravel. In my experience when working with real word data sets, not all problems end with deploying a Machine learning model, a complete analysis is expected by any business. So I have sharpend my skills in ML, DL and Visualisation to facilitate the industry need. When I started learning ML and DL, I found existing blogs explain a topic on an over all level and I had to follow some or the other book to get to know the derivations and Math behind it later, which obviously consumed more time. I started to put my learnings in blogs explaining methodically with mathematical equations, derivations and little bit of code that comprehends a topic well. I also keep drawn to learn interesting things happening around the world and if I feel it‚Äôs something one needs to know, I keep adding them here. I hope every blog here gives you a good learning point. "
    }, {
    "id": 14,
    "url": "http://localhost:4000/kowshikchills.github.io/Reinforcement-learning-for-Covid-19-Simulation-and-Optimal-Policy/",
    "title": "Reinforcement learning for Covid- 19- Simulation and Optimal Policy",
    "body": "2020/06/24 - While the ML community is wondering how they could help the war against the COVID-19 pandemic, I decided to use reinforcement learning to tackle this crisis. This investigation yielded some interesting results in finding the set of optimal actions to fight virus spread. 1. Introduction: Imagine you are playing a pandemic control game. Your objective is to control the spreading of the virus with the least economic disruption. You can choose between a multitude of actions like ‚Äòclose all infected residential areas‚Äô, ‚Äòrun tests in infected areas‚Äô, ‚Äòlockdown‚Äô etc. But the immediate question is: how do I quantify economic disruption? Fairly, we can assume that wider the restriction on the movement of the people, the worse the economic health. So our objective is to control the virus spread with the least impediment on the movement of the population. What if an algorithm gives you a trained agent that can take actions on your behalf to achieve the goals you set? Would you not employ such an intelligent agent to curb the virus spread? The subject of reinforcement learning(RL) is around modeling such an intelligent agent.  The most exciting part of this modelling is that we can design an agent that curbs the virus spread in the long term with the least disruption to the economic activity. 2. Reinforcement Learning: Reinforcement Learning is a subfield of machine learning that teaches an agent how to choose an action from its action space. It interacts with an environment, in order to maximize rewards over time. Complex enough? let‚Äôs break this definition for better understanding. Agent: The program you train, with the aim of doing a job you specify. Environment: The world in which the agent performs actions. Action: A move made by the agent, which *causes a change *in the environment. Rewards: The evaluation of an action, which is like feedback. In any RL modelling task, it‚Äôs imperative to define these 4 essential elements. Before we define these elements for our Covid-19 problem, let‚Äôs first try to understand with an example: how agent learn actions in an environment? Agent: Program controlling the movement of limbsEnvironment: The real world simulating gravity and laws of motionAction: Move limb L with Œò degreesReward: Positive when it approaches destination; negative when it falls down.  Agents learn in an interactive environment by trial and error using feedback (Reward) from its own actions and experiences. Agent essentially tries different actions on the environment and learns from the feedback that it gets back. The goal is to find a suitable action policy that would maximize the total cumulative reward of the agent. 3. Pandemic Control Problem: Now let‚Äôs define these 4 essential elements for our pandemic control problem:Agent: A Program controlling the movement of the citizens through different actions. Environment: The virtual city where the virus is spreading. By restricting the citizen‚Äôs movement, spread dynamics can be altered. Action: Control the movement of the citizens. Rewards: minimise infected from virus spread (pandemic control) +minimise people quarantined( least economic disruption)+ minimise people dead Now we need to code-up and discuss each element of this optimal control problem. let‚Äôs start with pandemic simulation environment. 4. Pandemic Simulation Environment:  Model the whole pandemic transmission dynamics as interactions between different components. Though there are a large number of pandemic simulation models, I decided to use my own simulation model drawing inspiration from the network model. I choose not to use the standard model because of the following reasons:    In existing simulation models, the transmission dynamics of the virus does not react to the actions taken by the decision maker/agent. (eg. How would closing public transport impact virus spreading).     Existing transmission models doesn‚Äôt output a comprehensive observation on the state of the city.  In order to prepare such an environment that overcomes above-mentioned shortcomings, I decided to break the whole pandemic transmission dynamics into interactions between different components. Let‚Äôs discuss these components and their respective assumptions of pandemic simulation environment. We will classify these components into Demographic Components, Transmission Dynamics, Contagious Components. Demographic Components: These are basic components of the simulation model on which the whole transmission dynamics are built. We will create a closed city where we intend to simulate the virus spread. There are assumptions considered about this city, such that the simulation process is less computationally expensive and also close to reality.  Transmission Dynamics: These transmission dynamics decide the extent and intensity of the virus spread. We can simulate any pandemic using these transmission dynamics.  As you can clearly visualize: Infected citizen makes the daily trip and he/she infects other citizens who came in contact with him with the probability of transmission at each unit. We essentially need to define how many citizens come in contact with the infected and what is the probability of transmission at each unit.  Contagious Components and Simulation Results: These contagious components help us build an environment. For a decision maker to take actions to curb the virus spread, he must understand the state of the infected city( eg. number of citizens infected, number of residential areas infected, number of citizens quarantined ,etc). These components facilitate the logging of infected/interaction information in a structured manner. We use the compartment model for simulation. Let‚Äôs simulate a simple compartment model with infinite hospital capacity. We will randomly infect 3 citizens and simulate a pandemic following the above transmission dynamics.  Contagious Compartment: All those active citizens who are infected and contagious are included in this list Recognized Compartment: All those infected who came to the governments notice. Hospitalized Compartment: All those infected citizens recognized by the government will be put in the hospital. Once the infected citizen enter this list, he will be removed from the Contagious Compartment. Hospital Infrastructure Capacity: The capacity of the hospital is limited. Once the capacity reaches, further infected citizens cannot enter the Hospitalized Compartment. This is a very important variable in our simulation, which you will see in plot 6. Death: Infected will be dead as the days progress with the probability proportional to his age Let‚Äôs look at the simulation results for the pandemic in a city of 1L population and with infinite hospital Infrastructure Capacity and limited(500) capacity. Also, we need to compare it with standard epidemiological models.   This is a simple epidemiological model. The ‚Äú contagious line‚Äù in my simulation model(Plot 6) is closer to the ‚Äúinfected line‚Äù in SIR model(Plot 7). This clearly implies that the pandemic simulation is accurate. 5. Actions: The need for creating a new environment for the pandemic problem is essentially because we ideally want our pandemic simulation environment to react to the actions taken by the decision maker. So defining action space is as important as defining the environment. So by defining wide action space, we are enriching the decision maker‚Äôs choices to curb the virus spread. The virus spread can be effectively curbed by:  Restricting the movement of the citizens  Conducting the tests on probable citizens, so that infected citizens come to the government‚Äôs note before the symptoms kick-in. You will now clearly see why I introduced the concept of transmission dynamics. By restricting the movement of the citizens, they are not susceptible to infection anymore. This condition can be easily embedded into the simulation and the dynamics of the virus spread change accordingly.  These are the actions defined for the decision maker. For example, if the decision maker chooses action: 8 (lockdown): then all the citizens in the city cannot move. The idea behind defining this action space is that we want to find the most optimal action policy of restricting citizen‚Äôs movement. We can design more actions, but for now, we limit to this action space. 6. Agent and Rewards: Out of 4 essential elements of Reinforcement Learning, we discussed 1. Environment 2. Actions for our pandemic control problem. Let‚Äôs discuss agent and reward in this section. An agent is essentially a program you train, with the aim of doing a job you specify. But how do we specify the job? *How can an agent understand your(decision maker) objectives? *The answer is through reward. The agent always try to find out the action policy that maximize the cumulative sum of rewards. So if we can tie the goals of the pandemic control problem with the reward function, we can train an agent which achieves goals for us. Let‚Äôs reiterate our objective: To control the virus spread with the least impediment on the movement of the population( least economic disruption). So we need to minimize:  Number of people Infected (ùú®ùíä) Number of people quarantined(ùú®ùíí) Number of people died because of infection(ùú®ùíÖ)We don‚Äôt essentially give equal weights to each number. For example, governments don‚Äôt let the economy remain healthy at the cost of citizens. One thing must be kept in mind when deciding ùë§ùíä, ùë§ùíí, ùë§ùíÖ. Apart from their ethical importance, these weights are just numbers. We need to choose them judicially such that the agent actually learns to achieve the objectives we set. In section 2( RL), we learnt how agent trains. Let‚Äôs try to understand the training process in the pandemic control problem. I used the DQN model to train the agent. In this DQN model, the agent tries random actions in the beginning (exploratory) to learn optimal action policy. An interesting concept in this model is discounted sum of rewards: agent gives lesser importance to the immediate rewards and strives to achieve long terms goals. I will briefly explain this RL model: Q-learning learns the action-value function Q(s, a): how good to take an action at a particular observation. Let‚Äôs try to understand Q value: Consider the pandemic simulation environment, for a given observation:{infected, hospitalized, dead, exposed, infected houses, average age of infected} Agent will learns Q value (expected rewards) for each action ( Total 16 actions). The agent chooses the action with the highest Q value. We will limit the discussion on RL modelling techniques and jump into the results and Interpretation. 7. Results and Interpretation: Now we reach the end and also the most interesting part of this blog. So let‚Äôs create a pandemic simulation in a city of size 1 Lakh. We will let the DQN agent take actions from its action space A (plot 8) to maximize the reward R( Equation 1).   8. Summary: This modelling and simulation can be extended to cities of different sizes. The actions taken by the agent are more intuitive as the agent understands/learns the pandemic simulation environment better. For example, agents choose to do a lot of tests in infected areas at the beginning of the spread. More action spaces and better reward function makes this whole RL modelling even closer to reality. As I mentioned in the beginning, the intention behind writing this blog is to explore the possibility of collaboration and help the war against the corona virus spread. If anyone believes that they can contribute to this RL project, please feel free to mail me kowshikchilamkurthy@gmail. com. Also, I would love to take suggestions from you for better simulation and better RL modelling. references:1. https://en. wikipedia. org/wiki/Compartmental_models_in_epidemiology#The_SIR_model  https://blogs. mathworks. com/headlines/2019/05/16/robot-quickly-teaches-itself-to-walk-using-reinforcement-learning/ H. S. Rodrigues, M. T. T. Monteiro, and D. F. M. Torres, ‚ÄúDynamics of dengue epidemics when using optimal control,‚Äù Mathematical and Computer Modelling, vol. 52, no. 9‚Äì10, pp. 1667‚Äì1673, 2010. "
    }, {
    "id": 15,
    "url": "http://localhost:4000/kowshikchills.github.io/Improve-Survival-Time-in-PUBG-A-Cox-Statistical-Approach/",
    "title": "Improve Survival Time in PUBG- A Cox Statistical Approach",
    "body": "2020/06/15 - A Real World Application of Cox Proportional-Hazards Model: Introduction: PUBG needs no introduction. It is one of the popular and the most played games right now. Players fight to death until one remains, so it is a survival game. There are pure statistical models to analyse the survival times. Using **PUBG data, **we will try to use one such survival models to understand how different strategies can improve the player‚Äôs survival rates. This blog is written for tech, non-tech readers and most importantly PUBG players. I will also include my python implementation for the benefit of tech readers. This can be seen as a sequel to my blog: [The Cox Proportional-Hazards Model](https://medium. com/point-processes/the-cox-proportional-hazards-model-da61616e2e50) and must read for those who are interested in understanding the mathematical background and python implementation of magical *Cox Proportional-Hazards Model. We will use the data published in Kaggle datasets where there are over 720,000 PUBG matches. The data log was extracted from pubg. op. gg, a game tracker website. We will use this data log to understand different modes of game strategies using statistical models and try to figure out the method to evaluate the strategies. A Quick Recap of Cox Proportional-Hazards Model: Cox proportional-hazards model is developed by Cox and published in his work[1] in 1972. It is the most commonly used regression model for survival data. The most interesting aspect of this survival modeling is it‚Äôs ability to examine the relationship between survival time and predictors. For example, if we are examining the survival of patients then the predictors can be age, blood pressure, gender, smoking habits, etc. These predictors are usually termed as covariates. Note: It must not be confused with linear regression, the assumptions might be linear in both regression and survival analysis but the underlying concepts are different. Methods we employ for parameter estimations of regression model and survival model are very different from each other.     Hazard function Œª(t): gives the instantaneous risk of demise at time t     Z: Vector of features/covariates     Œªo(t) is called the baseline hazard function  PUBG Problem Setup &amp; Data Engineering: Let‚Äôs have a look at the raw data before we define the problem setup. import pandas as pddf = pd. read_csv(‚Äòagg_match_stats_0. csv‚Äô)df. head() Feature Description: player_size: Team Size, player_dist_ride: Distance covered using vehicle by the player , player_dist_walk: Distance walked by the player, player_kills: Number of kills by the player, players_survive_time: time survived by the player Problem Setup: Players in PUBG can choose different strategies to maximise the survival time. We define strategy as a combination of one or more player‚Äôs decisions. Strategies can be something like:  Travel extensively with least confrontation with enemies, Use a motorised vehicle most of the time, Only Walk, but confront with enemies more often, or Even something funnier like: Play only afternoons over the weekendsüòù. There can be 1000‚Äôs of such strategies, some of them might look trivial other might not. Our goal is to find a way to evaluate these strategies based on their survival rates. Apart from raw data provided, we also need to engineer these columns to derive meaningful features(player decisions). Data Engineering: In this section, we will briefly discuss the features needed to be extract from the raw data available to use. These features can be simply seen as the decisions taken by the player. Let‚Äôs list them and also look at the distributions for some of these features.  import numpy as npimport pandas as pdimport seaborn as snsimport matplotlib. pyplot as pltfrom scipy import statsdf = pd. read_csv('agg_match_stats_0. csv')df_features = create_features(df) #func is defined at end of blogdf_features. head() Now that we extracted the features, lets jump into the implementation of cox proportional-hazards model. Survival Analysis: This is the most interesting section: the implementation of cox model in python with the help of lifelines package. It is very important to know about the impact of features on the survival rates. This would help us in predicting the survival rates of a PUBG player, if we know the associated feature values. The Cox model assumes that each features have an impact on the survival rates. One of the basic assumptions of the CPH model is that the features are not collinear. We can either solve the issue of multi-collinearity before fitting the cox model or we can apply a penalty to the size of the coefficients during regression. Using a penalty improves stability of the estimates and controls for high correlation between covariates. from lifelines import CoxPHFittercph = CoxPHFitter(penalizer=0. 1)cph. fit(df, duration_col='player_survive_time', event_col='dead')cph. plot() Coefficients of the features which indicate the measure of the impact on the survival rates of the PUBG player. Interpreting the summary:    Hazard ratio (HR) given by exp(coef), where coef is the weight corresposing to the feature. If exp(coef) = 1 for a feature, then it has no effect. If exp(coef) &gt; 1, decreases the hazard: improves the survival.     weekend_indi( that is whether player player over weekend or weekday ) doesn‚Äôt play any significant role in predicting his survival risk, whereas player_kills ( number of kills by player) variable plays significant role in predicting survival risk .     game size *feature with exp(coef) = 1. 0 has no effect on the survival rates: so it implies that the survival of the player does not depend on the *game size.     *%player_dist_ride *feature with exp(coef) = 1. 73 (&gt;1) this is good for survival. So preferring the vehicle instead of walking increases the survival rates.  For better understanding of the math behind above deductions, please refer to my earlier blog: *The Cox Proportional-Hazards Model. *In the next section, we will also see how different features play together to decide the survival rates of the PUBG player. Results &amp; Visualisation: The best way we understand impact of each features/decision is that we plot the survival curves for single feature/decision i. e. , we keep all other player‚Äôs decisions unchanged. we use[plot_covariate_groups()](https://lifelines. readthedocs. io/en/latest/fitters/regression/CoxPHFitter. html#lifelines. fitters. coxph_fitter. CoxPHFitter. plot_covariate_groups) method and give it the feature of interest, and the values to display. Also we will look at the survival rates for different strategies ( combination of decisions) In this section we will discuss  Survival profiles of Decisions Survival profiles of StrategiesSurvival profiles of Decisions: One quick way to interpret these different survival curves is that the decision with corresponding survival curve leaning to the right yields more survival probability than that of its left. Let‚Äôs try to understand this with an example.   Interpreting plot 3    It clearly implies that the survival time of PUBG player increases if he choose to walk instead of taking a vehicle     More the distance he traverses, better his survival rates (which is intuitive)  Survival profiles of Strategies: Let‚Äôs quickly see the survival profile for different strategies. For example, consider these four strategies:  Use vehicles extensively, travel longer distances and kill often Only walk, travel smaller distances and don‚Äôt confront with enemies often Do team work, use vehicle less often and travel large distances Select a match with small number of players and kill extensively The values for decisions are fixed as per the above 4 strategies Even in the real world survival situations, moving and confronting with the enemies is better than staying idle. We can handcraft 1000‚Äôs of such strategies and compare their survival behaviours. We can even understand and approximate the human behaviour during survival situations by applying these kind of statistical model on the data extracted from the survival games. Summary: We looked at a real world application of Cox proportional-hazards model. We understood how different strategies impact the survival times of the PUBG player. Out of those strategies we analysed, we found strategy of ‚Äúusing vehicles extensively, travelling longer distances and killing often‚Äù statistically promising the longest survival of a PUBG player in a match. There are also neural network variants of Cox proportional-hazards model, we will look at such neural variant of Cox PH model in my next blog in this series. Thanks for your time :) Here is the full code for reference: #import all the dependencies import pandas as pdimport numpy as npimport seaborn as snsimport matplotlib. pyplot as pltfrom scipy import stats #import data setdf = pd. read_csv(‚Äòagg_match_stats_0. csv‚Äô) #Create new features def create_features(df)  df['player_survive_time'] = df['player_survive_time']/60  df['date'] = [i. split('+')[0] for i in df['date']. values]  df['date'] = pd. to_datetime(df['date'],format='%Y-%m-%dT%H:%M:%S')  df['dayofweek_num']=df['date']. dt. dayofweek   df['dayofweek_name']=df['date']. dt. weekday_name  df['Hour'] = df['date']. dt. hour   df['weekend_indi'] = 0      df. loc[df['dayofweek_num']. isin([5, 6]), 'weekend_indi'] = 1  df['time_of_day'] = 0      df. loc[df['Hour']. isin([24,1,2,3,4,5,6]), 'time_of_day'] =  LateNight   df. loc[df['Hour']. isin([7,8,9,10,11]), 'time_of_day'] =  Morn   df. loc[df['Hour']. isin([12,13,14,15,16,17,18,19]), 'time_of_day'] =  Evening   df. loc[df['Hour']. isin([20,21,22,23]), 'time_of_day'] =  Night   df['%player_dist_ride'] = df['player_dist_ride']/(df['player_dist_ride']+df['player_dist_walk'])  df['%player_dist_walk'] = df['player_dist_walk']/(df['player_dist_ride']+df['player_dist_walk'])  df['total distance'] = df['player_dist_ride']+df['player_dist_walk']  df['only_walk'] = 0      df. loc[df['%player_dist_walk']. isin([1]), 'only_walk'] = 1  for i in ['date','team_id','team_placement','player_name','player_dmg','player_dbno','player_dist_ride',       'player_dist_walk','Hour','dayofweek_num','dayofweek_name','match_id','match_mode']:    del df[i]  one_hot = pd. get_dummies(df['time_of_day'])  df = df. drop('time_of_day',axis = 1)  df = df. join(one_hot)  del df[0]  df['dead'] = 1  df = df. dropna()  return(df)df_sampled = create_features(df)from lifelines import CoxPHFittercph = CoxPHFitter(penalizer=0. 1)cph. fit(df_sampled, duration_col='player_survive_time', event_col='dead')'''Individual features '''cph. plot_covariate_groups('only_walk', [0,1], cmap='coolwarm')plt. xlabel('time (minutes)')plt. ylabel('Survival Curve')'''strategies'''df_strategy = pd. DataFrame()df_strategy['game_size'] = [60,60,60,30]df_strategy['party_size'] = [2,2,2,1]df_strategy['player_assists'] = [1,0,4,1]df_strategy['player_kills'] = [6,1,2,5]df_strategy['weekend_indi'] = [0,0,0,0,]df_strategy['%player_dist_ride'] = [0. 8,0,0. 2,0. 5]df_strategy['%player_dist_walk'] = [0. 2,1,0. 8,0. 5]df_strategy['total distance'] = [9000,3000,7000,4000]df_strategy['only_walk'] = [0,1,0,0]df_strategy['Evening'] = [1,0,1,1]df_strategy['LateNight'] = [0,1,0,1]df_strategy['Morn'] = [0,0,1,0]df_strategy['Night'] = [0,0,0,0]df_strategy. index = ['Strategy 1','Strategy 2','Strategy 3','Strategy 4']cph. predict_survival_function(df_strategy). plot()plt. xlabel('time (minutes)')plt. ylabel('Survival Curve')"
    }, {
    "id": 16,
    "url": "http://localhost:4000/kowshikchills.github.io/Fundamentals-of-Reinforcement-Learning/",
    "title": "Fundamentals of Reinforcement Learning",
    "body": "2020/06/14 - Learning decisions that makes the difference: Introduction: Designing machine that learn to do a job by itself is one of the most researched topic than any other in recent times due to various reasons like increased computational power, availability of resources to experiment etc. , This lead to uncover significant innovations that made life simpler. If you just have data then an algorithm will provide insights or you train a model and it recognizes your face and many other use cases that we see around us which are built using Machine Learning and Deep Learning. Reinforcement Learning is burgeoning by gaining a lot of attention due to its proven capability in making sequential decision process. Concepts of RL: Reinforcement Learning basically consists of an agent(decision maker) that tries to learn from a state in a given surrounding that it interacts called environment and changes its state because of some action taken as per the feedback provided by the environment during the episode. This feedback is numerical(positive, negative or zero) and is called a reward. The optimal behavior of an agent is to learn such that it always gets good feedback i. e. , maximize the reward by taking suitable actions. So in RL we are providing the scenario to the agent and it can figure out itself or discover how to take decisions in the most applaudable way.  Lets understand the terms used in RL using the very well known PUBG game as a simple example:    The player in PUBG is an agent here and battleground is his environment     A complete game played is an episode and walking, running etc are states ‚Äî helps to pick actions     The agent has number of actions to take like moving left, right, front and back, run, fire, kill, bend, jump, change gun etc. ,     The reward the agent gets here is positive if he kills and zero if he survives with the help of his teammates till the end or negative if he gets shot by other player.  In order to win the game we have to maximize the reward by taking suitable actions at each time frame. Simply we start from a state, take an action and change to another state and get a reward for that action and repeat the process to learn more about the environment setting. There are few challenges RL has before us. Some of them are:    Trade-offs: As we understood that agent has to optimize the rewards and also it has to interact continuously with the environment i. e. , it needs to explore a lot. This leads to a trade off between exploration and exploitation. It has to choose whether it should keep exploring new states which might result in lower reward or take the path that has already seen and got quite a good set of rewards.     Generalization: Can the agent understand or learn if the actions are good/bad in its previously unseen states.     Delayed consequences: We also need to understand if an agent gives a high reward in a current state, it is because of just this state or a series of decisions that it has taken to reach this state?  There are few key concepts that are applicable in RL. A good knowledge on these will let us understand the formulation of agent‚Äôs decision process and model of the environment.  Markov Property: If an agent changes from one state to other it is called a transition and the probability at which it makes the transition is called transition probability. Generally if we have all the probabilities of an agent going from one state to the other, then it is represented in a transition matrix. Markov property says ‚ÄúFuture is independent of past given present‚Äù. The equation below depicts the probability of transitioning to next state St+1 in time t is only dependent on the current state St and the action taken At and is independent of the history This is the transition probability matrix which has all the transition probabilities of all states. For example, P12 depicts the probability of transitioning from state 1 to state 2 and so on. When we traverse through a set of states in the environment which follows Markov Property, then it is called a Markov chain. They might include random set of states in the Markov chain that also have transition probabilities and we can compute the optimal chain that results in high reward.    In RL, we are more concerned about optimizing the total reward that an agent receives from the environment rather than the immediate reward it gets by transitioning from one state to the other. So we measure the optimality using a function called return(Gt) which is sum of rewards the agent received from time t (Eq 1).     In many games like Atari, alpha go, chess or PUBG we know the game is going to terminate after certain time steps. If this is the setting then it is called an episodic task. If we start another game then we are initiating a new episode so episodes are independent of each other. There can also be problems where it is not going to have an end like certain robots that are used for personal assistance which do not terminate until an external signal from environment puts it in termination state. These are called continuous tasks.  In episodic tasks we can calculate the returns which is a total sum of its rewards till the termination but in continuous tasks as there is no termination, the rewards add up to infinity while calculating the returns. So we introduce a discount factor gamma(…§) to calculate the returns in continuous tasks by discounting it. It has it values from 0‚Äì1. It plays a crucial role in determining if we give importance to immediate rewards or future rewards. If …§=0 then the attention lies on immediate rewards and if …§=1 then on future rewards.  If we have a problem statement which says you get a reward of 1 for performing certain action for next k time steps with a discount factor 0. 8 and 0. 2 then the returns would be  We see that Gt with …§=0. 8 is yielding a good return even in future but for …§=0. 2 **the returns are high only in the immediate time step and in future it is almost tending to zero. So based on the problem statement we can set **…§ **that facilitates to decide the importance of either **immediate or **future **rewards. We now know an agent changes its states and gets a reward for that transition. Lets check an example to understand in detail: Consider a situation where a student is an agent and he have four states Home, School, Class, Movie and a discount factor of 0. 8. The probabilities of transitioning from one state to other is shown in blue boxes and rewards are shown in brown boxes. Agent might have many episodes i. e. , a sequence of traversing through states. For example,  Home -&gt; School -&gt; Class -&gt; Home -&gt; Terminate ‚Äî Lets calculate the returns of state Home. G = 3 + 50. 8 + 50. 8*0. 8 = 10. 2 Home -&gt; School -&gt; School -&gt; Movie -&gt; Home -&gt; Terminate ‚Äî Returns in this episode is G = 3 + 20. 8 + (-10)0. 80. 8 + 30. 80. 80. 8 = ‚Äî 0. 264It is clear that episode 1 results in high return than episode 2. So it would be feasible to follow it. Returns is a significant concept as it can decide the agents optimal path. Markov Reward Process(MRP):: MRP is a Markov process setting which specifies a reward function and a discount factor …§. It is formally represented using the tuple (S, P, R, Œ≥) which are listed below:    S : A finite state space.                 P : A transition probability model that specifies P(s`     s).                          R : A reward function that maps states to rewards (real numbers) R(s) = E[ri     si = s] , ‚àÄ i = 0, 1, . . . . (E here is expected value and i is every time step)           Œ≥: Discount factor ‚Äî lies between 0 and 1. State Value Function: State Value function Vt(s) is the expected sum of returns starting from state s at a time t.  Simply put, value function denotes how good it is for an agent to be in that particular state. Transitioning between states that result in a high reward during episodes is an optimal MRP. We have different methods to evaluate V(s). They are    Monte-Carlo Simulation Method: In MRP, for each episode returns are calculated and are averaged. So State Value function is calculated as Vt(s)=Sum(Gt)/Number of episodes.     Analytic Solution: If the number of time steps are infinite, then we cannot calculate sum or average of returns. In this process, we define Œ≥&lt;1 and State value function is equal to sum of Immediate Reward(reward obtained for transitioning from State s to s‚Äô) and discounted sum of future rewards. The equation can be represented in Matrix form as V=R + Œ≥PV. Rearranging gives V by multiplying inverse matrix of (I ‚àí Œ≥P) with R.    Iterative Solution: In this method we calculate Value Function at time step t by iterating through its previous value functions at time t-1,t-2 etc. , We will look into this in depth soon. Markov Decision Process (MDP):: An MDP is simply an MRP but with the specifications of a set of actions that an agent can take from each state. It is represented a tuple (S, A, P, R, Œ≥) which denotes:    S : A finite state space.     A : A finite set of actions which are available from each state s.                 P : A transition probability model that specifies P(s`     s).                          R : A reward function that maps states to rewards (real numbers) R(s) = E[ri     Si = s, Ai=a] , ‚àÄ i = 0, 1, . . . . (state s, action taken a, E here is expected value and i is every time step)           Œ≥ : Discount factor ‚Äî lies between 0 and 1. An episode of a MDP is thus represented as (s0, a0, r0, s1, a1, r1, s2, a2, r2, . . . ).       In MRP, we have transition probability of going from one state to the other. In MDP, the notation is slightly changed. We define transition probability with respect to action as well P(Si+1   Si , ai). An example of robot transitioning between different states also depends on the action it takes if its moving forward, left, right or halted. All the other notations of returns(Gt), discount factor(Œ≥) are exactly the same as referred in MRP.    Policy and Q-Function:: Suppose there is a robot which is currently at a state S1, it can take actions left or right with probabilities al and ar respectively for taking left or right which lands in two different states S2 and S3. Value function and rewards of that state are also mentioned and discount factor = 0. 8 Let us calculate the value function of S1:V1= al(R+Œ≥V2) + ar(R+Œ≥V3) = al(2+ 0. 810) + ar(1+ 0. 815) = al10 + ar13. 5 If a1 = 0. 2 and ar = 0. 8, then V1 = 12. 8 and if a1 = 0. 8 and ar = 0. 2 then V1 = 10. 7 Clearly giving more probability to take the action ar will give a better result in terms of value of the state S1.  To evaluate how good it is to transition to a state is we use value function but to determine how good is it to take an action ‚Äòa‚Äô from this state? This is where the concept of Policy sets in. Policy œÄ::       Policy is a decision making mechanism in MDP that maps states to actions. For a policy œÄ, if at time t the agent is in state s, it will choose an action a with probability given by œÄ(a   s).    Given a policy œÄ how can we evaluate if its good or not? The intuition is same as in MRP, we calculate the expected rewards. We can define as below: State Value Function (how good is it to transition to a state): Value function at a given state s of an agent, is the expected returns obtained by following a policy œÄ and reaching to a next state, until we reach a terminal state.  State-Action Value Function or Q ‚Äî Function:: The state-action value function for a state s and action a is defined as the expected return starting from the state St = s at time t and taking the action At = a, and then subsequently following the policy œÄ. It is written mathematically as So this tells us the value of performing an action a in state s following policy œÄ. These are just building blocks of MDP in RL. There are lot more concepts like Bell Man Backup Operator, Finding Optimal Value functions and Policies and Dynamic Programming etc. , Lets check them out in my next blog. Hope this article has pushed your understanding of RL to some level up. Thanks for your time ! "
    }, {
    "id": 17,
    "url": "http://localhost:4000/kowshikchills.github.io/Strategies-for-Customer-Retention-A-Cox-Survival-Model-Treatment/",
    "title": "Strategies for Customer Retention: A Cox Survival Model Treatment",
    "body": "2020/06/12 - Techniques to devise personalized strategies using statistical models: Introduction: Customer churn occurs when customers or subscribers discontinue their association with a company or service. There are many Machine Learning models to predict if a customer is going to churn or not. The problem doesn‚Äôt stop there, business has to deploy certain strategies to retain the customers who are at the verge of churn because it‚Äôs five times cheaper to retain an existing customer than to acquire a new one. Statistical models can be used to derive and evaluate personalized strategies which is a core challenge in CPG companies. We call the event of customer churn as failure and survival time is the time taken for such failure/churn. Survival models are statistical techniques used to estimate the time span taken for an event to occur. Cox Proportional-Hazards model is a popular statistical model for survival analysis. Using churn data set from Kaggle, we will try to use this model to understand customer behavior and compare different strategies that can improve customer retention. Please refer to this blog to understand Mathematical Equations and reason behind using this. Road map to enhance customer retention rate::    Customer‚Äôs characteristics and demographics play a pivotal role in understanding retention behavior. Our goal is to understand the relation between these features and survival time(time taken to churn). We can plot survival/retention curves that are specific to a customer to gain valuable insights.     Devise personalized strategies(for example, increase incentives/offers)for high-valued customers for different survival risk segments during the time. Our goal is to evaluate and compare how they improve the survival/retention behavior in a customer.  A Quick Recap of Cox Proportional-Hazards Model: Cox proportional-hazards model is developed by Cox and published in his work[1] in 1972. The most interesting aspect of this survival modeling is its ability to examine the relationship between survival time and predictors. For example, if we are examining the survival of patients then the predictors can be age, blood pressure, gender, smoking habits, etc. These predictors are usually termed as covariates.     Hazard function Œª(t): gives the instantaneous risk of demise at time t     Z: Vector of features/covariates     Œªo(t) is called the baseline hazard function: Describes how the risk of event changes over time. It is underlying hazard with all covariates equal to 0.  Model Implementation on churn data set:: Problem Setup &amp; Data Engineering: I have taken telecom customer churn data set. Lets check the data structure: import pandas as pddf = pd. read_csv(‚ÄòData_Churn_Telecom_Cox. csv‚Äô)df. head()  These features gives the customer‚Äôs demographics and characteristics / behaviour. There are 96 such features. ‚ÄúTotal number of months in service‚Äù column gives us the survival/retention time of a customer. ‚Äúchurn‚Äù column gives whether customer churns or not i. e. , event occurrence. Data Engineering: Listed down are the feature engineering steps and we also look at the distributions for some of these features. Code for each feature engineering step is published at end of the blog.  A fairly simple assumption is proportional hazards, which is crucial in Cox regression that is included in its name(the Cox proportional hazards model). It means that the ratio of the hazards for any two individuals is constant over time. We drop those features if they don‚Äôt pass this condition. Survival Analysis: Here comes the most interesting section: the implementation of cox model in python with the help of lifelines package. Understanding the impact of features on survival rates helps us in predicting the retention of a customer profile. The Cox model assumes that each feature have an impact on the survival/retention rate. One of the basic assumptions of the CPH model is that the features are not collinear. We can either solve the issue of multi-collinearity before fitting the Cox model or we can apply a penalty to the size of the coefficients during regression. Using a penalty improves stability of the estimates and controls high correlation between covariates. from lifelines import CoxPHFittrcph = CoxPHFitter(penalizer=0. 1)cph. fit(df_f, duration_col=‚ÄôTotal number of months in service‚Äô, event_col=‚Äôchurn‚Äô)cph. summary #output2cph. plot  #output3 Interpreting the summary:    Hazard ratio (HR) given by exp(coef), where coef is the weight corresponding to the feature. If exp(coef) = 1 for a feature, then it has no effect. If exp(coef) &gt; 1, decreases the hazard: improves the survival.     number of unique subscribers in the household has HR = 1. 35 which improves survival/retention. mean number of unanswered data calls has HR = 0. 16 implies it has bad effect on survival rate.  Results &amp; Visualization: The best way to understand impact of each features/decision is to plot the survival curves for single feature/decision by keeping all other customers characteristics/demographics unchanged. we use [plot_covariate_groups()](https://lifelines. readthedocs. io/en/latest/fitters/regression/CoxPHFitter. html#lifelines. fitters. coxph_fitter. CoxPHFitter. plot_covariate_groups) method and pass the arguments ‚Äî feature of interest and the values to display. Survival Profiles of Features: One quick way to interpret these different survival curves is that the decision with corresponding survival curve leaning to the right yields more survival/retention probability than that to its left. A plot below for Average monthly revenue over the life of the customer =400 has more survival probability as it is to the right compared to that of 10 which is to its left.  Personalized strategies for Customers: We can plot the survival profiles for each customer and analyse the reasons for low survival/retention rates by looking at customer features. From above discussions, we already know what actions can improve the survival rates of the customer.  We can plot the survival profiles of each customer. For time being let‚Äôs consider customer with ID 1032424 and compare the two strategies as shown below: We can clearly see that strategy 1( issue more models to the customer : 1032424) has comparatively longer survival time than strategy 2(Reduce revenue generated from the customer by giving offer). Similarly, we can analyse each and every customer and design proactive strategies to ensure highest retention statistically. We can also compare different strategies developed by business intelligence teams and deploy based on the effectiveness of a strategy to retain customers. Summary: We segmented customer behavior by grouping them based on average monthly revenue brackets, number of models issued etc. , and Cox proportional-hazards model enabled us to derive personalized strategies to reduce the churn rate. Not only deriving personalized strategies, we learnt to compare them. As an example, for one customer we saw in above section, statistically proves that issuing more models will have a better impact than providing incentives/offers for a customer to stick to the company for a longer time span. This is an outstanding way to meet the landscape of customer expectations and increase customer engagement with the company. Thanks for your time :) Full code for reference: import pandas as pddf = pd. read_csv('Data_Churn_Telecom_Cox. csv')with open('cols. txt') as f:  lines = f. readlines()cols_names = {}for i in range(len(lines)):  for j in df. columns:    if j == lines[i][:-1]:      cols_names[j] = lines[i+1][:-1]cols_names['Customer_ID'] = 'Customer_ID'df. columns =  cols_names. values()df['churn'] = df['Instance of churn between 31-60 days after observation date']del df['Instance of churn between 31-60 days after observation date']df = df. dropna()del df['N']df. set_index('Customer_ID', inplace=True)'''Drop categorical features with unique values &gt;2'''import numpy as npdf_str = df. loc[:, df. dtypes == object]for i in df_str. columns:  if len(np. unique(df_str[i]. values)) &gt;2:    del df[i]'''One hot encoding'''df_str = df. loc[:, df. dtypes == object]for i in df_str. columns:  one_hot = pd. get_dummies(df[i])  one_hot. columns = [i +'_'+j for j in one_hot. columns]  df = df. drop(i,axis = 1)  df = df. join(one_hot)survival_time = df['Total number of months in service']. valuesdel df['Total number of months in service']churn = df['churn']. valuesdel df['churn']'''Drop correlated features'''corr_matrix = df. corr(). abs()upper = corr_matrix. where(np. triu(np. ones(corr_matrix. shape), k=1). astype(np. bool))to_drop = [column for column in upper. columns if any(upper[column] &gt; 0. 98)]df. drop(to_drop, axis=1, inplace=True)df = df[list(df. columns[:69])+['Credit card indicator_N']]df['Total number of months in service'] = survival_timedf['churn'] = churndf = df[df['churn'] == 1]'''Select valuable features'''df_sampled = df. sample(n=1000)from lifelines import CoxPHFittercph = CoxPHFitter(penalizer=0. 01)cph. fit(df_sampled, duration_col='Total number of months in service', event_col='churn')df_stats = cph. summaryfeatures_valuable = list(df_stats[df_stats['exp(coef)']. values &gt; 1. 01]. index) + list(df_stats[df_stats['exp(coef)']. values &lt; 0. 98]. index)df = df[features_valuable+['churn','Total number of months in service']]from lifelines import CoxPHFittercph = CoxPHFitter(penalizer=0. 01)"
    }, {
    "id": 18,
    "url": "http://localhost:4000/kowshikchills.github.io/Game-Theory-The-Genius-of-Nash/",
    "title": "Game Theory- The Genius of Nash",
    "body": "2020/06/10 - We discussed strict dominance solution concept in great detail in the last blog. Its application is limited and only applicable to some section of games( Games with strict dominant strategy). Strict dominant strategy often fails to exist. Lets consider Battle of sexes game.  Dominant strategy equilibrium did not apply, because there is no dominant strategy. In the last blog, we discussed the concept of belief. Player will behave optimally( best response ) to their beliefs. Chris may behave optimally and go to football given his belief that Alex is going to the football game. But their beliefs can be wrong. In this blog, we will discuss one of the most central and best known solution concept in the game theory. This overcomes many shortcoming faced by other solution concepts, this is developed by John Nash. Let‚Äôs define Nash‚Äôs solution concept. Nash equilibrium is as a profile of strategies(defined in the last blog) for which each player is choosing a best response to the strategies of all other players. Each strategy in a Nash equilibrium is a best response to all other strategies in that equilibrium Lets formally define nash equilibrium Definition: The pure-strategy profile s= (s‚ÇÅ, s*‚ÇÇ, . . . , sn) ‚àà S** is a Nash equilibrium if **s·µ¢ is a best response to s*‚Çã·µ¢ , for all i ‚àà N, that is, v·µ¢(s‚àó·µ¢ , s‚àó‚Çã·µ¢) ‚â• v·µ¢(s·µ¢, s‚àó‚Çã·µ¢) for all s·µ¢ ‚àà S·µ¢ and all i ‚àà N. Please note that s* is strategy profile, not strategy. strategy profile refers of set of actions taken by all the players in a strategic environment/game. lets try to understand this definition by working out an example.  Consider this matrix representation. Now lets write down all possible strategy profiles. S = {(L,U), (C,U),(R,U),(L,M), (C,M),(R,M),(L,D), (C,D),(R,D)}. Now lets evaluate payoff functions vis-a-vis best response. if player 1 chooses U best response for player 2 is L: BR‚ÇÇ(U) = LBR‚ÇÇ(U) = L, BR‚ÇÇ(M) = C, BR‚ÇÇ(D) = RBR‚ÇÅ(L) = U, BR‚ÇÅ(C) = D, BR‚ÇÅ(R) = UNow closely observe If player 2 chooses L, then player 1‚Äôs best response is {U}; at the same time, if player 1 chooses U, then player 2‚Äôs best response {L}. It clearly fits the definition above. So this is the s*: {L, U} Nash equilibrium. let‚Äôs apply Nash‚Äôs solution concept to prisoners dilemma.  S = {(RS,BE), (BE,BE), (BE,RS), (RS, RS)} Nash equilibrium s* is (BE,BE)I encourage readers to solve this and find out how (BE,BE) is Nash Equilibrium. Here are the assumptions for a Nash equilibrium:  Each player is playing a best response to his beliefs.  The beliefs of the players about their opponents are correct. We will not dig too deep into these assumptions as it can put us in mid of some philosophical discussion. Lets compare Nash solution concept with other solution concepts Here it easy to deduce that there is strictly dominant strategy for both players: thus strict dominance concept fails. There is no strictly dominated strategy for any player, so iterative elimination method is not applicable. Lets check if a pure-strategy Nash equilibrium does exist. BR‚ÇÅ(L) = D, BR‚ÇÅ(C) = M, BR‚ÇÅ(R) = MBR‚ÇÇ(U) = L, BR‚ÇÇ(M) = C, BR‚ÇÇ(D) = Lwe find that (M, C) is the pure-strategy Nash equilibrium‚Äî and it is unique. Solution concept is finest if it predicts or prescribes an unique strategy. It is necessary to understand if Nash equilibrium always yields unique strategy. Lets consider the battle of sexes game.  Let‚Äôs solve Nash equilibrium for this game. S = {(O, F), (O, O), (F, F), (F,O )}BRa(O) = O, BRa(F) = FBRc(O) = O, BRa(F) = F We can clearly observe that we may not have a unique Nash equilibrium, but it usually lead to more refined predictions than those of strict dominant solution concept and iterative elimination. Nash equilibrium solution concept has been applied widely in economics, political science, legal studies, and even biology. Let‚Äôs discuss an example where we can apply Nash‚Äôs solution concept to real life problem. Stag Hunt:  Two individuals go out on a hunt. Each can individually choose to hunt a stag or hunt a hare. Each player must choose an action without knowing the choice of the other. If an individual hunts a stag, they must have the cooperation of their partner in order to succeed. An individual can get a hare by himself, but a hare is worth less than a stag. This has been taken to be a useful analogy for social cooperation, such as international agreements on climate change. The payoff matrix is as follows BR‚ÇÅ(S) = S, BR‚ÇÅ(H) = HBR‚ÇÇ(H) = H, BR‚ÇÇ(S) = SGame has two pure-strategy equilibria: (S, S) and (H, H). However, the payoff from (S, S) Pareto dominates that from (H, H). If a player anticipates that the other individual is not cooperative, then he would choose to hunt a hare. But if he believes that other individual will cooperate then we would choose stag. When both individuals choose stag i. e when both believe other individual will cooperate, as a whole both of them would be better off. Scarce Resource: Let‚Äôs try to understand how self interested players might behave in scenario of scarce resources. Imaging there are n fertiliser manufacturing companies each choosing how much to produce around a fresh water lake. Each manufacturing companies degrades some amount of fresh water in that lake and uses, Lets say the total units of water in lake is K. Each player i chooses his own consumption of clean water for production, k·µ¢ ‚â• 0, and the amount of clean water left is therefore K -‚ÖÄki . The benefit of consuming an amount k·µ¢ ‚â• 0 gives player i a benefit equal to ln(k·µ¢) to the fertiliser company, and no other player benefits from i‚Äôs choice. Each player also enjoys consuming the remainder of the clean air, giving each a benefit ln(K ‚àí‚ÖÄ kj). Hence the total payoff of player i is For player i from the choice k= (k‚ÇÅ, k‚ÇÇ, . . . , kn). To compute Nash equilibrium, we need to find a strategy profile for which all players choose best-response to their beliefs about his opponent). That is we find strategy profile (k‚àó‚ÇÅ, k‚àó‚ÇÇ, . . . , k‚àón) for which k‚àó·µ¢= BRi(k‚àó‚Çã·µ¢) for all i ‚àà N. For player I, we can get best response the by maximising the value function written above. To find ki, which maximises the value function of industry i, We can equate its derivative to zero.  Solving above equation gives player‚Äôs i best response. Lets take only 2 industries case and solve this. ki(kj ) be the best response of player i.  Lets plot this with k1 payoff in x axis and k2 payoff in y axis.  If we solve the two best-response functions simultaneously, we find the unique Nash equilibrium, which has both players playing k‚ÇÅ= k‚ÇÇ = K/3. Mixed Strategies: So far we discussed pure strategies, but we need to discuss the problem where player may choose to randomise between several of his pure strategies. There are many interesting applications to this kind of behaviour where player chooses actions stochastically( i. e. Instead of chooses a single strategy, player chooses a distribution of strategies). The probability of choosing any of pure strategy is nonnegative, and the sum of the probabilities of choosing any all pure strategies events must add up to one. We will also closely observe applicability of Nash equilibrium to these mixed strategies. In fact, Nash equilibrium is applied to the games only if player chooses mixed strategies instead of pure strategies. We start with the basic definition of random play when players have finite strategy sets S·µ¢:Let S·µ¢ = {s·µ¢‚ÇÅ, s·µ¢‚ÇÅ, . . . , s·µ¢m} be player i‚Äôs finite set of pure strategies. Define ŒîS·µ¢ **as the simplex of **S·µ¢, which is the set of all probability distributions over S·µ¢ . A mixed strategy for player i is an element œÉ·µ¢ ‚àà S·µ¢, so thatœÉ·µ¢= {œÉ(s·µ¢‚ÇÅ), œÉ·µ¢(s·µ¢‚ÇÇ), . . . , œÉ·µ¢(s·µ¢m)) is a probability distribution over S·µ¢ **,where **œÉ·µ¢(s·µ¢) is the probability that player i plays s·µ¢ . Now consider the example of the rock-paper-scissors game, in which S·µ¢= {R, P, S} (for rock, paper, and scissors, respectively). We can define the simplex as ŒîSi ={(œÉ·µ¢(R), œÉ·µ¢(P ), œÉ·µ¢(S)) : œÉ·µ¢(R), œÉ·µ¢(P ), œÉ·µ¢(S)‚â•0, œÉ·µ¢(R)+œÉ·µ¢(P )+œÉ·µ¢(S)=1}, The player i and his opponents -i both choose mixed actions. It implies that player‚Äôs i belief about his opponents -i is not fixed but random. Thus a belief for player i is a probability distribution over the strategies of his opponents. Definition: A belief for player i is given by a probability distribution œÄ·µ¢‚ààS‚Çã·µ¢ over the strategies of his opponents. We denote by œÄ·µ¢(s‚Çã·µ¢) the probability player i assigns to his opponents playing s‚Çã·µ¢ ‚àà S‚Çã·µ¢ . For example in the rock-paper-scissors game, Belief of player i is represented as (œÄ·µ¢(R), œÄ·µ¢(P ), œÄ·µ¢(S)). We can think of œÉ*‚Çã·µ¢ as the belief of player i about his opponents, œÄ·µ¢, which captures the idea that player i is uncertain of his opponents. behavior. In pure strategy, the payoff is straight forward. In mixed strategy, to evaluate payoff we need to reintroduce the concept of expected payoff.  The expected payoff of player i when he chooses pure strategy s·µ¢‚àà S·µ¢ and his opponents choose mixed strategy œÉ‚Çã·µ¢‚àà ŒîS‚àí·µ¢Please note that pure strategy is part of mixed strategy.  When player i choose mixed strategy œÉ·µ¢‚àà ŒîS·µ¢ and his opponents choose mixed strategy œÉ‚Çã·µ¢‚àà ŒîS‚Çã·µ¢.  Let calculate payoff in mixed strategy scenario. lets assume that player 2 plays œÉ‚ÇÇ(R) = 0. 5œÉ‚ÇÇ(P ) = 0. 5œÉ‚ÇÇ(S) = 0 We can now calculate the expected payoff for player 1 if he chooses pure strategy. V‚ÇÅ(R, œÉ‚ÇÇ) = 0. 5(0)+ 0. 5(-1) + 0 (1)=-0. 5V‚ÇÅ(P, œÉ‚ÇÇ) = 0. 5(1)+ 0. 5(0) + 0 *(-1)= 0. 5V‚ÇÅ(S, œÉ‚ÇÇ) = 0. 5(-1)+ 0. 5*(1) + 0 *(0)=0 V‚ÇÅ(P, œÉ2)&gt;V‚ÇÅ(S, œÉ2)&gt;V‚ÇÅ(R, œÉ‚ÇÇ) To given player 2‚Äôs mixed strategy, we see a best response to player 1, which is action P. Now let‚Äôs understand how Nash equilibrium solution concept applies to mixed strategies. It actually simpler than it looks, we just replace strategy profile with mixed strategy profile. Definition: The mixed-strategy profile œÉ* = (œÉ‚ÇÅ , œÉ‚ÇÇ , . . . , œÉn )** is a Nash equilibrium if for each player **œÉ·µ¢ **is a best response to **œÉ‚Çã·µ¢**. That is, for all i ‚àà N, **v·µ¢(œÉ·µ¢ , œÉ‚Çã·µ¢) ‚â• v·µ¢(œÉ·µ¢, œÉ‚Çã·µ¢). ‚àÄ œÉ·µ¢‚àà S·µ¢. Each mixed strategy in a Nash equilibrium is a best response to all other mixed strategies in that equilibrium. Let‚Äôs close the discussion on mixed strategies here. We will discuss more about them in the next blog in my blog series. Hope you enjoy reading this blog. Thanks :) "
    }, {
    "id": 19,
    "url": "http://localhost:4000/kowshikchills.github.io/Bio-Mimicry-Nature-Inspired-Innovations/",
    "title": "Bio Mimicry ‚Äî Nature Inspired Innovations",
    "body": "2020/06/01 - Nature is so altruistic that we depend on it for every minute thing in life, yet it never ceases to influence us along the timeline. Early man had seen fire erupting due to friction and he rubbed stones to create fire. Every organism in nature has its uniqueness and its ability to change its capabilities according to the situations is a striking phenomenon where we can derive motivation from. A very popular example‚Ää‚Äî‚ÄäWright Brothers have closely observed pigeon and bats while flying which flashed the idea to design aircraft with wings.   Deriving inspiration from nature for invention to a complex human problem is called BioMimicry. The term ‚Äòbiomimetics‚Äô was coined in 1950s by American biophysicist and polymath Otto Schmitt. He has worked on neural system in squid and certain findings of his work led him to the invention of an amplifier. Later, Biomimicry was popularized by scientist and author Janine Benyus in her 1997 book Biomimicry: Innovation Inspired by Nature. She defines biomimicry as ‚Äúnew science that studies nature‚Äôs models and then imitates or takes inspiration from these designs and processes to solve human problems‚Äù. If you have seen the leaves of Lotus, the adhesiveness of the leaf with water particles is really low. This ultrahydrophobicity which means the leaf does not allow water to stay on it due to its versatile architecture of having wax crystals like small buds that trap air. When a droplet falls on it, the arrangement in its cells allow it to have minimal contact area with the droplet and repel the drops instead of making it wet. This idea has led to the invention of dust proof paints and water proof cloths.  Termites are not widely known for their excellence in creating impressive ventilation systems for cooling. They build intricate connections to pass on air flowing rather than stuck at a place and cause hot suffocated air in the room. This has stirred the idea in many architects to have elaborate ventilation system to keep buildings cool minimizing electric energy consumption.  Having transparent glasses in tall buildings is a beauty of architectural fineness but birds flying at that height cannot really see the glass as its completely transparent and might die by hitting them. Architects have found a way by observing spider web which shines in light that can draw attention of birds. They have designed special shiny glasses that can alert birds but they look like a normal transparent glass to us. They are known as spider glasses.  If you have ever observed the pace at which wood pecker drills a tree then you might afraid with the pace it drums the surface of a tree with its nose. It can beat 22 times per second with a deceleration of 1200g and a tolerating deceleration of humans is less than 80g. Scientists then observed via CT scans and found that Wood pecker has nearly 4 muscle structures around its neck so as to bear with the mechanical shock while pecking. This has sown the seed of an amazing invention of shock absorbers in automobiles. To protect from any injuries while pecking at fast pace where there is a high chance of injuring its head, but its skull is wrapped with a soft tissue of bones which will reduce the effect of hitting hard. This observation was inspired to design helmets.  Sun light is a renewable source of energy which made solar electricity a sustainable source of energy. Many of us might have seen solar panel arranged using a lot of space and they are faced at the direction of sun rise mostly. We know that Sun flower moves according to the direction of Sun light and its petals are arranged in such a way that they are separated and not overlapped to receive enough sun light. This has inspired a new design of solar panel which takes minimum space setup and with the panel that changes its direction as the sun moves along its path.  Japan has designed worlds first bullet speed train. They were a huge success in many other parts of the world too. It had a problem while it passes through tunnels in the mountain ranges which are so common in Japan. Due to its speed while passing through tunnel, it made deafening noise that caused trouble to people living near by. Scientists have researched a lot on this and one cannot believe that the bird kingfisher has provided a solution to this!! Yes that‚Äôs right! Kingfisher has its nose narrow and pointed. It just dives into water to catch fish with very little splash and noise. One of the engineer who is a bird watcher observed this and wondered if this redesign to face of the train that looks similar to kingfishers beak would reduce the loud boom. Luckily with positive results, the noise reduced and also brought down energy consumption to 15%.  Prey trapping mechanism in pitcher plant has provided a solution to design s net that traps insects. The capability of LED lights is maximized by understanding Fireflies mechanism. Qualcomm‚Äôs Mirasol display has microscopic reflective units that drew inspiration from colorful butterflies. These screen produce light using reflection and not by producing in the screen itself. These screens has reduced the usage of energy. Not only in technology, there are numerous examples where we derive insights from nature to treat diseases. We know there is no vaccine yet for AIDS. An extensive research is going on to increase immunity power in humans by observing alligators that are present in bay area of Louisiana which have extraordinary immune system that can fight hazardous diseases like HIV AIDS. Researching on mosquitoes to design thin needles for extracting blood for diagnosis and operations is another interesting development going on. There are several instances where Biomimicry is shaping innovative solutions to solve real world problems. This field is soaring high in developments to provide Millions of jobs in next 10 years with Billions of revenue. There is an institute founded by Janine Benyus to welcome researches to expand this field that has a great potential in giving rise to significant innovations. After all, we all need an easy life :) Hope you learnt a new thing today! Thanks for your time!! "
    }, {
    "id": 20,
    "url": "http://localhost:4000/kowshikchills.github.io/Understanding-Point-Processes/",
    "title": "Understanding Point Processes",
    "body": "2020/05/22 - Understanding Point Processes: In this world, many events occur and their trends are likely to follow a pattern. In this blog, we try to lay foundations to model those patterns. For example, the likeliness of a new earthquake typically increases in the region where an earthquake has already occurred. This increase in likeliness can be mainly because of the aftershocks created by the earlier earthquake. A panic selling of stocks in one country can cause a similar event in a different country. Take an example of wildfire, a wildfire in an amazon forest this year can greatly decrease the occurrence of another wildfire in the coming year. This decrease in the likeliness of wildfire next year is mainly because of the combustion of existing forest fuel. So it is clear that probabilities of a similar event can be elevated or decreased ‚Äî by patterns in the sequence of previous events. If the probabilities of a similar event are elevated i. e. , each occurrence increases the rate of future occurrence, like in earthquakes example, then these events can be categorised as stochastically excited or self- excited. If the probability of a similar event is decreased, like in earthquakes example then these events can be categorised as stochastically inhibited or self-regulating. If the probability of a similar event is unaffected, each occurrence doesn‚Äôt have any impact on the rate of future occurrence, then these events can modelled as a Poisson point process Theory of Point Process: A point process is a stochastic model underlying the occurrence of events in time and/or space. In this blog, we will emphasis on purely temporal aspects of point process i. e. , the space in which the points fall is simply a portion of the real line which represents time. Counting Process( N(t) ): To start, consider a line that represents time and event times T‚ÇÅ,T‚ÇÇ,‚Ä¶ of event times falling along the line, T·µ¢ (event times ) can usually be interpreted as the time of occurrence of the i-th event. This event can be earthquake in a particular region or wildfire in amazon forest. Our job is to model these event times. Instead of modelling these event times T‚ÇÅ,T‚ÇÇ ,‚Ä¶ Tn, it can alternatively be described by a counting process N(t). A counting process N(t) can be viewed as a cumulative count of the number of ‚Äòarrivals‚Äô into a system up to the current time t. If 146 earthquakes had occurred in Himalayans for last 80 years since the seismograph in installed, the N(80) = 146. Simple enough right!let‚Äôs also define history: H(u) history of the arrivals up to time u. Conditional Intensity Function ( ŒªŸ≠(t) ): When we are discussing the concepts of stochasticity, it is pertinent to define a function that gives the expectation of event occurrence at time t. That function is called intensity function and denotes as ŒªŸ≠(t), which represents the infinitesimal rate at which events are expected to occur around a particular time t. It is conditional on the prior history H(t) *of the point process prior to time *t.  The behaviour of a simple temporal point process N(t) is typically modeled by specifying its conditional intensity, ŒªŸ≠(t). We introduced terms like ‚Äòself-exciting‚Äô and ‚Äòself-regulating‚Äô which can be easily understood using the conditional intensity function. If a recent arrival in history *H(t) *causes the conditional intensity function to increase then the process is said to be self-exciting. In general, ŒªŸ≠(t) depends not only on *t *but also on the times T·µ¢ of preceding events i. e. , is H(t). When N is Poisson point process, the conditional intensity function ŒªŸ≠(t) depends only on information about the current time, but not on history H(u). Poisson point process is neither self-exciting nor self-regulating. ŒªŸ≠(t) is just function of over time for Poisson point process, stationary Poisson process has constant conditional rate: ŒªŸ≠(t) = Œ±, for all *t. *ŒªŸ≠(t) = Œ± implies that the probability of occurrence of an event is constant at any point of time regardless of how frequently such events have occurred previously. Hawkes process: The Hawkes process belongs to a family of self-exciting point processes named after its creator Alan G. Hawkes. Self-exciting point process models are used model events that are temporally clustered. Events like ‚Äúearthquakes‚Äù and ‚Äúpanic selling of stocks‚Äù are often temporally clustered, i. e. , the arrival of an event increases the likelihood of observing such events in the near future. Let‚Äôs define the Hawkes conditional intensity function ‚Äî Definition {t1, t2, . . . , tk} to denote the observed sequence of past arrival times of the point process up to time t, the Hawkes conditional intensity is The constant Œª is called background intensity, Œº(¬∑) is called excitation function. if Œº(¬∑) equals zero then this self-exciting point process reduces to simple stationary poisson process. A common choice for excitation function, Œº(¬∑) is exponential decay.  Parameters Œ± and Œ≤ are the constants. Œ±, Œ≤ can be interpreted as that each arrival in the system instantaneously increases the arrival intensity by Œ±, then over time, this arrival‚Äôs influence decays at rate Œ≤.  The modified Hawkes conditional intensity looks like this. Another frequently used excitation function in power law function. Œ± and Œ≤ are the parameters of ŒªŸ≠(t) and let Œ∏ represent parameters. The parameter vector Œ∏ for a point process is estimated by maximizing the log-likelihood function. We can also use parametric functions to approximate conditional intensity function, we will discuss more about that in the next blog in this blog series. There is an obvious extension to the self exciting point process which is mutually-exciting point process. These are essentially set of one-dimensional point processes which excite themselves and each other. This set of point process are called multi-variate or mutually exciting point processes. If for each i = 1, . . . , m then each counting process N·µ¢(t) has conditional intensity of the form: Simulations: Let‚Äôs simulate a simple Hawkes point process: Œª: 0. 1, Œ±:0. 1, Œ≤:0. 1 and try to understand conditional intensity function.  We can clearly observe the excitation and decay in the above graph. Now let‚Äôs increase the background intensity Œª to 0. 5.  We clearly see the number of events increased as the background intensity increased and hovered above 0. 5. We will now try to understand the impact of Œ±, Œ≤. Let‚Äôs increase Œ± to 0. 5.  We can clearly see that the number of events increased, this is because each occurrence of event increases the arrival intensity of next event by Œ±. So ŒªŸ≠(t) increase became higher, also one interesting observation is that ŒªŸ≠(t) varied from 0. 1 to 0. 6. Now let‚Äôs increase the Œ≤ to 0. 5. Remember Œ≤ controls the influence of decay rate of an event on its successive event.  Compare this plot with Fig 1, the decay is in Fig 1 is very less than the decay in Fig 4. The core concepts of Hawkes point process are demonstrated in the above examples. Point processes have extensive applications in various fields. We can model streams of discrete event/events in continuous time. We can also use function approximations where the conditional intensity functions of multiple event type can be approximated by novel neural architectures like LSTM. In the next blog in this series, we will discuss neural hawkes process. Thanks for your timeüòá "
    }, {
    "id": 21,
    "url": "http://localhost:4000/kowshikchills.github.io/The-Cox-Proportional-Hazards-Model/",
    "title": "The Cox Proportional-Hazards Model",
    "body": "2020/05/20 - A Modelling Technique to Estimate the Survival Rates: Modelling time has been a topic of interest for scientists, sociologists, and even epidemiologists. A maintenance engineer wants to predict the time it takes for the next failure of a particular component in a vehicle engine occurs so that he can schedule preventive maintenance. It is of epidemiologist‚Äôs interest to predict when the next outbreak will occur, so he can plan for medical interventions. Business analyst want to understand the time it takes for an high values customer to churn so that he/she can take preventions measures. In our earlier blogs on point process model, we explored statistical techniques that estimate the likeliness of a certain event occurrence in the backdrop of the time dimension. In this new statistical techniques, we will keep the event in backdrop and model time. Survival models are statistical techniques used to estimate the length of time taken for an event to occur. We call event occurrence as failure and survival time is the time taken for such failure. Cox proportional-hazards model is developed by Cox and published in his work[1] in 1972. It is the most commonly used regression model for survival data. The most interesting aspect of this survival modeling is it ability to examine the relationship between survival time and predictors. For example, if we are examining the survival of patients then the predictors can be age, blood pressure, gender, smoking habits, etc. These predictors are usually termed as covariates. *Hazard Function ( *Œª(t) ): The hazard function *Œª(t) is defined as the event rate at time *t. Suppose that an item has survived for a time t, then Œª(t) is the probability that it will not survive for an additional time dt. Hazard function Œª(t) gives the instantaneous risk of demise at time t, conditional on survival to that time and covariates.     Z is a vector of covariates     Œªo(t) is called the baseline hazard function  Baseline hazard function describes how the risk of event per time unit changes over time. It is underlying hazard with all covariates Z1, ‚Ä¶, Zp equal to 0.  Parameters Estimation: Cox proposed a partial likelihood for Œ≤ without involving baseline hazard function Œªo(t). The parameters of the Cox model can still be estimated by the method of partial likelihood without specifying the baseline hazard. The likelihood of the event to be observed occurring for subject j at time Xj can be written as L‚±º(Œ≤) is probability that individual j fails give that there one failure from risk set. Partial Probability L(Œ≤) = ‚àè(L‚±º(Œ≤)). R(Xj) is called risk set, it denote the set of individuals who are ‚Äúat risk‚Äù for failure at time t [3]. This partial likelihood function can be maximised over Œ≤ to produce maximum partial likelihood estimates of the model parameters[2]. For convenience we apply the log to the partial likelihood function: log-partial likelihood( ùìÅ(Œ≤)): We differentiate log-partial likelihood( ùìÅ(Œ≤)) and equate it to zero for calculating the Œ≤. The partial likelihood can be maximised using the Newton-Raphson algorithm[2]. Python Implementation: Let‚Äôs jump into the final and most interesting section: implementation of CoxPH model in python with the help of lifelines package. An example dataset we will use is the Rossi recidivism dataset. **from** **lifelines** **import** CoxPHFitter**from** **lifelines. datasets** **import** load_rossirossi_dataset = load_rossi()    arrest column is the event occurred,     The other columns represent predicates or covariates     Week is the time scale   cph = CoxPHFitter()cph. fit(rossi_dataset, duration_col=‚Äôweek‚Äô, event_col=‚Äôarrest‚Äô)cph. print_summary()   cph. plot() outputs this pictorial representation of coefficient for each predictor. The summary statistics above indicates the significance of the covariates in predicting the re-arrest risk. Age doesn‚Äôt play any significant role in predicting the re-arrest, whereas marriage variable plays significant role in predicting time for re-arrest. Lets look at a survival curve for one candidate with particular features(predicates/ covariates) using cph. predict_survival_function(df_vector). plot(). Survival rates (S(t)) simply gives us the probability that event will not occur beyond time t.  we can also plot what the survival curves for single covariate i. e we keep all other covariates unchanged. This is useful to understand the impact of a covariate. we use[plot_covariate_groups()](https://lifelines. readthedocs. io/en/latest/fitters/regression/CoxPHFitter. html#lifelines. fitters. coxph_fitter. CoxPHFitter. plot_covariate_groups) method and give it the covariate of interest, and the values to display[4].  We can clearly see that the survival rates of married prisoner is higher than that of unmarried as married tends less to do crimes again as he got family to take care. We can simply deduce such similar and valuable insights from the above survival curves. Summary: We introduced the most famous survival model: Cox model; in this blog and understood its mathematical implementation. We also saw through its python implementation that the model has kept its promise of interpretability. There are more and robust model to discuss in survival model. We will discuss more examples and other famous survival models in the next blog in this series. Thanks for your timeüòÄ "
    }, {
    "id": 22,
    "url": "http://localhost:4000/kowshikchills.github.io/Support-Vector-Machines/",
    "title": "Support Vector Machines",
    "body": "2020/05/16 - Given a problem statement to categorize a set of data into classes, we can resort to algorithms like logistic regression, decision trees, boosting techniques etc. , There is one more interesting and intuitive concept that helps in classification which is support vector machines. To understand SVM we must have clear idea on hyperplane, margin, kernel. Here is my attempt to help you understand these terms¬†:) Hyperplane:: Assume you have a 2D space with some data points as shown, and a line (ax+ by +c=0) is able to group this space into two parts.  Similarly for data in 3D space, a 2D plane can group the data into parts and for higher dimensions this works as well but it is hard to visualize. This flat line(1D)/plane(2D)/sub space of dimension p-1 which categorizes the given data space of p dimensions(features) is called a hyperplane. Suppose we are solving a classification problem and the data is spread as shown in fig-1. If the algorithm is able to find the hyperplane then we are sorted because it classified all the points correctly. Let us understand this approach in depth. For a p-dimension hyperplane, if a point X = (X1,X2,‚Ä¶,Xp) in p-dimensional space (i. e. a vector of length p) satisÔ¨Åes eq-1, then X lies on the hyperplane. If X does not satisfy the equation and instead if Œ≤0 + Œ≤1X1 + Œ≤2X2 + ‚Ä¶+ Œ≤pXp &gt; 0 then it lies on one side of the hyper plane and if Œ≤0 + Œ≤1X1 + Œ≤2X2 + ‚Ä¶+ Œ≤pXp &lt;0 then X lies on the other side of the hyper plane. With this equation we can divide the whole data space into 2 parts and can easily classify the new data point based on the above conditions. For example, a hyperplane of 1+2X1+3X2=0 will look like: There is a data set which looks as shown below. We can bring up many hyper planes, let us understand the A,B,C hyper planes as shown below. C is able to correctly classify and B had few errors and A has many errors in classifying the data. But how does the algorithm pick if A,B,C as best hyperplane to classify data? Margin:: Let us take the same data space with only one hyperplane, the lines to the closest data points from the hyperplane are represented as M1 and M2 ( these should be perpendicular to the hyperplane selected) and the distance from the hyperplane to either M1 or M2 is called marign and these nearest points where arrows are put are called support vectors. Support vectors lie along the margins and indicates the length of margin thus supporting the hyperplane because if these points are moved either near or away from hyperplane, then margin is decreased or increased respectively. If we observe closely, the hyperplane and margin is dependent only on support vectors and not on any other data points.  For different hyper planes we have different lengths of margins, and the hyperplane which has the maximum margin is called the Maximal margin hyperplane or the optimal separating hyperplane. If we are categorizing a data point into a class based on the position it lies with respect to the maximal margin hyperplane (left or right of hyperplane) then it is called maximal margin classifier. For this kind of data set as shown in the figure above, the hyperplane is able to correctly categorize the points but its not the same always, many real time data sets might have hyper planes having misclassified data points as well. This an important point to make while learning SVM. This helps in adding robustness to the model when using on test data.  For example in fig 2, point 1,8 are to the wrong side of their respective margins but to the correct side of hyperplane. Points 11 and 12 are located on the wrong side of hyperplane and margin as well. What happens with different lengths of M? If margin is large i. e. , if the support vectors are far from hyperplane, then it is obvious that the data points are well distanced enough and a new point coming in can be easily classified with very less error. If M is small, then the support vectors are really close to hyperplane and any new point coming in has a very high probability of misclassification. So optimization would be to maximize M(width of margin). Mathematically, here, yi is the classification variable where it takes values of 1 and -1 only for the two classes present, C is a non negative tuning parameter. If Œ≤0,Œ≤1,‚Ä¶,Œ≤p are the coefficients of the maximal margin hyperplane, then a new test data point x is classified based on the sign of f(x) = Œ≤0 + Œ≤1x1 + Œ≤2x2* + ‚Ä¶+ Œ≤pxp*. epsilon-1,‚Ä¶,n are slack variables that allow individual observations to be on the wrong side of the margin or the hyperplane. If epsilon-i = 0 then the ith observation is on the correct side of the margin. If epsilon-i &gt; 0 then the ith observation is on the wrong side of the margin, and we say that the ith observation has violated the margin. If epsilon-i &gt; 1 then it is on the wrong side of the hyperplane as explained in fig-2. The role of the tuning parameter C ‚Äî It bounds the sum of the epsilon-i‚Äôs, so it determines the number and severity of the violations to the margin (and to the hyperplane) that we will tolerate. C is the assignment of amount that margin can be violated by n observations. ‚Äî If C = 0 then there is no budget for violations to the margin, and it must be the case that epsilon-1 = ‚Ä¶= epsilon-n = 0, in which simply amounts to the maximum margin hyperplane (in the equation above). ‚Äî C is picked using cross-validation. If C is small, the margins are closer and the model has high fit on train data with low bias but high variance. If C is larger then margins have more width and a new data point has a high probability of violating the hyperplane separation, thus leading to more bias but lower variance. Tuning all these parameters will result in a good classifier.  For a random data with blue and red points as different classes, as the value of C increases, the model is clearly intolerant of misclassified data and tries to classify all of them correctly by adjusting the hyperplane. Kernels:: A question that might already have arised looking at fig-1 is that what if the data points are spread randomly in the space? what if there is no linear boundary separating the classes- then how does SVM work?‚Ää‚Äî‚ÄäThe trick is with kernels. To deal with this problem of non- linear boundary between classes, we can transform the feature space to a large size using polynomial function of predictors by making them squared or cubed or to any higher degree. But doing so would lead to computational inability because of huge predictor space. Here is where kernels have an upper hand in ruling out computational roadblocks. So let us try to understand it. Looking at eq-2 above, it is comprised of a basic mathematical computation ‚Äî inner product of the observations. Inner product of any two observations xi,xi‚Äô is For a linear boundary, this is one representation, there will be different representations for non-linear boundaries which we will see soon. So when ever we refer to a function (like inner product here) we can write a generalized form K (xi,xi‚Äô). K is any function and it is referred as Kernel here which measures the similarity of two observations.  So, the support vector classifier with any kernel K is represented as where there are n parameters Œ±i,i=1,‚Ä¶,n, one per training observation. However, it turns out that Œ±i is nonzero only for the support vectors in the solution ‚Äî that is, if a training observation is not a support vector, then its Œ±i equals zero. S is the collection of indices of these support points. The polynomial kernel can be defined as above with degree d. Instead of using linear kernel, polynomial kernel provides a much better decision boundary for classifying the data. So, a kernel essentially amounts to Ô¨Åtting a support vector classiÔ¨Åer in a higher-dimensional space involving polynomials of degree d, rather than in the original feature space. Another popular kernel used is radial kernel which has gamma as a positive constant that has a great impact on the model. If a given test observation x* =( x1, ‚Ä¶xp) is far from a training observation xi in terms of Euclidean distance is large but the equation in radial kernel discounts it using gamma and the distance is greatly reduced, so xi will play virtually no role in f(x)‚Ää‚Äî‚Ääeq-3 above. Recall that the predicted class label for the test observation x is based on the sign of f(x). In other words, training observations that are far from x will play essentially no role in the predicted class label for x*. This means that the radial kernel has very local behavior, in the sense that only nearby training observations have an eÔ¨Äect on the class label of a test observation.   Deploying kernels with with support vectors for non linear boundaries is called Support Vector Machine.  A little comparison between these kernels would do no harm¬†:P Time taken to train the model¬†, ability to fit data and risk of overfitting: linear&lt; polynomial &lt; radial Learning in python: Like all other models, SVM is also included in sklearn package. We have various parameters in sklearn. svm where SVC function is for classification and linearSVC for regression. As we talked about classification here, lets check out the important parameters in SVC function that might greatly effect the results. C: This is a regularization parameter that we have in eq-2 as explained above. default C value is 1 kernel: Values that go into this parameter are ‚Äòlinear‚Äô, ‚Äòpoly‚Äô, ‚Äòrbf‚Äô, ‚Äòsigmoid‚Äô, ‚Äòprecomputed‚Äô, default =‚Äôrbf‚Äô (rbf- radial basis function) degree: If a polynomial kernel is in use, then we have to specify the degree here. For rest of the kernels this will not be used. default is 3 gamma: this is another important parameter while using radial, polynomial or sigmoid kernels. Let us understand how this is implemented in python with a tutorial. I have taken breast cancer anlaysis data set from kaggle for classification. For data cleaning and EDA‚Ää‚Äî‚Ääplease check this blog if you are interested. from sklearn. svm import SVCfrom sklearn. metrics import accuracy_score,f1_scorec_range = [0. 01,0. 1,1,10,100]eval_metric = pd. DataFrame(columns = [‚ÄòC_parameter‚Äô,‚ÄôAccuracy‚Äô])eval_metric[‚ÄòC_parameter‚Äô] = c_rangej = 0for i in c_range: svm_linear = SVC(kernel = ‚Äòlinear‚Äô, C = i, random_state = 0) svm_linear. fit(x_train,y_train) y_pred = svm_linear. predict(x_test) eval_metric. iloc[j,1] = metrics. accuracy_score(y_test,y_pred) j += 1print(eval_metric) I have taken a linear kernel with different values of C and fit the train data into the model. We observe that for C being 1 or 10 or 100, the model has not improved much and got a decent accuracy score. We can similarly change different parameters like kernel to rbf or poly etc. , and pick the one that fits the requirement. Hope this helps! thank you!! References: An Introduction to Statistical Learning: With Applications in R "
    }, {
    "id": 23,
    "url": "http://localhost:4000/kowshikchills.github.io/ENLIGHTENMENT/",
    "title": "ENLIGHTENMENT",
    "body": "2020/05/05 - Intellectual Conquest against Invincible Conquerors: This is a great story of our collective pasts that hatched a generation of visionary leaders, fearless cultures, successful industrialists, and a guild of philosophical economists whose literature jostled into people‚Äôs minds like never before. This glorification of our past conveniently concealed the inhuman acts of our humanity. This story is not meant to be heard by either those who lived through or those who ruled. Nevertheless, this story is the heritage of humanity, which is meant to be passed to the coming generations most objectively. This is my attempt to give the objective description of this story illustrating the important epochs/facts that shaped the thoughts of humanity. This story begins with the enlightenment of Europe, then the story is dominated by mother of all revolutions ‚ÄúAmerican war of Independence‚Äù and we also look at the American civil war. We will see how these thoughts of enlightenment engulfed France that contend with the timid and fragile monarchy in France leading to the most famous French revolution. We then are set out to understand the consequences of the rise of greatest military leader Napoleon Bonaparte who changed the political contours of Europe like never before. This nationalism is now inevitable in the minds of enlightened men and in no time, we will see, turn into radical nationalism, causing the unification of Italy and Germany under the great leadership of Cavour and Bismarck respectively. What we see next are the infamous ‚Äútotal‚Äù wars which plunged the whole world into the bloodiest confrontation. A brief discussion on leaders like Adolf Hitler of Germany, Joseph Stalin of Russia, Tsar Nicholas II of Russia, Kaiser William II of Germany is inevitable, these military dictators responsible for this long persisted manslaughter, undeniably had the least regard for the humanity. Enlightenment: Enlightenment is defined as the cultural and ideological movement witnessed in Europe in the 18th century. First emerged in Holland and Britain, later spread across the whole of Europe like a wild torch. These ideas caught the imagination of the educated middle class of Europe. These ideals will be no longer restricted to just Europe, the cloud of enlightenment engulfs the colonial lands and thus causing disillusion that lead to decolonization. The main reason for the emergence of this radical movement is the need for countering catholic domination in the social, economical, and even political lives of people in the empire. The wide consensus that church is a sole authority in interpreting religion render it the status of a dominant institution. Pope is kept above the king and religious toleration is still not a reality. The riches of the church manufactured a class of degenerated elite and thus fueling the voices of dissent against the perpetual paramountcy of church. This catholic domination in their life is questioned by the protestant movement. Greatest French philosophers like Voltaire demanded separation of church and state. The questioning of catholic‚Äôs dominance involves questioning their fundamentals, the spiritualism they argue from which the church draws its power. This incessant questioning forms the bedrock for enlightenment. Famous ‚Äú95 thesis‚Äù is such articulated questions written by famous German priest Martin Luther . In this quest of questioning, many scholars promulgated the ideals like liberty, the supremacy of nature, efficacy of reason, and logic. Notions like individualism which argue that social institutions exist for individuals, not vice verse became more clearer to the people. John Locke, father of liberalism argues that individuals must have complete freedom from church, society, and state. These ideas which were never sensed by the middle class, now becoming visible, soon they realize that their marginalization and oppression are because of totalitarian governments. Ideals of democracy, god-given rights, liberty, and humanism which was long denied to the majority of the populace by the far right-ruling elite, now couldn‚Äôt stop them from spreading. Equality of the man was now the guiding principle of those who reject absolute rule. Scholars vehemently condemned discrimination based on birth and material processions through their writings. The voices demanding the constitutional monarchy in place of absolute monarchy became louder and hostile. Intellectual maturity is the direct consequence of Enlightenment, scientific innovations sprung in European society like never before. Apart from Empiricism, rational thought, cultural consciousness is the clear manifestation of this intellectual movement.  Newspapers, books, and debate in saloons worked as the vehicles of thought. Merchants and military expeditions also contributed to the spreading of these thoughts across the world. A new guild of philosophers was born whose ideas and contributions are felt till date. Immanuel Kant is one of most famous Germany philosophers belonging to the age of enlightenment. He introduced the idea for the universal moral law, he claims that moral law is the highest law, and states must derive authority from it. No law of the state should contradict with this universal law, these kinds of ideas promulgated by Kant will open stir the nationalist feelings amongst Germans. Another most important philosopher whose writings form the bedrock for french revolutions is Rousseau. His famous ideas on artificial and natural inequalities are eloquently depicted in his book ‚Äú The Social Contract ‚Äú. Unlike many contemporary philosophers who argue with logic and reason, Rousseau‚Äôs appeal to the people reaches their hearts. Rousseau‚Äôs believed in the power of community and not in favor of individualism i. e, the interest of individuals can be surrendered for the sake of the community.  Though these ideals are appealing to the middle class, it is still limited to a very small percentage of people. There is no mass action against the regime that vehemently oppose the ideas of the enlightenment. These ideals are still sensed as radical to the countryside of the states, this thwarted the likeliness of mass action. Summary: Movement of enlightenment not only introduced the ideas of liberty, democracy, rights to the people, but also led to decolonisation and mass actions against totalitarian regimes. Enlightenment is just the beginning of an end, a defiance against the British colonial masters is ready to explode in the American colonies. In the next blog, we will discuss the mother of all revolutions: THE AMERICAN REVOLUTION. "
    }, {
    "id": 24,
    "url": "http://localhost:4000/kowshikchills.github.io/Neural-Hawkes-Process/",
    "title": "Neural Hawkes Process",
    "body": "2020/05/02 - Neural Hawkes Process: In the last blog in this publication, we looked at the definition of point process and also looked at Hawkes process in detail. In this blog, we will understand how to construct a neurally self-modulating multivariate point process in which the conditional intensity functions of multiple events is modelled according to a LSTM. It is assumed that reader is aware of some basic concepts of LSTM like hidden state, memory cell and memory control gates. Introduction: In the vanilla Hawkes process, the conditional intensity function is constructed using background intensity and excitation function. Past events can temporarily raise the probability of future events. However, in the real world application, the flexibility of vanilla Hawkes process is very limited to approximate many sections of the problems. We need a better model with least assumptions to approximate the conditional intensity function. We can generalise the Hawkes process by determining the event intensities using a neural network architecture. We will try to understand why we need a neural network model to approximate a conditional intensity function. In the vanilla model, for example lets consider exponential decay Hawkes process ( Understanding point Process).  We assume that each arrival of event excites the future events positive and additive in nature and exponentially decaying with time. Parameters Œ±, Œ≤ can be interpreted as that each arrival in the system instantaneously increases the arrival intensity by Œ±, then over time this arrival‚Äôs influence decays at rate Œ≤. Parameter Œ± fails to capture inhibition effects, this limits the expressivity of the model. Neural Network Approximation: In many real-world patterns doesn‚Äôt follow these assumptions. The effect of past events on future events can now be additive even subtractive, and can depend on the sequential ordering of the past events. For example, its not really simple to accurately model the earthquakes and complex after-shocks by just assuming exponentially decaying conditional intensity function. We can further generalise the Hawkes process by using LSTM to model the conditional intensity functions from the hidden state of a recurrent neural network. Long short-term memory (LSTM) is a modified recurrent neural network (RNN), built to handle the vanishing gradients problem RNN faces. It process entire sequences of data instead of just single data points. The hidden state is a deterministic function of the past history. Neural Hakes process removes the restriction that the past events have independent, additive influence on ŒªŸ≠(t). This method uses a recurrent neural network to predict ŒªŸ≠(t). This allows learning underlying dynamics and approximate the conditional intensity function based on history H(t): number, order, and timing of past events. Let assume that we are solving multi-variate or mutually exciting point processes, these are essentially set of one-dimensional point processes which excite themselves and each other. Event type k has a time-varying intensity ŒªkŸ≠(t) is approximate by a value in hidden state vector h(t). In LSTM, h(t) is a sufficient statistic of the history and approximately summarise the past event sequence. The Neural Hawkes process is different from exponential decay Hawkes process in following ways:    The base rate is not a constant Œª, but can change with time.     The excitation can be can be non-monotonic, because the influences on ŒªkŸ≠(t) can be excitatory and inhibitory.     The model can generalise the influence of events on other events, also takes into account that some pairs of event types do *not *influence one another.  If we closely observe, the familiar discrete-time LSTM is developed on discrete time steps, but it cannot directly consume the event sequence for generating the conditional intensity function thorough its hidden states. Thus we are using continuous-time LSTM, which is familiar to discrete-time LSTM. The difference is that in the continuous interval following an event, each memory cell c exponentially decays at some rate Œ¥ toward some steady-state value[1].  We can use this loss function for optimising the LSTM parameters. The derivation of this equation is not discussed in the blog, readers who are interested to understand the mathematical rigour can appendix B. 1 in the paper in references[1]. Summary: We discussed a more flexible multivariate Hawkes process using the novel continuous-time recurrent neural network (LSTM). This significantly increased the expressibility of the model. This model can be used in cases where set of events can have both inhibition and excitation effect based on the sequence of the events. This model has interesting applications in various business problems like predictive maintenance, advetising etc. In the next blog in this series we will discuss one of its applications in solving real world problems. Thanks for your time :) References: [1] The Neural Hawkes Process: A Neurally Self-Modulating Multivariate Point Process: https://arxiv. org/abs/1612. 09328 "
    }, {
    "id": 25,
    "url": "http://localhost:4000/kowshikchills.github.io/Bagging,-Random-Forests-and-Boosting/",
    "title": "Bagging, Random Forests and Boosting",
    "body": "2020/04/27 - When ever we deal with trees in Machine Learning, bagging and boosting are two commonly heard words. They are usually methods of ensemble modelling. It is similar to dividing a big task into numerous small tasks and aggregating them to achieve desired result from it. Have you ever faced the issue of over fitting in decision trees? we can try changing the parametres like adjusting max_depth etc. , but it won‚Äôt differ much in some cases. If we can recall that in decision trees, there is a high probability of model performing excellently on train data and poor performance in test set. As the decision trees are sensitive to the data they are trained, so any new observation or changes to train data will result in highly fluctuated values, resulting in high variance. To avoid this we have bagging and boosting. Look at the dataset where we have heights of 20 persons S={173, 179, 188, 163, 165, 176, 178, 157, 160, 157, 185, 188, 167, 168, 158, 178, 178, 160, 166, 171}. Mean or Average height=170. 75 and variance is 98. 28 Let us now divide the data randomly into 4 samples of 6 observations each. S1={173, 179, 188, 163, 165,188} , S2={176, 178, 157, 160, 157,160}, S3={ 185, 188, 167, 168, 158,166}, S4={178, 178, 160, 166, 171,157} Mean of S1=176, S2=164. 66, S3=172, S4=168. 33. Mean of means of S1, S2, S3, S4 ([176+164. 44+172+168. 33]/4) is 170. 25 close to mean of S= 170. 75 Variance of the means of S1, S2, S3, S4 is 17. 76 (very low variance compared to variance of S ‚Äî 98. 28). It is clear from this example that there is no loss in mean but variance is greatly reduced and we can work with samples and combine them later by averaging. This is the basic idea of bagging ‚Äî ‚ÄúAveraging reduces variance‚Äù. The process of randomly splitting samples S1 to S4 is called bootstrap aggregating. If the sample size is same as original data set size then it is called **boost strapping. **Bagging is a parallel process with an applications found in Random forests.  The process of using bootstrapped samples with replacement and applying a model on each of them and then average out the results to avoid high variance is called bagging. Random Forests: I was working on prediction of house prices data set to understand the algorithms on regression. One successful attempt was using Lasso and Ridge to reduce over fitting on high dimensional data (In case you are interested in full code it is at the end of the blog). Then I applied decision trees which produced decent accuracy but when I want to improve it, I tried different options like changing max_depth parametre. As max_depth is an important tuning feature which decides the complexity of the model, we can tune it in such a way that it does not overfit on the data. Increasing the depth will let the tree capture all the splits and patterns in the train data accurately and fail on a new data point in test set so reducing max_depth is one way to combat overfitting. The observations on house prices data are: from sklearn. tree import DecisionTreeRegressordtreg = DecisionTreeRegressor(random_state = 100,max_depth=5)dtreg. fit(x_train, y_train)dtr_pred = dtreg. predict(x_val)dtr_pred= dtr_pred. reshape(-1,1)dtr_x_pred = dtreg. predict(x_train)dtr_x_pred = dtr_x_pred. reshape(-1,1) max_depth here is 5 and decent scores of RMSE and R2-Score of training and validation data set. When max_depth is changed to 6, the model is over fitting as the metrics are really good on training data but on validation set, they are not upto the mark. I experimented a bit on depths 7 to 10 but the over fitting increased a lot, leading bad accuracy on validation data. In bagging, the whole data is bootstrapped into various samples with replacement and on each of this samples, a decision tree is built with all the features and the results are averaged out to get a final prediction. If we apply this concept with some twist on number of features and correlated trees, then the result is a random forest.  In each decision tree here, the split is made such that it considers only ‚Äòm‚Äô number of features which are randomly picked from the total of ‚Äòp‚Äô features. A new sample of ‚Äòm‚Äô features is picked at each split and we choose m ‚âà‚àöp features in general.  Therefore, while building each tree in random forest, not all the features are considered at each split. In bagging, we take all the features at each split unlike discounting them in random forests. Assume there is one strong feature which is more effective in finding out the patterns in the predictions. In bagging, if we consider all the features, then all the small trees will produce similar predictions which are correlated and averaging out will not reduce any variance, in many cases this might produce bad results. But consider random forests, we consider only m features at each split and that one string feature may or may not be there in the selected features, so each decision tree will produce uncorrelated results and averaging them will result in substantial reduction in variance. This processing of decorrelating the trees is more grounded and valid.  Uncorrelated trees are another important feature of random forests which finds its uses in real time data sets for regression and classification. In python sklearn. ensemble package we have RandomForestRegressor and RandomForestClassifier models. Lets check out the commonly used parameters involved which upon changing could produce a great performing model. n_estimators: This defines the number of decision trees we would like to have in the random forest (default is 100) min_samples_split: The minimum number of sample required to split a node which is internal(Neither the root node or leaf node is internal node) max_depth :The maximum depth of the tree. If None, then nodes are expanded until all leaves are pure or until all leaves contain less than min_samples_split samples max_features: Maximum number of features for best split, default is square root of p(total number of features) as discussed above. We can also specify log2(p) or a fraction of p I have used RandomForestRegressor on house prices predicting data set to check the difference of using Decision Tree and RF, n_estimators is the number of trees in the forest we would like to input, as the data size is around 1200 rows, we can experiment around with 10‚Äì15 trees. max_depth can be changed and experimented with and we can come up with an optimal one after few observations. As a start I took 4 and checked with 3,5,6,7,as well. optimal number of max features at split as discussed above would be square root of total number of features. In house prices data set, we have around 205 features so the max_features I took is 16 and min_sample_split can be given from a range of 1‚Äì40 for decision trees, higher values might lead to overfitting here I chose 4 and by default it is 2 if we do not specify and the results are: from sklearn. ensemble import RandomForestRegressorrfreg = RandomForestRegressor(n_estimators=15,random_state = 100, max_depth=4, max_features = 16,min_samples_split=4)rfreg. fit(x_train, y_train)rf_pred = rfreg. predict(x_val)rf_pred= rf_pred. reshape(-1,1)rf_x_pred = rfreg. predict(x_train)rf_x_pred = rf_x_pred. reshape(-1,1) If we play around with the parameters a bit, I observed that max_depth =4 is optimal and when increased to 5 or more there is a sign of overfit. Increasing other features like n_estimators or min_sample_split resulted in high accuracy on train data but less on test data. When DecisionTreeRegressor with max_depth = 5 is used, RMSE score is 0. 49 and R2 is 0. 75 which is a good score but with RandomForestRegressor max_depth=4 and max_features=20, RMSE has reduced to 0. 43 and R2 score has increased to 0. 80 on validation set, with no sign of overfitting. Thus, Random Forests finds a great use in Regression and Classification problems when we want to have best accuracy measures. Random Forests finds the use in many fields as they have the property of performing well on large data sets with high dimensions. Predominantly they are used in banking(fraud detection), finance and stock markets due to the property of uncorrelated trees that reduce correlation. Boosting: The concept of feedback is used in boosting ‚Äî *Learn from the weak to get strong. *Boosting is a sequential process. Boosting involves combining a large number of decision trees let them be ÀÜf1,‚Ä¶, ÀÜfB (B is number of trees) In Boosting we have 3 parametres that decide the performance:    Number of trees B. In bagging and random forests if we increase B then there is no effect but in boosting, it can over fit if B is large. The optimal number of B is found by using cross validation.     Each prediction of data points by the model is called residual, the shrinkage parameter Œª(small positive number) slows the process down even further, allowing more and diÔ¨Äerent shaped trees to change the residuals. Typical values are 0. 01 or 0. 001, and the right choice can depend on the problem. Very small Œª can require using a very large value of B in order to achieve good performance     The number d of splits in each tree, which controls the complexity of the boosted ensemble. Often d = 1 works well, in which case each tree is a stump, consisting of a single split. In this case, the boosted ensemble is Ô¨Åtting an additive model, since each term involves only a single variable. More generally d is the interaction depth, and controls the interaction order of the boosted model, since d splits can involve at most d variables.  An overview of Boosting algorithm,    Set ÀÜf(x) = 0 and ri = yi for all i in the training set     For b=1,2,3‚Ä¶ B repeat the following steps:  ‚Äî Fit a tree ÀÜfb with d splits (d+1 terminal nodes) to the training data (X,r). ‚Äî Update ÀÜf by adding the new version of the tree that is formed by shrinking: ‚Äî Update the residuals(weights),  Final model will be the consolidated one with all the trees, In simple words, the original data is taken and sequentially the data points are updated in boosting (unlike taking samples in bagging) and we give extra weightage to the wrongly predicted observations, the next models will try to correct them and thus the error(bias) is minimized. Boosting learns the patterns in the data from weak learners to finally develop a strong model. There is always an ambiguity in deciding when to use bagging or boosting in the data sets that we will encounter. Bagging will give best results when it is in concern with over fitting or high variance and Boosting will perform well with models that have high errors and reduces them as it enhances and reduces the risk of errors by adjusting weights to weak learners. Boosting is not much recommended for reducing over fitting as it self has the disadvantage if it. Bagging is parallel and Boosting is sequential thus it makes it quite slow in computation with huge data sets. Widely used separate algorithms used for these ensemble methods are:    Random Forest ‚Äî bagging.     Ada Boost, Gradient Boosting and XG Boost ‚Äî boosting.  Hope this gives a good understanding of the ensembling methods :) The code I used for House prices data set with Lasso andRidge, Decision Trees and Random Forest import pandas as pdimport numpy as npfrom matplotlib import pyplot as plt import seaborn as snsfrom sklearn. preprocessing import LabelEncoder, StandardScalerfrom sklearn. model_selection import train_test_splitfrom sklearn. linear_model import LinearRegressionfrom sklearn import metricsfrom sklearn import ensemblefrom sklearn. linear_model import Lasso,Ridge#load train datadf_data=pd. read_csv( data_price. csv )df_data. head()#to know each and every column execute the followingprint(df_data. columns)print(df_data. shape)total = df_data. isnull(). sum(). sort_values(ascending=False)percent = (df_data. isnull(). sum()/df_data. isnull(). count()). sort_values(ascending=False)missing_data = pd. concat([total, percent], axis=1, keys=[‚ÄòTotal‚Äô, ‚ÄòPercent‚Äô])missing_data. head(20)df_data= df_data. drop(missing_data[missing_data[‚ÄòTotal‚Äô]&gt;1]. index. values,1)df_data= df_data. drop(df_data. loc[df_data[‚ÄòElectrical‚Äô]. isnull()]. index)corr_mat=df_data. corr()fi,ax=plt. subplots(figsize=(20,20))sns. heatmap(corr_mat,square=True)del df_data[‚ÄòId‚Äô]le=LabelEncoder()cat_mask= df_data. dtypes==‚Äôobject‚Äôcat_cols= df_data. columns[cat_mask]. tolist()cat_cols#Lets convert the columns to one ht encodingdf_data[cat_cols]=df_data[cat_cols]. apply(lambda x: le. fit_transform(x. astype(str)))df_data_c = df_data. copy()#get_dummies is used for one hot encodingdf_data_c = pd. get_dummies(df_data_c,columns=cat_cols)x_train, x_test, y_train, y_test = train_test_split(df_data_c. drop(‚ÄòSalePrice‚Äô,axis=1),df_data_c[‚ÄòSalePrice‚Äô], test_size =0. 25,random_state=120)y_train= y_train. values. reshape(-1,1)y_test= y_test. values. reshape(-1,1)sc_X = StandardScaler()sc_y = StandardScaler()x_train = sc_X. fit_transform(x_train)x_test = sc_X. fit_transform(x_test)y_train = sc_X. fit_transform(y_train)y_test = sc_y. fit_transform(y_test)#Linear Regressionlm = LinearRegression()lm. fit(x_train,y_train)#predictions on train datax_pred = lm. predict(x_train)x_pred = x_pred. reshape(-1,1)#Prediction of test datay_pred = lm. predict(x_test)y_pred= y_pred. reshape(-1,1)def scores_(y,x):  print('MAE:', metrics. mean_absolute_error(y, x))  print('MSE:', metrics. mean_squared_error(y, x))  print('RMSE:', np. sqrt(metrics. mean_squared_error(y, x)))  print('R2 Score:' ,metrics. r2_score(y,x))print('InSample_accuracy')scores_(y_train, x_pred)print('---------------------------')print('OutSample_accuracy')scores_(y_test,y_pred)def regularization_model(model,alpha_range): rmse_score_insample=[] rmse_score_outsample=[] r2_score_insample=[] r2_score_outsample=[] for i in alpha_range: regularization = model(alpha=i,normalize=True) regularization. fit(x_train,y_train) y_pred_train = regularization. predict(x_train) y_pred_train = y_pred_train. reshape(-1,1) y_pred_test=regularization. predict(x_test) y_pred_test = y_pred_test. reshape(-1,1) rmse_score_insample. append(np. sqrt(metrics. mean_squared_error(y_train,y_pred_train ))) rmse_score_outsample. append(np. sqrt(metrics. mean_squared_error(y_test, y_pred_test))) r2_score_insample. append(metrics. r2_score(y_train, y_pred_train)) r2_score_outsample. append(metrics. r2_score(y_test, y_pred_test))df=pd. DataFrame() df[‚Äòalpha‚Äô]=alpha_range df[‚Äòrmse_score_insample‚Äô] = rmse_score_insample df[‚Äòrmse_score_outsample‚Äô]= rmse_score_outsample df[‚Äòr2_score_insample‚Äô] = r2_score_insample df[‚Äòr2_score_outsample‚Äô] = r2_score_outsample return df. plot(x = ‚Äòalpha‚Äô, y = [‚Äòrmse_score_insample‚Äô,‚Äôrmse_score_outsample‚Äô])alpha_range_lasso = np. arange(0. 001,0. 03,0. 001)print(regularization_model(Lasso,alpha_range_lasso))alpha_range_ridge = np. arange(0. 001,1,0. 1)print(regularization_model(Ridge,alpha_range))from sklearn. tree import DecisionTreeRegressordtreg = DecisionTreeRegressor(random_state = 100,max_depth=5)dtreg. fit(x_train, y_train)dtr_pred = dtreg. predict(x_val)dtr_pred= dtr_pred. reshape(-1,1)dtr_x_pred = dtreg. predict(x_train)dtr_x_pred = dtr_x_pred. reshape(-1,1)from sklearn. ensemble import RandomForestRegressorrfreg = RandomForestRegressor(n_estimators=15,random_state = 100, max_depth=4, max_features = 16,min_samples_split=4)rfreg. fit(x_train, y_train)rf_pred = rfreg. predict(x_val)rf_pred= rf_pred. reshape(-1,1)rf_x_pred = rfreg. predict(x_train)rf_x_pred = rf_x_pred. reshape(-1,1)Thank you! References: An Introduction to Statistical Learning: With Applications in R "
    }, {
    "id": 26,
    "url": "http://localhost:4000/kowshikchills.github.io/Decision-Trees/",
    "title": "Decision Trees",
    "body": "2020/04/11 - Have you ever had difficulty in deciding what to do in a situation? Well as human beings we have this amazing(mostly defective üòõ) habit of taking a decision in split second with out thinking. But assume you start thinking about the outcome, then your thought process would be to analyse the situation and draw insights to take a decision. Now think about a machine, it is also capable of taking a decision in split second but also by thinking and analyzing. Let us understand how Machine Learning finds its application in decision making.  Fig 1: A simple Decision Tree to understand if a patient is recovered from a disease or not. (classification tree)Predictor space(the whole data points of the independent variables) segmented into a number of simple regions. To predict a new data point, we assign it to a region in the predictor space based on the criteria it satisfies during the splits such that it will have the value as the mean of training observations(regression) or most commonly occurring class(classification) in that space. These kind of splitting rules can be used to segment are summarized in a tree, then these type of approaches are known as Decision Tree methods. Common terminology in Decision Trees: Root Node: The first condition where the whole data can be divided into two partsParent and Child Node: All the nodes in bold in Fig. 1 are parent nodes i. e. , which can further be divided. The further divided nodes(Sub-Nodes) are child nodes. Leaf Node: In Fig. 1 the nodes which are Recovered/Not Recovered are leaf nodes or terminal nodes as the decision conditions are terminated with them. Decision Node: All parent nodes except root node are decision nodes. We can find the Decision Tree applications in Regression and Classification problems. Fig 2: A regression tree with two features deciding on the approx numbers of days a patient takes for recovery Regression Trees:: If we have a linear data then we can use classic linear models, but if the data is non-linear and we need to perform regression on output variable then Trees do a better job. Let us understand on the data related to patient‚Äôs immunity scores, age and the days taken for recovery. In the above figure, it is clear how we can divide or split the feature space into different subsets(regions). If we can imagine it on a plane of data points, it should look like: Fig 3 R1, R2, R3 are the regions this feature space is divided based on the split conditions. As we see all the data points are divided into 3 regions based on the split conditions. So if we know the immunity score and age of a new patient, the algorithm places the new data point in one of these 3 regions and decide the approximate recovery days. The example shown is having only two features so it can be seen on 2D plane for Fig-2. As the features increase, the visualization gets challenging. Let us understand a little math behind data splitting into regions: ‚Äî If we are talking about the regression problem, we divide the whole data points into J distinct and non overlapping regions R1,R2,R3,‚Ä¶,RJ. (in our example ‚Äî 3 regions)‚Äî For every observation that falls into the region Rj, it assigns the same value for unseen data point. for example in Fig 2 we take mean of recovery days in each region and assign this mean to the new observation that falls in. The challenge lies in the conditions to be picked for splitting into regions R1, R2,. . ,RJ. We have to divide them in such a way that we have to minimize the RSS, Eq 1: ÀÜyRj is the mean response for the training observations within the jth region For this we follow the top-down approach. Top-Down approach is nothing but the split starts from a feature and goes on with next ones(an optimal feature which can divide the whole data into 2 or more homogeneous sets). This is also called greedy approach as the split starts from the beginning rather than waiting and picking a split that will lead to a better tree in some future step. The algorithm starts finding the best split by minimizing RSS with in each of the new regions. But in next step it proceeds with one of the identified regions and apply then same steps on it, now we have 3 regions. Then the split happens on these 3 regions by minimizing the RSS and so on. This process reaches an end with a stopping criteria like depth is 5 or 6 based on the problem statement. Tree Pruning:: If we have many regions then the algorithm have understood and split the train data perfectly. But when a new data point comes in there is a high probability of overfitting. So it is advisable to have fewer regions with low variance and low bias such that it can predict test data with the same accuracy as train data which should be the ideal case. But how do we know how many regions or splits is needed to achieve the ideal conditions? Here comes the concept of Tree Pruning. In pruning, the strategy is to first split into many regions so that we obtain a very big tree lets say To (T not) and then prune it back to obtain smaller subtree with minimal error rate. This process of pruning by reducing error is called Cost Complexity Pruning. As we are not sure which subtree will be a correct one to prune back, we go by math intuitively. We introduce a non negative tuning parametre Œ±(alpha) such that a sequence of trees form a subtree T that is subset of To such that the equation shown below is as small as possible. Eq 2: Cost Complexity Pruning       Here   T   indicates the number of terminal nodes of the tree T, Rm is the region (i. e. the subset of feature space) corresponding to the mth terminal node, and ÀÜyRm is the predicted response associated with Rm ‚Äî that is, the mean of the training observations in Rm. The tuning parameter Œ± controls a trade-oÔ¨Ä between the sub tree‚Äôs complexity and its Ô¨Åt to the training data. When Œ± = 0, then the sub tree T will simply equal To, as it is simply Eq 1 shown and is just training error. However, as Œ± increases, there is a price to pay for having a tree with many terminal nodes, Eq 2 tends to minimize for a smaller subtree. We pick alpha using Cross-Validation technique.     If you are familiar with Lasso regularization, Eq 2 is similar to it as we curb the complexity of linear model using Lasso. (My blog on Lasso for reference if needed) Classification Trees:: These are similar to Regression trees but instead of taking the mean of observations in the leaf node or region, we calculate the highest number of occurrences of a class of that target variable in that region. In regression we take RSS as the criteria for splitting optimally, however in this scenario the classiÔ¨Åcation error rate is simply the fraction of the training observations in that region that do not belong to the most common class. Classification error rate ÀÜPmk represents the proportion of training observations in the mth region that are from the kth class. There are two more metrics which are used widely other than E, to calculate the classification error. They are Gini Index: Gini Index G defines the total variance across K classes. It measures the degree or probability of a particular variable being wrongly classified when it is randomly chosen from the data set. It is also said Gini Index is the measure of Purity of that node so if a small Gini Index says that the node is Pure with majority of the predictions belongs to a single class. Another measure is Cross-Entropy which is: Cross Entropy Since 0 ‚â§ ÀÜ Pmk ‚â§ 1, it follows that 0 ‚â§‚àí ÀÜPmk log ÀÜPmk. If ÀÜPmk is near to zero or near one, Cross-Entropy will take a value near zero and it implies the node is pure. Cross Entropy is also similar to Gini Index and it used to understand the disorder of a grouping by the target variable. So E, G and D are used as metrics to understand the quality of a split at each node and help in Pruning. Advantages of Decision Trees::  Easy to understand and explain Works really good on smaller data sets and can be viewed graphically With out creating dummy variables, they can perform well on classification problems Computationally fast in classifying unknown data points Inexpensive in terms of space utilizationDisadvantages::  Trees can be non robust and overfit. A small change in data can cause large impact on predictions.  Large tress often unable to predict and the results may not match the expected.  If a node is having many splits then there is a possibility of giving more importance to that hence resulting in biased predictions. In python we have Decision Trees model in sklearn package each for Regression and Classification. Let us understand the important parametres that can be modified to achieve better results. criterion: Defines the error functions we want to use to obtain the quality of split at each node. Regression- {default= ‚Äúmse‚Äù, ‚Äúmae‚Äù, ‚Äúfriedman_mse‚Äù}Classification- { default=‚Äùgini‚Äù,‚Äúentropy‚Äù} max_depth: The maximum depth of the tree. min_samples_split: Defines the minimum number of samples that are needed to split and internal node in the tree. default = 2 There are few other parametres in the model but these 3 are mainly used in shaping the model for the need. The other parametres include min_samples_leaf, min_weight_fraction_leaf, max_leaf_nodes etc. , We find numerous applications in daily life of decision trees it is important to understand the problem statement precisely to know use the parametres in the model to get desired results. References: An Introduction to Statistical Learning: With Applications in R Thank you!: "
    }, {
    "id": 27,
    "url": "http://localhost:4000/kowshikchills.github.io/Game-Theory-Contention-and-Cross-Effects/",
    "title": "Game Theory: Contention and Cross-Effects",
    "body": "2020/03/22 - We have so far discussed decision problems that a rational individual could face. But as we move more closer to the reality, we more often face decision problems where our well-being does depend not only on our actions but also the actions of other decision makers. Just as you are trying to optimize your decisions, so are they. In order to maximise your well being, you not only think of your actions but also guess what other players are doing, in order to maximise your reward (refer to the examples in my first blog). Your contenders are also no less rational than you, they take decisions in a similar way. In essence, you and your peers are engaged in a strategic environment in which you have to think hard about what other players are doing in order to decide what is best for you ‚Äî knowing that the other players are going through the same difficulties. Now we see the theoretical framework, we laid for individual rational decision maker is falling apart as we introduce other decision makers into the decision problem. We now need a simple framework to capture these strategic situations. To start with, lets call these games. Lets introduce concept of static game: Static game is a game where each player chooses their action without the knowledge of the actions chosen by other players and after which these choices will result in a particular outcome, or probabilistic distribution over outcomes. Remember the assumptions about rational choice in Game Theory: Story of Thinking blog: we need to improve more such assumptions so that each player in strategic environment can behave rationally. These assumptions help us analyse games within a structured framework. 1) All the possible actions of all the players2) All the possible outcomes3) How each combination of actions of all players affe cts which outcome will materialise, and4) The preferences of each and every player over outcomes Vanilla Games with Pure Strategy: It‚Äôs time to develop a formal framework to understand the strategic environment. Just like normal decision problem which involves single player(refer to my blog: Game Theory: Story of Thinking), we can introduce a decision problem where players ( More than one) have to choose actions from action space and the combinations of those such choices results in outcomes. Each player in the decision problem have preferences for these outcome. Let‚Äôs start with decision problem with deterministic actions and deterministic outcomes. We rule out stochasticity in outcomes for now to simply lay the theoretical framework and introduce the notion of stochasticity in outcomes or probabilistic outcomes in coming blogs in this series. As discussed above vanilla game or normal-form game consists of 1. A set of players 2. set of actions for each players and 3. set of payoff functions for each player Payoff functions in normal-form game: which gives the payoff value for combination of actions chosen by each player in the normal game. This is defined for each player. Now lets‚Äô understand the concept of strategy. Strategy is just a plan of actions. To simplify, we will interchangeably use strategy and actions. Pure strategy is simply deterministic plan of actions, there is no concept of randomness involved. We will introduce stochastic strategy or mixed strategy in next blogs in this series. Let‚Äôs discuss simple example to make things more clearer for the readers. Prisoners Dilemma: A well known and simple example in game theory, we encounter this problem repeatedly in coming blogs.  Two members of a criminal gang are arrested and imprisoned. Each prisoner is in solitary confinement with no means of communicating with the other. The prosecutors lack sufficient evidence to convict the pair on the principal charge, but they have enough to convict both on a lesser charge. Simultaneously, the prosecutors offer each prisoner a bargain. Each prisoner is given the opportunity either to betray the other by testifying that the other committed the crime, or to cooperate with the other by remaining silent. The possible outcomes are:1. If A and B each ‚Äúbetray‚Äù(BE) the other, each of them serves two years in prison2. If A betrays B but B ‚Äúremains silent‚Äù(RS), A will be set free and B will serve three years in prison (and vice versa)3. If A and B both remain silent, both of them will serve only one year in prison (on the lesser charge). Players: N= {A,B}Strategic sets: S= {BE, RS}Payoffs: vA(sA,sB) be the payoff of player A and vB(sA,sB) be the payoff of player B vA(BE,BE) = vB(BE,BE) = -2vA(RS,RS) = vB(RS,RS) = -1vA(BE,RS) = vB(RS,BE) = 0vA(RS,BE) = vB(BE,RS) = -3 Its more convenient to represent these numbers in matrix representations.  Rows: Represent players A strategiesColumns: Represents player B strategiesMatrix Entries: payoff of A/B To get use to this matrix representation, lets look at an another famous example in game theory rock-paper-scissors. : Recall that rock (R) beats scissors (S), scissors beats paper (P), and paper beats rock. Let the winner‚Äôs payoff be 1 and the loser‚Äôs be ‚àí1, and in case of tie (choose the same action) be 0. This is a game with two Players: N = {1, 2}Strategic sets: S= {R, P, S}payoff matrix: I urge readers to closely analyse the payoff of each player for different combinations of action taken by each player and check if the numbers match the payoffs already described verbally above. Strategy Profile: It is basically set of actions taken by player, there are 9 possible strategy profiles. For example {R,R} is one strategy profile, which implies Player 1 and 2 both decides to choose rock. v·µ¢(s) is payoff of a player i from a profile of strategies s = (s‚ÇÅ, s‚ÇÇ, . . . , s·µ¢‚Çã‚ÇÅ , si, s·µ¢‚Çä‚ÇÅ, . . . , sn). We define strategy profile s‚Çã·µ¢ ‚àà S‚Çã·µ¢ as a particular possible profile of strategies for all players who are not i. Solution Concept: Let‚Äôs introduce the idea of solution concept in this section. So far we stressed on representation of payoff for different combinations of unique player‚Äôs decisions in the strategic environment. These representations are useless until we apply some model to predict the decision of a given player considering the anticipated decisions taken by other rational players. We describe this model as solution concept. For example, solution concept can be ‚Äú players always choose the action that they think the opponent can choose‚Äù or ‚Äú player act in accordance with pareto optimal outcomes‚Äù. Pareto optimality is a situation that cannot be modified so as to make any one individual better off without making at least one individual worse off. This solution concept is finest if it is applied to wide variety of games, not just to a small and select family of games. This solution concept usage should ideally result in unique action. We doesn‚Äôt want solution concept to result in ‚Äú take any action‚Äù. In the next section, we will define some very important solution concepts in game theory. Let‚Äôs use our prisoners dilemma example to illustrate concepts before formally defining them.  if player choose to remain silent, the possible outcomes are -1 and -3 depends on whether player‚Äôs opponent choose to remain silent and betray respectively. if player choose to betray then the possible outcomes are 0 and -2 depends on whether player‚Äôs opponent choose to remain silent and betray respectively. Here we can easily deduce that opting to remain silent {RS} is worse than betraying {BE} for each player regardless of what the player‚Äôs opponent does. We say that such a strategy of betraying {BE} is dominated. Definition: Let s·µ¢‚àà S·µ¢ and s‚Äù·µ¢‚àà S·µ¢ be possible strategies for player i. We say that s‚Äù·µ¢ is strictly dominated by s·µ¢, if for any possible combination of the other players‚Äô strategies, s‚Çã·µ¢‚àà S‚Çã·µ¢, player i‚Äôs payoff from s‚Äù·µ¢ is strictly less than that from s·µ¢ . That is,v·µ¢(s·µ¢, s‚Çã·µ¢) &gt; vi(s‚Äù·µ¢, s‚Çã·µ¢) for all s‚Çã·µ¢‚àà S‚Çã·µ¢. We will write s·µ¢ &gt;·µ¢ s‚Äù·µ¢ to denote that s‚Äù·µ¢ is strictly dominated by s‚Çã·µ¢ We can propose a new solution concept using the definition above Strict dominance concept : ‚Äústrictly dominant strategy is a strategy that is always the best thing you can do, regardless of what your opponents choose‚Äù It is not difficult to use this Strict dominance concept. It basically requires that we identify a strict dominant strategy for each player and then use this profile of strategies to predict or prescribe behaviour. In prisoners dilemma problem, each player have a strictly dominated strategy of {BE} Betraying, so the you would predict the players to choose betraying. But, this solution concept only applies to a section of problems. We can easily endorse this statement by applying the Strict dominance concept to advertising game. Two competing brands can choose one of three marketing campaigns ‚Äî low (L), medium (M), and high (H) ‚Äî with payoffs given by the following matrix: It is easy to observe that there is no strictly dominant strategy for both players. ( if player 2 playsM then player 1 should also play M, while if player 2 plays H then player 1 should also play H). In absence of strictly dominant strategy, we need to conclude that the strict-dominance solution concept might not apply for all kinds of games. Note: To those games, where strict dominance solution concept applies. The solution it predicts or prescribes is unique i. e there can be only one strictly dominant strategy, if exists. In fact, this important intended feature is what lured us to explore this solution concept in good detail. Common Knowledge of Rationality: This is an important assumption which states that the structure of the game and the rationality of the players are common knowledge among the players. For example if we consider to use strict-dominance solution concept, all the players are aware each player will never play a strictly dominated strategy, they can ignore those strictly dominated strategies that their opponents will never play, and their opponents can do the same thing. Rational player will never play a dominated strategy. We can eliminate those strategies which player will not choose for sure. We can iteratively eliminate the original game to a restricted game. In fact we may indeed find additional strategies that are dominated in the restricted game that were not dominated in the original game. let‚Äôs illustrate these concepts in an example. Consider the following two-player finite game: If player 1 chooses U, the strategy with highest payoff for player 2 is L. if player 1 chooses M, the strategy with highest payoff for player 2 is R. By analysing this way, it can be deduced that there is no strictly dominant strategy for both players. If you closely observe, there exists one strictly dominated strategy for player 2. Strategy C is strictly dominated by R. This results in reduced game.  In this new matrix representation , both M and D are strictly dominated by U for player 1. This led to next level of elimination, where M,D actions are eliminated.  It is quite straight forward from here, Player 1 chooses U and player 2 chooses L as v(L) = 3 &gt; v(R)=2. Lets try to dig little deeper into the example we discussed:  If a strategy s·µ¢ is not strictly dominated for player i then it must be that there are combinations of strategies of player i‚Äôs opponents for which the strategy s·µ¢ is player i‚Äôs best choice. This is a central concept in the game theory. The player has to choose a best strategy as a response to the strategies of his opponents. The player chooses an action considering the belief about his opponent as his/her best response. Definition: The strategy s·µ¢ ‚àà S·µ¢ is player i‚Äôs best response to his opponents‚Äô strategies s‚Çã·µ¢ ‚àà S‚Çã·µ¢ if v·µ¢(s·µ¢, s‚Çã·µ¢) ‚â• v·µ¢(s‚Äù·µ¢, s‚Çã·µ¢). ‚àÄs‚Äù·µ¢‚àà S·µ¢  If s·µ¢ is a strictly dominated strategy for player i, then it cannot be a best response to any s‚Çã·µ¢ ‚àà S‚Çã·µ¢. I urge readers try to prove this proposition. Belief and Best Response: Suppose that s·µ¢ is a best response for player i to his opponents playing s‚Äô‚Çã·µ¢. Player i will play s·µ¢ only when he believes that his opponents will play s‚Äô‚Çã·µ¢. The concept of belief in central to the analysis of strategic behaviour. If a strict dominant strategy exist for player i, then regardless of his belief system player i always choses the strict dominant strategy. The player‚Äôs strictly dominant strategy is his best response independent of his opponents‚Äô play.  If player 1 believes that player 2 is chooses strategy R then both U and D are best responses. So a player may have more than one best response given his belief on opponent‚Äôs choice. Now, we have learned a bunch on decision making in strategic environments. In next blogs in this series we will delve deep into the idea mixed strategies and more solution concepts. Thanks :) "
    }, {
    "id": 28,
    "url": "http://localhost:4000/kowshikchills.github.io/Evaluation-metrics-in-classification-algorithms/",
    "title": "Evaluation metrics in classification algorithms",
    "body": "2020/03/22 - This blog is completely dedicated to the crucial metrics used in classification problems. You might have come across problem statements where we have to use metrics other than the well known ‚Äòaccuracy‚Äô score. Let us try to understand confusion matrix, accuracy, recall, precision, F1 Score, ROC- AUC curve and their usage. fig. 1 Metrics in a nut shell Accuracy score is widely used for evaluating model which do not have any issue with type I and II errors or if it is a balanced data set. But certain problems like cancer analysis or customer churn data which are imbalanced, the focus will mainly be on False Positives and False Negatives. In such situations, we need other metrics ‚Äî Recall, Precision and F1-Score. Recall is the measure of actual true values captured by the model where as Precision is the measure of relevant true values predicted by the model. F1-Score is the harmonic mean of recall and precision. All of them ranges from 0 ‚Äî 1 and any score close to 1 is considered good. This might be confusing a bit. Lets understand in detail with an example. Here is a problem statement where we have to predict if a tumor is benign or malignant based on few features. I have used logistic regression for modelling in the previous blog and the results are as following: Analysing the confusion matrix of train data, by comparing to fig. 1 above. TN = 240, TP = 130, FP = 19, FN = 9. Accuracy =(240+130)/(240+130+19+9)= 0. 93 i. e. , 93% of the predictions are correctly classified Recall = 130/(130+9) =0. 93 i. e. , Model correctly identifies 93% of all malignant tumors Precision = 130/(130+19) = 0. 87 i. e. , the model is correct 87% of the time in classifying malign(out of 149 predicted true values, 130 are correct) F1- Score = 20. 870. 93/(0. 87+0. 93) = 0. 89 If we would have had precision and F1-Scores less than 0. 5 then we should try out models other than logistic regression for classification. As we have pretty good scores of above 0. 85, we can treat this model is fairly good but we should always try to increase F1-Score to avoid wrong predictions of Malign and Benign. Trade off of Recall and Precision. : In logistic regression, while classifying into Malign or Benign, there is a threshold value of the probability which is 0. 5 by default. If the probability function results in p‚â•0. 5 then it is Malign else Benign. But if the threshold moves below or above 0. 5 then all the metrics will change. This results in a trade off between Recall and Precision. In the tumor problem, we do not want to have False Negatives(type II errors) i. e. , predicting a tumor is Benign though in reality it is Malignant. So we choose a model that can perform this task of reducing FNs that in turn increases the score of Recall. But due to this threshold changing, FP‚Äôs will increase, leading to low Precision. This is called Recall Precision Tradeoff. ROC ‚Äî AUC:: Receiver operating characteristics: An evaluation metric where we can visualize the performance of the model is called ROC curve. This is plotted on True Positive rate against False Positive rates for different threshold values (probabilities as explained above). This optimal threshold helps in achieving Precision Recall balance. Typical ROC curve looks like (source) The black line shows the rates for a random classifier. Red and blue curves are for different models. We can have only one curve for one model. At different thresholds, the function for ROC plots this graph. At a threshold of 1, there are no positives and negatives yet so the graph starts from 0 and as threshold increases, the curve moves towards right upwards as more TP and FP‚Äôs come into picture. We can quantify the performance of the model using this curve by finding the area under the curve ‚Äî AUC (using differentials but the package takes care of it all). AUC values ranges from 0‚Äì1, any score near 1 evaluates as a good model. Hope this helps in understanding the important metrics used for classification. Happy learning! :): "
    }, {
    "id": 29,
    "url": "http://localhost:4000/kowshikchills.github.io/Logistic-Regression/",
    "title": "Logistic Regression",
    "body": "2020/03/11 - In any data set, we can have numerical or/and categorical features. We need to be careful while dealing with the response/target variable. The modelling algorithms should be picked considering if the target variable is numerical or categorical. A basic algorithm for numerical variable is Linear Regression and for categorical variable we have Logistic Regression. We might come across many data sets where we have to predict, for example, if a patient has a disease or not. Here we are classifying into two parts ‚Äî has disease or not. This is called classification problem. If you might be wondering why we can not use linear regression in classification, it is not appropriate in this scenario. For example, we have a data set of some features where we have the classes for response variable as ‚ÄòClassA‚Äô, ‚ÄòClassB‚Äô and ‚ÄòClassC‚Äô. As computer does not under stand text we have to convert them into numbers and we assign Y = {1, 2, 3} for {‚ÄòClassA‚Äô, ‚ÄòClassB‚Äô, ‚ÄòClassC‚Äô}. When we model it using Linear Regression, to minimize the error it uses Least Squares Method. In this process, the ordering of Y will have an impact on predictions. For instance, as 3&gt;1, ClassC is prioritized compared to ClassA. Also, the difference between ClassA, ClassB(2‚Äì1=1) and ClassB, ClassC(3‚Äì2=1) is same to its eye. But it is absolutely different in reality. There is no dependency of each class and they cannot be compared with each other. Therefore, Linear Regression is not effective for categorical target variables. Like in Linear Regression, we have to predict Y using X and a function involving coefficients. In Logistic Regression, it calculates the probability of occurrence of the event in response variable(Y). Based on a threshold value we can classify it. But how do we find the probability of an event? We must model p(X) using a function such that it always produces output between 0 and 1 for any value of X.  When modified this equation a little, we get By adjusting coefficients Œ≤0 and Œ≤1, we get desired p(X) and thus classifying it correctly. But how do we estimate Œ≤0 and Œ≤1? Like we have Least Squares in Linear Regression, Maximum Likelihood function is used in Logistic Regression. In this function, we try to find Œ≤0 and Œ≤1 such that the predicted probability p(xi) of class for each observation as closely as possible to the actual class. In other words, we try to find Œ≤0 and Œ≤1 such that plugging these estimates into the model for p(X) shown above, yields a number close to one for one class, and a number close to zero for the other class. This intuition can be formalized using a mathematical equation of likelihood function: Estimates Œ≤0 and Œ≤1 are picked in a such a way that this function is maximized. Detailed mathematical equations of this function is beyond the scope. Multiple Logistic Regression:: In many data sets we not only have one predictor/independent as shown for equations above. There will be many others and the probability function also depends on all of them. So the modified equation will be: Using Likelihood function we can find estimations for Œ≤0, Œ≤1, . . . , Œ≤p. Python Tutorial for Logistic Regression:: Let us apply logistic regression on ‚ÄòBreast Cancer Classification‚Äô data set. We have to classify if it is ‚ÄòMalign‚Äô or ‚ÄòBenign‚Äô. Let us import all the dependencies and data import numpy as npimport pandas as pdimport seaborn as snsimport matplotlib. pyplot as pltfrom sklearn. model_selection import train_test_splitfrom sklearn. linear_model import LogisticRegressionfrom sklearn. metrics import accuracy_scoredata = pd. read_csv('data. csv')data. head() This data set contains 569 Rows 33 Columnsdata. columns Column names#Null value check in datadata. isna(). sum() Unnamed: 32 has 569 nulls i. e. , it is an empty column and should be removed and rest all are goodLet us separate the y and x in the data and remove some unwanted columns like id y = data. diagnosis #target variablelist = [‚ÄòUnnamed: 32‚Äô,‚Äôid‚Äô,‚Äôdiagnosis‚Äô]x = data. drop(list,axis = 1 ) #drop few columns x. shape #results in 569 rows and 30 columns #Lets analyse the target variableax = sns. countplot(y,label=‚ÄùCount‚Äù) B, M = y. value_counts()print(‚ÄòCount of Benign: ‚Äò,B)print(‚ÄòCount of Malignant : ‚Äò,M) Let us observe the correlation matrix to identify co related features #From this matrix we can drop the columns with 1 as correlation scoredrop_list = [‚Äòperimeter_mean‚Äô,‚Äôradius_mean‚Äô,‚Äôcompactness_mean‚Äô,‚Äôconcave points_mean‚Äô,‚Äôradius_se‚Äô,‚Äôperimeter_se‚Äô,‚Äôradius_worst‚Äô,‚Äôperimeter_worst‚Äô,‚Äôcompactness_worst‚Äô,‚Äôconcave points_worst‚Äô,‚Äôcompactness_se‚Äô,‚Äôconcave points_se‚Äô,‚Äôtexture_worst‚Äô,‚Äôarea_worst‚Äô]x_1 = x. drop(drop_list,axis = 1 ) # do not modify x, we will use it later Let us again check if there are any fields that still have correlation score of 1f,ax = plt. subplots(figsize=(14, 14))sns. heatmap(x_1. corr(), annot=True, linewidths=. 5, fmt= ‚Äò. 1f‚Äô,ax=ax) There are no columns with high co relation score The columns looks good to go ahead with modelling. First split the data for train and test with some percentage. Here I have chosen 70% train and 30% test. def split_data(X,Y,size):  x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=size, random_state=42)  return x_train, x_test, y_train, y_testx_train, x_test, y_train, y_test = split_data(x_1,y,0. 3)Implementation of logistic regression: def model_data(model,X,Y,x_test): model. fit(X, Y) x_pred = model. predict(X)   #predictions on train data y_pred = model. predict(x_test) #predictions on test data return x_pred,y_predlogreg = LogisticRegression()model_data(logreg,x_train,y_train,x_test)Lets check the accuracy of this data and confusion matrix def model_metrics(X,Y):  confusion_matrix = metrics. confusion_matrix(Y,X)  print(‚ÄòAccuracy: {:. 2f}‚Äô. format(accuracy_score(Y,X)))  print(‚ÄòConfusion Matrix: \n‚Äô,confusion_matrix)print(‚ÄòPerformance of logistic regression classifier on train set:‚Äô)model_metrics(y_train,x_pred)print(‚Äò\n‚Äô)print(‚Äú ‚Äî ‚Äî ‚Äî ‚Äî ‚Äî ‚Äî ‚Äî ‚Äî ‚Äî ‚Äî ‚Äî ‚Äî ‚Äî ‚Äî ‚Äî ‚Äú)print(‚ÄòPerformance of logistic regression classifier on test set:‚Äô)model_metrics(y_test, y_pred) We see that the model has good accuracy on both train and test data. A confusion matrix shows correctness of number of observations predicted VS actual.  In train data set we see that 240 and 130 are correctly predicted as Benign and Malign respectively.  19 are predicted as Malign when its Benign actually and 9 are predicted as Benign when it is Malign in real.  Similarly in test data set, 104 and 61 are correct predictions.  Predicted 2 observations as Malign when Actually it is Benign and predicted 4 observations as Benign when actually its Malign. One question that might get popped ‚Äî is accuracy score well enough to understand model performance? In our predictions given the use case in real world, it is okay if we predict Benign as Malign but if a Malign observation is predicted as Benign, that‚Äôs where the problem with accuracy score comes in as it fails to capture such critical situations. There are several other Metrics used in classification to deal with this. Check them out in next blog. References: An Introduction to Statistical Learning: With Applications in R ##Thank You! "
    }, {
    "id": 30,
    "url": "http://localhost:4000/kowshikchills.github.io/Lasso-and-Ridge-Regularization/",
    "title": "Lasso and Ridge Regularization",
    "body": "2020/03/05 - 1. 1 Introduction: In machine learning when we use supervised learning algorithms on a data set, there will be situations where the model performs really well on train data and when tested on new data it might not perform well and also has high error. This is due to multiple reasons like collinearity, bias-variance decomposition and over modeling on train data. Dealing with collinearity is discussed in my previous blog :) 1. 2 Bias Variance Trade off: Bias and Variance are the measures which helps us understand how deviation of the function is varied. Bias is the measure of deviation or error from actual value of the function. Variance measures deviation in response variable function if we estimated it with a different training sample of data set.  From the definitions it can be inferred that while modelling we must keep bias as low as possible that implies accuracy is high Also by changing samples in training data set, one should not get highly varied results of the output. Therefore low variance is preferred for a good performing model But here comes a catch,If we try to reduce the bias, then the model would fit exactly well on that specific sample of training data and it cannot find the underlying patterns in the data set that it has never seen. So it is very likely that the model will have deviated output when another sample is used for training. This then results in high variance.  Similarly when we want to have less deviation or low variance when different samples are used then, the model will not fit exactly on the data points and results in high bias 1. 3 Overfitting: The situation where we had low bias and high variance is called overfitting as the model fits absolutely well with high accuracy on available data and when it sees a new data it fails to predict, leading to high test error. This generally happens with data that has many features and the model considers the contribution of the estimated coefficients of all of them and tries to over estimate the actual value. But in reality it might be the case that only few features of the data set are really important and impact the predictions. So if the less impactful features are more in number they tend to add value to the function in training data and when new data comes up that has nothing to do with these features then the predictions goes wrong. 1. 4 Regularization: So it is highly important to restrict the features while modelling to minimize the risk of overfitting and this process is called regularization. In regression we know that the features are estimated using coefficients and these estimates are the real game changes in modelling. If there is a possibility to ‚Äòrestrict‚Äô or ‚Äòshrink‚Äô or ‚Äòregularize‚Äô the estimates towards zero, then the effect of the non- impactful features is reduced and it saves the model from high variance with a stable fit. In terms of a typical linear regression model using ordinary least squares, this is done by modifying our typical loss function (Residual Sum of Squares, RSS) by adding a penalty for higher magnitude coefficient values. There are few things to keep in mind while using regularization. There needs to be a constant lookup on bias Vs. variance trade off while using the shrinkage parameter. The more we shrink the coefficients the more we reduce the variance which might pitch in high bias. Noting all the trade offs we now proceed to learn the regularization techniques 2. 1 Ridge and Lasso Regularization: Ridge Regularization:: Recall from the previous blog where we discussed about RSS and how it helps in estimating the coefficients by reducing RSS.  Ridge regression is quite similar to RSS except that there is also a shirnkage parametre ‚ÄòŒª‚Äô that minimizes their value. ‚ÄòŒª‚Äô is also called as ‚Äòtuning parametre‚Äô and it is determined separately using cross-validation technique Suppose the coefficients Œ≤1, . . . , Œ≤p are having some values and out of them few must have values already close to zero which as discussed above where features that do no have much impact on the response variable. When we add the shrinkage parametre these values which are already having small value will tend to zero in the equation shown above. So, the second term after RSS is called shrinkage penalty or l2 norm. If Œª=0 then the equation is as normal as RSS, but if Œª ‚Üí ‚àû, the impact of shrinkage penalty increases and the ridge regression estimate coefficients will approach to zero. Ridge regression‚Äôs advantage over least squares is rooted in the bias-variance trade-off. As Œª increases, the flexibility of the ridge regression fit decreases, leading to decreased variance but increased bias. Ridge regression does have one disadvantage. Ridge regression will include all p predictors in the final model. The shrinkage penalty will shrink all of the coefficients towards zero, but it will not set any of them exactly to zero (unless Œª = ‚àû). So we need to resort to step wise selection models again to pick up the important features. Lasso Regularization:: To overcome the problem that ridge has, Lasso(Least Absolute Shrinkage and Selection Operator) is an alternative that can pick relevant features that will be useful for modelling. Lasso also has the shrinkage parametre but the difference that has with Ridge is that there is no squared term of the estimated coefficient but only an absolute value.  Like in Ridge regression, lasso also shrinks the estimated coefficients to zero but the penalty effect will forcefully make the coefficients equal to zero if the tuning parameter is large enough. Hence, much like best subset selection, the lasso performs feature selection. As a result, models generated from the lasso are generally much easier to interpret. The term after RSS is called the shrinkage penalty or l1 norm 3. Python Tutorial on Lasso &amp; Ridge Regularization: We can measure the accuracy or how good the model is fit with the measure Mean Squared Error(MSE) which calculates the mean of squared terms of difference between actual and predicted values Now lets get some hands on with house prices data set. We need to predict the prices of houses given some features. Lets import the dependencies and data that we need to use import pandas as pdimport numpy as npfrom matplotlib import pyplot as plt import seaborn as snsfrom sklearn. preprocessing import LabelEncoder, StandardScalerfrom sklearn. model_selection import train_test_splitfrom sklearn. linear_model import LinearRegressionfrom sklearn import metricsfrom sklearn import ensemblefrom sklearn. linear_model import Lasso,Ridge#load train datadf_data=pd. read_csv( data_price. csv )df_data. head() #to know each and every column execute the followingprint(df_data. columns)print(df_data. shape) As we have a basic idea on the data lets see how to deal with Null values in it total = df_data. isnull(). sum(). sort_values(ascending=False)percent = (df_data. isnull(). sum()/df_data. isnull(). count()). sort_values(ascending=False)missing_data = pd. concat([total, percent], axis=1, keys=[‚ÄòTotal‚Äô, ‚ÄòPercent‚Äô])missing_data. head(20) We can drop the columns having more than 15% of null values. So the columns till ‚ÄòLotFrontage‚Äô can be removed. If we check the columns like ‚ÄòGarageXXXX‚Äô they seem to be related with Garage area and we can remove them as well are they are collinear. Similar case with ‚ÄòBsmtXXX‚Äô and ‚ÄòMasVnrXXXX‚Äô. For ‚ÄòElectrical‚Äô there is only one datapoint that is null. So we can remove that specific row from the data. df_data= df_data. drop(missing_data[missing_data[‚ÄòTotal‚Äô]&gt;1]. index. values,1)df_data= df_data. drop(df_data. loc[df_data[‚ÄòElectrical‚Äô]. isnull()]. index) Lets check the correlation matrix to find any unforeseen relations corr_mat=df_data. corr()fi,ax=plt. subplots(figsize=(20,20))sns. heatmap(corr_mat,square=True) Though column ‚ÄòID‚Äô seems to have no correlation to all the columns but there is no use in modelling as its just row number. So we can remove it too. del df_data[‚ÄòId‚Äô]There are many columns that are categorical. We need to one hot encode them. le=LabelEncoder()cat_mask= df_data. dtypes==‚Äôobject‚Äôcat_cols= df_data. columns[cat_mask]. tolist()cat_cols #Lets convert the columns to one hot encodingdf_data[cat_cols]=df_data[cat_cols]. apply(lambda x: le. fit_transform(x. astype(str)))df_data_c = df_data. copy()#get_dummies is used for one hot encodingdf_data_c = pd. get_dummies(df_data_c,columns=cat_cols)Now the data is ready for modelling. Before that we need to split the data into train and test. x_train, x_test, y_train, y_test = train_test_split(df_data_c. drop(‚ÄòSalePrice‚Äô,axis=1),df_data_c[‚ÄòSalePrice‚Äô], test_size =0. 25,random_state=120)y_train= y_train. values. reshape(-1,1)y_test= y_test. values. reshape(-1,1)Normalize the values in train and test using Standard Scaler function sc_X = StandardScaler()sc_y = StandardScaler()x_train = sc_X. fit_transform(x_train)x_test = sc_X. fit_transform(x_test)y_train = sc_X. fit_transform(y_train)y_test = sc_y. fit_transform(y_test)Let‚Äôs now fit a linear regression model on the data lm = LinearRegression()lm. fit(x_train,y_train)#predictions on train datax_pred = lm. predict(x_train)x_pred = x_pred. reshape(-1,1)#Prediction of validation datay_predictions = lm. predict(x_test)y_predictions= predictions. reshape(-1,1)def scores_(y,x):  print('MAE:', metrics. mean_absolute_error(y, x))  print('MSE:', metrics. mean_squared_error(y, x))  print('RMSE:', np. sqrt(metrics. mean_squared_error(y, x)))  print('R2 Score:' ,metrics. r2_score(y,x))print('InSample_accuracy')scores_(y_train, x_pred)print('---------------------------')print('OutSample_accuracy')scores_(y_test,y_pred) The model performed really well on training data with a good 0. 92 r2 score and &lt;1 RMSE score but with test data the performance is no where near good. This is a clear overfitting model. Reason might me because of numerous features. To tackle this we can perform Ridge and Lasso regularization. Lasso or l1 regularization:: For a given range of alpha lets try to find out the RMSE scores of training(In sample) and test(Out sample) data sets. def regularization(model,alpha_range):  rmse_score_insample=[]  rmse_score_outsample=[]  r2_score_insample=[]  r2_score_outsample=[]  for i in alpha_range:    regularization = model(alpha=i,normalize=True)    regularization. fit(x_train,y_train)    y_pred_train = regularization. predict(x_train)    y_pred_train = y_pred_train. reshape(-1,1)    y_pred_test=regularization. predict(x_test)    y_pred_test = y_pred_test. reshape(-1,1)    rmse_score_insample. append(np. sqrt(metrics. mean_squared_error(y_train,y_pred_train )))    rmse_score_outsample. append(np. sqrt(metrics. mean_squared_error(y_test, y_pred_test)))    r2_score_insample. append(metrics. r2_score(y_train, y_pred_train))    r2_score_outsample. append(metrics. r2_score(y_test, y_pred_test))  df=pd. DataFrame()  df['alpha']=alpha_range  df['rmse_score_insample'] = rmse_score_insample  df['rmse_score_outsample']= rmse_score_outsample   df['r2_score_insample'] = r2_score_insample  df['r2_score_outsample'] = r2_score_outsample  return df. plot(x = 'alpha', y = ['rmse_score_insample', 'rmse_score_outsample'])alpha_range_lasso = np. arange(0. 001,0. 03,0. 001)print(regularization(Lasso,alpha_range_lasso)) We can see that there is no huge difference in in sample and out sample RMSE scores so Lasso has resolved overfitting. One observation here is that after alpha= 0. 017 there is no difference in RMSE scores of In sample and Out sample. Let us also check for Ridge. Ridge or l2 regularization:: alpha_range_ridge = np. arange(0. 001,1,0. 1)print(regularization(Ridge,alpha_range_ridge))#writing functions helps reduce redundant lines of code as seen #above we can just input the parametre Ridge or Lasso We see in the graph that around alpha=0. 1 there is no much difference in the RMSE scores and clearly there is no sign of over fitting as there is very less difference of insample and outsample RMSE scores as compared to huge difference in Linear Regression. By comparing Lasso and Ridge RMSE or R2 and we can pick the model that has good score as desired for the problem statement. 3. Final Code: import pandas as pdimport numpy as npfrom matplotlib import pyplot as plt import seaborn as snsfrom sklearn. preprocessing import LabelEncoder, StandardScalerfrom sklearn. model_selection import train_test_splitfrom sklearn. linear_model import LinearRegressionfrom sklearn import metricsfrom sklearn import ensemblefrom sklearn. linear_model import Lasso,Ridge#load train datadf_data=pd. read_csv( data_price. csv )df_data. head()#to know each and every column execute the followingprint(df_data. columns)print(df_data. shape)total = df_data. isnull(). sum(). sort_values(ascending=False)percent = (df_data. isnull(). sum()/df_data. isnull(). count()). sort_values(ascending=False)missing_data = pd. concat([total, percent], axis=1, keys=[‚ÄòTotal‚Äô, ‚ÄòPercent‚Äô])missing_data. head(20)df_data= df_data. drop(missing_data[missing_data[‚ÄòTotal‚Äô]&gt;1]. index. values,1)df_data= df_data. drop(df_data. loc[df_data[‚ÄòElectrical‚Äô]. isnull()]. index)corr_mat=df_data. corr()fi,ax=plt. subplots(figsize=(20,20))sns. heatmap(corr_mat,square=True)del df_data[‚ÄòId‚Äô]le=LabelEncoder()cat_mask= df_data. dtypes==‚Äôobject‚Äôcat_cols= df_data. columns[cat_mask]. tolist()cat_cols#Lets convert the columns to one ht encodingdf_data[cat_cols]=df_data[cat_cols]. apply(lambda x: le. fit_transform(x. astype(str)))df_data_c = df_data. copy()#get_dummies is used for one hot encodingdf_data_c = pd. get_dummies(df_data_c,columns=cat_cols)x_train, x_test, y_train, y_test = train_test_split(df_data_c. drop(‚ÄòSalePrice‚Äô,axis=1),df_data_c[‚ÄòSalePrice‚Äô], test_size =0. 25,random_state=120)y_train= y_train. values. reshape(-1,1)y_test= y_test. values. reshape(-1,1)sc_X = StandardScaler()sc_y = StandardScaler()x_train = sc_X. fit_transform(x_train)x_test = sc_X. fit_transform(x_test)y_train = sc_X. fit_transform(y_train)y_test = sc_y. fit_transform(y_test)#Linear Regressionlm = LinearRegression()lm. fit(x_train,y_train)#predictions on train datax_pred = lm. predict(x_train)x_pred = x_pred. reshape(-1,1)#Prediction of test datay_pred = lm. predict(x_test)y_pred= y_pred. reshape(-1,1)def scores_(y,x):  print('MAE:', metrics. mean_absolute_error(y, x))  print('MSE:', metrics. mean_squared_error(y, x))  print('RMSE:', np. sqrt(metrics. mean_squared_error(y, x)))  print('R2 Score:' ,metrics. r2_score(y,x))print('InSample_accuracy')scores_(y_train, x_pred)print('---------------------------')print('OutSample_accuracy')scores_(y_test,y_pred)def regularization_model(model,alpha_range): rmse_score_insample=[] rmse_score_outsample=[] r2_score_insample=[] r2_score_outsample=[] for i in alpha_range: regularization = model(alpha=i,normalize=True) regularization. fit(x_train,y_train) y_pred_train = regularization. predict(x_train) y_pred_train = y_pred_train. reshape(-1,1) y_pred_test=regularization. predict(x_test) y_pred_test = y_pred_test. reshape(-1,1) rmse_score_insample. append(np. sqrt(metrics. mean_squared_error(y_train,y_pred_train ))) rmse_score_outsample. append(np. sqrt(metrics. mean_squared_error(y_test, y_pred_test))) r2_score_insample. append(metrics. r2_score(y_train, y_pred_train)) r2_score_outsample. append(metrics. r2_score(y_test, y_pred_test))df=pd. DataFrame() df[‚Äòalpha‚Äô]=alpha_range df[‚Äòrmse_score_insample‚Äô] = rmse_score_insample df[‚Äòrmse_score_outsample‚Äô]= rmse_score_outsample df[‚Äòr2_score_insample‚Äô] = r2_score_insample df[‚Äòr2_score_outsample‚Äô] = r2_score_outsample return df. plot(x = ‚Äòalpha‚Äô, y = [‚Äòrmse_score_insample‚Äô,‚Äôrmse_score_outsample‚Äô])alpha_range_lasso = np. arange(0. 001,0. 03,0. 001)print(regularization_model(Lasso,alpha_range_lasso))alpha_range_ridge = np. arange(0. 001,1,0. 1)print(regularization_model(Ridge,alpha_range))References: An Introduction to Statistical Learning: With Applications in R Thank You: "
    }, {
    "id": 31,
    "url": "http://localhost:4000/kowshikchills.github.io/Game-Theory-The-prelude/",
    "title": "Game Theory: The prelude",
    "body": "2020/03/05 - Intention to write this series of blogs to introduce game theory to readers at an introductory level without requiring any prior knowledge in advanced mathematics. Also readers find applications of game theory in their own field by understanding its applications in other field like diplomacy, economics, trading and conflict handling. This series of blogs explain common applications with assumption that reader has no prior knowledge of these fileds. Decision problems confront us every day, We play games of strategy all the time, with parents, friends and even contenders. Some games involve trivial decisions and other are serious, some have insignificant consequences and some have serious repercussions. We have instinctive expertise over these decision makings, but the need for formalising these decision making process in contention and creating a language for this strategic thinking is served by game theorists. Every field in involves decision making, strategic thinking at some level, let it be war, diplomacy, finance, economics, trading and even rent split among room mates. Game theory provides a general concepts and techniques of analysis to help decision making. Let‚Äôs start with formalising some terms we commonly use, but have subtly different meaning in the game theoretic approach. Game theory is the analysis, or science, if you like, of such interactive decision making. Provide some general principles for thinking about strategic interactions. Strategic thinking is about interacting with similar player thinking similarly in similar situation. As we must take into account what the other player is thinking, opponent is also taking into account what we are thinking. Now it might seems to the reader that psychological aspects are involved in this game theatric treatment, now we are going to introduce term which is more than adequately used in psychology: Rational Behaviour. You are said to be behaving rationally, if you are choosing your actions in a way to do the best according to you own criteria, given your objectives or preferences and of any limitations or constraints on your actions. To combine all the terms we just learned:: Game theory provide some general principles for thinking about strategic interactions in order to behave rationallyLet‚Äôs discuss few examples, these cases helps motivate the development of many conceptual or theoretical framework of game theory and also offers a concrete and memorable vehicle for the underlying concepts. we can call these examples as strategic games, because of the involvement of strategic thinking amongst player. lets just try to draw a distinction between strategic games and Just games. Strategic game involves interactions between mutually aware players and decisions for action of each person depends on the actions taken by the contender and cross-effects derived from these actions taken by the contender and himself. It is this mutual awareness of the cross-effects of actions and the actions taken as a result of this awareness that constitute the most interesting aspects of strategy. we see lot of these trivial games actually translate into interesting applications in weightier matters like war truce, diplomatics standoff‚Äôs, economic behaviours etc. There are different types of games which involves non-indentical underlying principles to help player think rationally, we will discuss more about this classification of games in finer detail in the coming blogs Example 1: Guess 2/3 of the Average: Game where several people guess what 2/3 of the average of their guesses will be, and where the numbers are restricted to the real numbers between 0 and 100, inclusive. The winner is the one closest to the 2/3 average.  Alain Ledoux is the founding father of the guess 2/3 of the average-game. In 1981, Ledoux used this game as a tie breaker in his French magazine Jeux et Strat√©gie. He asked about 4,000 readers, who reached the same number of points in previous puzzles. Example 2: Keynesian beauty contest: Game in which entrants are asked to choose the six most attractive faces from a hundred photographs. Those who picked the most popular faces are then eligible for a prize.  Concept developed by John Maynard Keynes and introduced in Chapter 12 of his work, The General Theory of Employment, Interest and Money (1936), to explain price fluctuations in equity markets. These are two popular examples which are used by academia to explain the concepts of game theory. Readers are encouraged to find out the answer for these two cases described above. Please note that it‚Äôs not only about what you think the answer is, its also about what other contenders think the answer is.  Consider example 2: ‚ÄúIt is not a case of choosing those faces that, to the best of one‚Äôs judgment, are really the prettiest, nor even those that average opinion genuinely thinks the prettiest. We have reached the third degree where we devote our intelligences to anticipating what average opinion expects the average opinion to be. And there are some, I believe, who practice the fourth, fifth and higher degrees. ‚Äù (Keynes, General Theory of Employment, Interest and Money, 1936) In the next blog, we state some basic concepts and terminology ‚Äîprovide a set of tools that will lend structure to the way in which we think about decision problems. While laying this theoretical framework, it is inevitable for us to lay down Assumptions about the behaviour of decision makers or players. "
    }, {
    "id": 32,
    "url": "http://localhost:4000/kowshikchills.github.io/Game-Theory-Story-of-Thinking/",
    "title": "Game Theory: Story of Thinking",
    "body": "2020/03/05 - In this blog, we will discuss about thinking, which is inevitable process before any decision making. We will lay theoretical framework for this thinking process. All decision making problems involves player, alternatives to choose, consequences of the outcome and preferences of those consequences. These consequences can be borne by the player himself or other players. ( Note that the consequences of decisions of other players can influence your payoff as well). Actions: Alternatives from which player can chooseOutcomes: consequences from player‚Äôs actionsPreferences: how the player ranks the set of possible outcomes The above described features quantify a decision problem, these decision problems can be as trivial as choosing attire in morning and as weightier as drawing peaceful frontiers in the conflict land. Let‚Äôs use a simple example to elucidate the theory we are going to explore in coming sections Consider a case, where you are asked to choose your desert and you are given choice of milkshake and ice cream. We can define your set of actions as A = {a, b}, where a denotes the choice of milkshake and b denotes the choice of ice cream. we will denote the set of outcomes by X = {x, y}, where x denotes drinking milk shake and y denotes eating ice cream. Preference Relations: Now lets include preferences, we will now introduce term Preference Relations. For example you prefer drinking milkshake to eating ice cream. Then we will write x &gt;‚àº y, which should be read as ‚Äúx is at least as good as y. ‚Äù From now, This is how we express players preferences. Lets also include 2 other important relations:  Strict Preference Relation: x &gt; y, for ‚Äúx is strictly better than y,‚Äù Indifference Relation: x ‚àº y, for ‚Äúx and y are equally good. ‚ÄùLooks like we defined decision problem, its features and also discussed preference in detail. But, everything looks trivial when a simple example is taken. Imagine having continuous action space, where you are asked to choose a rational number between a given range or imagine having a probabilistic outcomes, where the outcomes to your actions are not certain but follows a distribution. Assumptions: Before laying down theorems, we will make two important assumptions about the player‚Äôs ability to think through the this decision problem.    The Completeness Axiom: Any two outcomes x, y ‚àà X can be ranked by the preference relation, so that either x &gt;‚àº y or y &gt;‚àº x. This way we are enforcing the player to take preferences. Given two outcomes, player should prefer one over other.     The Transitivity Axiom: The preference relation &gt;‚àº is transitive: for any three outcomes x, y, z ‚àà X, if x &gt; ‚àº y and y &gt;‚àº z then x &gt;‚àº z.  If you observe closely, with these two assumptions, we are enforcing player to definitely prefer one outcome given all possible outcomes. This way we can ensure that player behaves consistently. Payoff Functions: In this section, we will try to quantify this preference. let‚Äôs take a example where you can take walk, bus and cab to your school and you will have to pay fine for your late arrival. Walking costs you 0$ but you will end up paying 10$ fine, bus costs you 2$ but you will have to pay 4$, car costs you 15$ and you reach just in time. Actions A: {Walk, Bus, Cab}Outcomes X : {-10$, -6$, -15$} if you prefer the outcome which costs you the least, then -6$ &gt; -10$ &gt;-15$. Hence you should choose Bus alternative in possible actions A. In this way, we can define the profit function. Every action a ‚àà A yields a profit œÄ(a). Then we can just look at the profit from each action and choose an action that maximises profits. In line with above example, let define Payoff Function u :X‚ÜíR represents the preference relation &gt;‚àº if for any pair x, y ‚àà X, u(x) ‚â• u(y) if and only if x &gt;‚àº y. We can define payoff functions to make preferences closer to the realistic situations. Rational being always chooses actions that maximise his well-being as defined by his payoff function over the resulting outcomes, for this to happen, the rational being is completely aware of all the features of the decision problem he is encountering. A player facing a decision problem with a payoff function v(. ) over actions is rational if he chooses an action a ‚àà A that maximises his payoff. That is, a‚àó ‚àà A is chosen if and only if v(a‚àó) ‚â• v(a) for all a ‚àà A. Let‚Äôs discuss an example with continuous action space. There is a one-kg cake, so your action set is A = [0, 1], where a ‚àà A is how much you cake you can eat. Your preferences are represented by the following payoff function over actions: you must maximise your payoff, in order to do that take a differential and equate it to zero. we obtain2 ‚àí 8a = 0 and a = 0. 25implies that in-order to maximise your payoff, you must eat 250 grams considering how much cake to eat as a decision problem. Stochastic Outcomes: Not always the outcomes of the actions taken by the player are certain, these outcomes can be random ( stochastic). In order to fit this stochasticity into the theoretical framework, we must introduce the concept of stochastic outcomes and probabilities so that player can compare uncertain consequences in a meaningful way. There is an uncertain element attached to the action taken by the player. In order to capture the uncertainty in a precise way, we will use the well understood notion of randomness, or risk, as described by a random variable. Using random variables is a standard mathematical way of consistently describe situations where randomness is involved. We can utilise a decision tree to describe the player‚Äôs decision problem that includes uncertainty. Take a decision problem, where player‚Äôs action space is A: {g,s} and player gets 10 units payoff with probability 0. 75 and 0 units with probability 0. 25, if player choose action g . The probabilities are 0. 5:0. 5 if player chooses action s to get payoff 10:0. We can describe this decision problem as follows: We will try to point out some obvious deductions from the above decision problem and then generalise it for a given decision problem.  The probability of each outcome cannot be a negative number The sum of all probabilities over all outcomes must add up to 1It is also important to note that the probability is conditional on the action taken by the player. Hence, given an action a ‚àà A, the conditional probability that x‚±º ‚àà X occurs is given by p(x‚±º|a), where p(x‚±º|a) ‚â• 0, and ‚ÖÄp(xk|a) = 1 for all a ‚àà A. Definition: A decision problem with outcomes X = {x‚ÇÅ, x‚ÇÇ, . . . , xn} is defined as a probability distribution p = (p(x‚ÇÅ), p(x‚ÇÇ), . . . , p(xn)), where p(x‚±º) ‚â• 0 is the probability that x‚±º occurs and ‚ÖÄp(x·µ¢ ) = 1. Note that our trivial decision problem of certain consequences to a action can be considered as a decision problem in which the probability over outcomes after any choice is equal to 1 for some outcome and 0 for all other outcomes. We call such a lottery a degenerate lottery. You can now see that decision problems with no randomness are just a very special case of those with randomness.  Question: Try solving this decision problem: What are the probabilities of outcome g and outcome s?we will discuss the solution in the next blog. ( Here you see more than one nodes for outcome g, do not get confused about it. The randomness unfolds over time, for a given action, the distribution of payoff can change with time. This decision tree depicts exactly the same thing. ) Continuous Outcomes: we will go a step further to describe random variables over continuous outcome sets, that is outcome as discussed might not be discrete. To start, consider the following example. Farmer had a orange farm, the yield depends on watering and temperature. The supply water can vary from 0 to 100 gallons continuously and the temperature can also vary continously. This implies that your final yield, given any amount of water, will also vary continuously. In above decision problems, we describe the uncertainty with a discrete probability, but in this continuous outcome set we will have describe uncertainty with a cumulative distribution function (CDF) Definition: A simple probability over an interval X is given by a cumulative distribution function F :X‚Üí[0, 1], where F(x) = Pr{x ‚â§x} is the probability that the outcome is less than or equal to x. Lets understand this little closely: it is somewhat meaningless to talk about the probability of growing a certain exact weight of oranges. However, it is meaningful to talk about the probability of being below a certain weight x, which is given by the CDF F(x*), or similarly the probability of being above a certain weight x. CDF F(10) gives the probability the orange yield is less than 10kg. Till now we have only discussed how to represent randomness , we can now move along to see how our decision-making player evaluates these random outcomes. Decision Making: If the outcomes are certain, then the decision making is simple. lets consider this example of decision problem.  The player has choices b,m,a,s. In this example the payoff of each outcome is also given at the end node. Since the player always prefers outcomes that has highest payoff. The preference order is s&gt;a&gt;m&gt;b, The player chooses action s. Pretty straight forward isn‚Äôt it. Now lets consider another decision problem which involves random outcomes. Unlike above case, it is not straight forward as it involves stochastic outcomes.  Intuitively, it seems that the two probabilities that follow g and s are easy to compare. Both have the same set of outcomes, a profit of 10 or a profit of 0. The choice g has a higher chance at getting the profit of 10, and hence we would expect rational player to choose g. Since the payoffs are 10,0 with different probability for both the outcomes, we can simply judge which outcome is more preferred based on just probability. Now lets consider a less obvious revision of above decision problem.  As you can easily observe, the revision is that if player chooses outcome g, the with probability 0. 25 player will borne a -1(negative payoff). Now the comparison is not as obvious as earlier. In order to tackle this decision problem we must calculate the payoff that can be expected from an out-come, lets introduce the concept of expected payoff Definition: Let u(x) be the player‚Äôs payoff function over outcomes in X = {x‚ÇÅ, x‚ÇÇ, . . . , xn }, and let P= (p‚ÇÅ, p‚ÇÇ, . . . , pn) be a lottery over X such that P‚±º = Pr{x = x‚±º}. Then we define the player‚Äôs expected payoff from the lottery P as E[u(x)|p]= ‚ÖÄp‚±º. u(x‚±º) = p‚ÇÅ. u(x‚ÇÅ) + p‚ÇÇ. u(x‚ÇÇ) + . . . Using above definition, if we try to solve the revised decision problem By choosing g, the expected payoff to the player isv(g) = E[u(x)|g]= 0. 75 (9) + 0. 25(‚àí1) = 6. 5In contrast, by choosing s his expected payoff isv(s) = E[u(x)|s]= 0. 5(10) + 0. 5(0) = 5. The expected payoff from s is still 5, while the expected payoff from g is 7. 5, so that g is his best choice. Let‚Äôs continue this decision evaluation case in continuous action space by using the introduced topic of cumulative distribution function in the next section. Imagine outcomes can be any one of a continuum of values distributed on some interval X. We will start with evaluation of decision problem involving continuous outcomes. We will try to estimate expected payoff. Using the concept of cumulative distribution functions which is again introduced in the last blog, we will define expected payoff in continuum case as follows: Definition Let u(x) be the player‚Äôs payoff function over outcomes in the interval X with a lottery given by the cumulative distribution F(x), with density f(x). Then we define the player‚Äôs expected payoff: Also we must keep in mind that the density function f(x) is just a derivative for CDF F(x).  After discussing the discrete and continuous action and outcome cases, it is bit settled that the rational player, who understands the stochastic consequences of each of his actions, will choose an action that offers him the highest expected payoff. Let‚Äôs illustrate an another example of maximizing expected payoff with a finite set of actions and outcomes, Imagine that you have been working for a company and you are taking a decision whether or not join MBA. MBA fees and coaching costs you 10L(opportunity costs included)if labour marker is strong and economy is bullish . your income value from having an MBA is 32L, while your income value from your current job is 12L. if labour marker is average and economy is monotonous . your income value from having an MBA is 16L, while your income value from your current job is 8L. if labour marker is weak and economy is bearish. your income value from having an MBA is 12L, while your income value from your current job is 4L. lets assume the labor market will be strong with probability 0. 25, average with probability 0. 5, and weak with probability 0. 25. The decision is :Should you pursue the MBA? Lets illustrate this decision problem in decision tree: Note that, if player chooses to pursue MBA: then we subtract the cost of the degree from the income benefit in each of the three states of nature. Let‚Äôs calculate the expected payoff from each action. By evaluating the expected payoff values we can tell which outcome is more preferred.  By looking at the expected payoff values, the rational player would choose to pursue MBA. Till now, a single player is involved in decision problem. In the next blogs in this series, we will discuss the multi player scenario. Thanks You :) "
    }, {
    "id": 33,
    "url": "http://localhost:4000/kowshikchills.github.io/Detailed-explanation-of-Linear-Regression/",
    "title": "Detailed explanation of Linear Regression",
    "body": "2020/03/05 - 1. 1 Introduction: In a data set we can characterize features or variables as either quantitative or qualitative (also known as categorical). Quantitative variables are nothing but numerical values like a person‚Äôs weight or temperature of a city and qualitative variables are values in one of ‚Äôn‚Äô different classes, or categories like gender (male or female), different blog categories(technical, cooking, fashion etc. ,). We tend to refer to problems with a quantitative response as regression problems. The response variable here is referred to as target or dependent variable and the other independent variables are predictors. Linear regression is used for finding linear relationship between target and one or more predictors. There are two types of linear regression- Simple and Multiple. In Simple linear regression we find the relationship between a dependent Y and independent variable X, the mathematical equation that approximates linear relationship between X and Y is Œ≤0 and Œ≤1 are two unknown constants that represent the intercept and slope terms in the linear model. Together, Œ≤0 and Œ≤1 are known as the model coefficients or parameters. Once we have used our training data to produce estimates ÀÜŒ≤0 and ÀÜŒ≤1 for the model coefficients, where ÀÜy indicates a prediction of Y on the basis of X = x.  represents the i th residual (error) which is the difference between the actual i th response value and the i th response value that is predicted by our linear model. We define the residual sum of squares (RSS) as which is equivalent to In regression, there is always a notion of a best-fit line ‚Äî the line which fits the given data in the best way. RSS here is called loss function or cost function and minimizing it would result in good fit or accuracy. This approach is called least squares method. Least squares method chooses ÀÜŒ≤0 and ÀÜŒ≤1 to minimize the RSS using some calculus. Then a new set of coefficients are generated and we need some metrics to validate the accuracy of these estimated coefficients. Here comes a set of metrics that help to perform the validating task easy: 1. 2 Validation of Estimated Coefficients:  Standard Errors associated with ÀÜŒ≤0 and ÀÜŒ≤1, where sigma is standard deviation, In general Var(Error) is not known and it is approximated from the data as Residual Standard Error  We now compute t-static that measures the number of standard deviations ÀÜŒ≤1 is away from 0  The probability of observing any value equal to |t| or larger, assuming Œ≤1 =0(which implies there is no relation ship between X and Y) is called p-value. A small p-value indicates it is an unlikely event that Œ≤1 = 0 and that Y is dependent on X or a relation exists between X and Y. Similarly a high p-values indicates no relation and X is insignificant in predicting Y. 1. 3 Assessing Model Using Metrics: These metrics are useful in estimating the accuracy of coefficients. So now we can model with updated coefficients or features and evaluate the accuracy of this model. The extent of fit of linear regression is generally assessed with two Metrics    RSE can be defined in different terminologies ‚Äî The RSE is an estimate of the standard deviation of error‚Äî The average value that the dependent variable deviated from the true-regression line or‚Äî Lack of fit of the model     R-Squared statistic‚Äî As RSE is measured in the units of Y we are never sure of what value is a good RSE. But R-squared is measured as proportion of variability in Y that can be explained using X and always will be range of 0 to 1 unlike RSE. ‚Äî Formula of R-squared is  ‚Äî Total Sum of Squares measures the total variance or the inherent variance present in the response variable Y before the regression was performed. ‚Äî A value near 0 implies the model is unable to explain variance and a value close to 1 says model is able to capture the variability. A good performing model would have the R2 score close to 1 1. 4 Assumptions in Linear Regression:    Linear relationship: linear regression needs the relationship between the independent and dependent variables to be linear. It is also important to check for outliers since linear regression is sensitive to outlier effects. The linearity assumption can best be tested with scatter plots.   Normal Distribution of error terms: If the error terms are non- normally distributed, confidence intervals may become too wide or narrow i. e. , unstable. This does not help in estimation of coefficients based on cost function minimization.    No auto-correlation: The presence of correlation in error terms drastically reduces model‚Äôs accuracy. This usually occurs in time series models where the next instant is dependent on previous instant. The estimated standard errors tend to underestimate the true standard error as the intervals become narrower. This further results in reducing p-value which results incorrect conclusion of an insignificant variable.     Heteroscedasticity: The presence of non-constant variance in the error terms results in heteroscedasticity. Generally, non-constant variance arises in presence of outliers or extreme leverage values causing the confidence interval for out of sample prediction to be unrealistically wide or narrow.   No or little multi collinearity: Two variables are collinear if both of them have a mutual dependency. Due to this,it becomes a tough task to figure out the true relationship of a predictors with response variable or find out which variable is actually contributing to predict the response variable. ‚Äî This causes the standard errors to increase. With large standard errors, the confidence interval becomes wider leading to less precise estimates of coefficients. 1. 5 Feature Engineering: As we talked about collinearity ,there are a few points to be marked. Collinearity of variables is found by plotting a co relation matrix and we eliminate one of the correlated variables that do not add any value to the model. After eliminating the them reconfigure the correlation matrix and continue eliminating till all the variables are independent of each other. Instead of inspecting the correlation matrix, a better way to assess multi- collinearity is to compute the variance inflation factor (VIF). The smallest possible value for VIF is 1, which indicates the complete absence of collinearity. VIF value that exceeds 5 or 10 indicates a problematic amount of collinearity. This is one of the feature engineering steps. Now train the model and check the metrics that define the accuracy and based on p-values we can eliminate the variables to reach an optimal score. This is performed directly in python packages. 1. 6 Stochastic gradient Descent (SGD): We can measure the accuracy or how good the model is fit with the measure Mean Squared Error(MSE) which calculates the mean of squared terms of difference between actual and predicted values To optimize the model we have to reduce the MSE, we define loss function (L) which is equal to MSE and by a set of iterative steps we subtract the negative derivate of loss and update it at each step so that MSE or L is reduced. To calculate the negative derivative we use Stochastic Gradient method which helps in finding the global minimum of a function(here Loss function). If we can imagine the function as shown in the figure, the red dot is global minima and if the function is able to reach the estimated coefficients there, then it will be minimized resulting in better accuracy. Shifting the model to go in the steepest downhill direction would be the equivalent of subtracting the negative derivative of the loss, times some constant. Thus, we can formalize gradient descent for this problem as an update rule it keeps on updating based on the gradient. Œ± is the learning rate, and it affects how quickly m changes(m here refers to Œ≤ in the convention used above) 2. Python Tutorial on Linear Regression: Let‚Äôs get into the practice session in python using beer consumption dataset that has temperatures of a particular day, rainfall measure,weekend or not and final response variable consumption of beer in liters. All the dependencies are resided in the top lines import numpy as np import pandas as pd import seaborn as snsimport matplotlib. pyplot as pltfrom statsmodels. stats. outliers_influence import variance_inflation_factorfrom sklearn. model_selection import train_test_splitfrom sklearn. linear_model import LinearRegressionfrom sklearn. metrics import mean_squared_error,r2_scorebeer_data=pd. read_csv(‚Äúbeer_consumption_data. csv‚Äù) #read csv data #into a dataframe using pd. read_csvbeer_data. head(10) #head() prints top 5 rows in the data set As we see that the column names are in different language we can rename them by using the following command beer_data. columns=[‚ÄúDate‚Äù,‚ÄùTemperature_Median‚Äù,‚ÄùTemperature_Min‚Äù,‚ÄùTemperature_Max‚Äù,‚ÄùRainfall‚Äù,‚ÄùWeekend‚Äù,‚ÄùConsumption_litres‚Äù]Also another observation is that temperature values and rainfall have comma instead of a dot to denote the number and they should also be converted to float or double as shown below beer_data[‚ÄòTemperature_Median‚Äô] = beer_data[‚ÄòTemperature_Median‚Äô]. str. replace(‚Äò,‚Äô, ‚Äò. ‚Äô). astype(‚Äòfloat‚Äô)beer_data[‚ÄòTemperature_Min‚Äô] = beer_data[‚ÄòTemperature_Min‚Äô]. str. replace(‚Äò,‚Äô, ‚Äò. ‚Äô). astype(‚Äòfloat‚Äô)beer_data[‚ÄòTemperature_Max‚Äô] = beer_data[‚ÄòTemperature_Max‚Äô]. str. replace(‚Äò,‚Äô, ‚Äò. ‚Äô). astype(‚Äòfloat‚Äô)beer_data[‚ÄòRainfall‚Äô] = beer_data[‚ÄòRainfall‚Äô]. str. replace(‚Äò,‚Äô, ‚Äò. ‚Äô). astype(‚Äòfloat‚Äô)beer_data. info() #info() outputs total number of rows,number of #columns and null values present in each of them. #drop Blank rows read from the input CSV and describe shows all #statistics beer_data = beer_data. dropna()beer_data. describe() Primary analysis on data is done and now we have to separate the predictor and response variables(here it is consumption_litres). As date is of no use and consumption_litres is a response variable, we separate them from other variables to perform the analysis and training. Then save them in different data frames. X = beer_data. drop(columns=[‚ÄòDate‚Äô, ‚ÄòConsumption_litres‚Äô])Y = beer_data[‚ÄòConsumption_litres‚Äô]Now that X and Y are obtained, we perform some tests like collinearity as discussed in section 1. 5. We can do it by checking correlation matrix and VIF.  Analyse with correlation matrix plot or heat map that gives the score of collinearity in range of 0 to 1.  1 being highly collinear and 0 being no mutual dependency.  We can plot this using heatmap in seaborn library in python which we imported with name sns in the first step.  The argument to this heatmap will be X. corr() which gives the pairwise correlation of all columns of X in the dataframe. plt. figure(figsize=(7,7))sns. heatmap(X. corr())plt. title(‚ÄúCorrelation Heatmap‚Äù)plt. show() In the heat map here we can see that the Temperature_Median, Temperature_Min, Temperature_Max are highly collinear as their score is close to 1. Let us eliminate the unwanted features using VIF in next steps vif = pd. DataFrame() #Let us show th VIF scores in a data framevif[‚ÄòFeatures‚Äô] = X. columnsvif[‚ÄòVIF Factor‚Äô] = [variance_inflation_factor(X. values, i) for i in range(X. shape[1])] #variance_inflation_factor calculates the scores #for each Featurevif As we can see here that Temperature features have really high score of VIF. We first eliminate the feature with high score and re run VIF function to see if there is any change in the scores and repeat this process until all of the variables have a score &lt;5 #If we write a function then we do not need to re run same set of lines all the time. After checking VIF scores we give the column name with high VIF score as an argument in this function and it is dropped form the dataframedef check_vif_drop_column(X,column_name): X = X. drop(columns=column_name) vif = pd. DataFrame() vif[‚ÄòFeatures‚Äô] = X. columns vif[‚ÄòVIF Factor‚Äô] = [variance_inflation_factor(X. values, i) for i in range(X. shape[1])] return vif,Xvif1,X = check_vif_drop_column(X,‚ÄôTemperature_Median‚Äô)vif1 The VIF scores of Temperature_Min and Temperature_Max have decreased and lets eliminate Temperature_Min in this step as VIF is higher than Temperature_Max and recheck the scores vif2,X = check_vif_drop_column(X,‚ÄôTemperature_Min‚Äô)vif2 Now all the variables have VIF scores allowed range, we can move to model buildingWe split the data into X_train,X_test,Y_train,Y_test.  X_train,Y_train are used in training process and X_test,Y_test for testing the model.  train_test_split function is imported from sklearn. model_selection which does the splitting job This function has a parameter ‚Äòtest_size‚Äô that allows the user to set the proportion of data to be used for testing the model (here we use 0. 25)def split_train_data(X,Y): X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0. 25) return(X_train, X_test, Y_train, Y_test)X_train, X_test, Y_train, Y_test = split_train_data(X,Y)As we have defined the data sets used for training and testing, we now move to model building. In sklearn we have all kinds of models as functions which we import and fit the data to train. We have already imported the LinearRegression() from sklearn. linear_model. #Arguments will be the model used for training and train data. We can change this function according to the problem statement and requirement( remember to change it in argument too :P)def model_fit(model,X_train, Y_train): model = LinearRegression() model. fit(X_train, Y_train) return modellin_model = model_fit(LinearRegression,X_train, Y_train)The model has been trained and we need to predict with test data and validate it using different metrics  model_name. predict(X) is used to predict the response variable.  mean_squared_error and r2_score are calculated as discussed in section 1. 3 in sklearn. metrics We check the scores for both X_train and X_test which means how good the model has predicted for train dataset and test datadef scores_(model,X,Y): y_predict = model. predict(X) rmse = (np. sqrt(mean_squared_error(Y, y_predict))) r2 = r2_score(Y, y_predict) print(‚ÄòRMSE is {}‚Äô. format(rmse)) print(‚ÄòR2 score is {}‚Äô. format(r2))print( The model performance of training set )scores_(lin_model,X_train,Y_train)print( -------------------------------------- )print( The model performance of testing set )scores_(lin_model,X_test,Y_test)3. Final Code: import numpy as np import pandas as pd import seaborn as snsimport matplotlib. pyplot as pltfrom sklearn. model_selection import train_test_splitfrom sklearn. linear_model import LinearRegressionfrom sklearn. metrics import mean_squared_error,r2_scorebeer_data=pd. read_csv(‚Äúbeer_consumption_data. csv‚Äù)beer_data. columns=[‚ÄúDate‚Äù,‚ÄùTemperature_Median‚Äù,‚ÄùTemperature_Min‚Äù,‚ÄùTemperature_Max‚Äù,‚ÄùRainfall‚Äù,‚ÄùWeekend‚Äù,‚ÄùConsumption_litres‚Äù]beer_data[‚ÄòTemperature_Median‚Äô] = beer_data[‚ÄòTemperature_Median‚Äô]. str. replace(‚Äò,‚Äô, ‚Äò. ‚Äô). astype(‚Äòfloat‚Äô)beer_data[‚ÄòTemperature_Min‚Äô] = beer_data[‚ÄòTemperature_Min‚Äô]. str. replace(‚Äò,‚Äô, ‚Äò. ‚Äô). astype(‚Äòfloat‚Äô)beer_data[‚ÄòTemperature_Max‚Äô] = beer_data[‚ÄòTemperature_Max‚Äô]. str. replace(‚Äò,‚Äô, ‚Äò. ‚Äô). astype(‚Äòfloat‚Äô)beer_data[‚ÄòRainfall‚Äô] = beer_data[‚ÄòRainfall‚Äô]. str. replace(‚Äò,‚Äô, ‚Äò. ‚Äô). astype(‚Äòfloat‚Äô)beer_data = data. dropna()X = beer_data. drop(columns=[‚ÄòDate‚Äô, ‚ÄòConsumption_litres‚Äô])Y = beer_data[‚ÄòConsumption_litres‚Äô]plt. figure(figsize=(7,7))sns. heatmap(X. corr())plt. title(‚ÄúCorrelation Heatmap‚Äù)plt. show()vif = pd. DataFrame() #Let us show th VIF scores in a data framevif[‚ÄòFeatures‚Äô] = X. columnsvif[‚ÄòVIF Factor‚Äô] = [variance_inflation_factor(X. values, i) for i in range(X. shape[1])] print(vif)def check_vif_drop_column(X,column_name): X = X. drop(columns=column_name) vif = pd. DataFrame() vif[‚ÄòFeatures‚Äô] = X. columns vif[‚ÄòVIF Factor‚Äô] = [variance_inflation_factor(X. values, i) for i in range(X. shape[1])] return vif,Xvif1,X = check_vif_drop_column(X,‚ÄôTemperature_Median‚Äô)print(vif1)vif2,X = check_vif_drop_column(X,‚ÄôTemperature_Median‚Äô)print(vif2)#Modellingdef split_train_data(X,Y): X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0. 25) return(X_train, X_test, Y_train, Y_test)def model_fit(LinearRegression,X_train, Y_train): lin_model = LinearRegression() lin_model. fit(X_train, Y_train) return lin_modeldef scores_(lin_model,X,Y): y_predict = lin_model. predict(X) rmse = (np. sqrt(mean_squared_error(Y, y_predict))) r2 = r2_score(Y, y_predict) print(‚ÄòRMSE is {}‚Äô. format(rmse)) print(‚ÄòR2 score is {}‚Äô. format(r2)) X_train, X_test, Y_train, Y_test = split_train_data(X,Y)lin_model = model_fit(LinearRegression,X_train, Y_train)print(‚ÄúThe model performance of training set‚Äù)scores_(lin_model,X_train,Y_train)print(‚Äú ‚Äî ‚Äî ‚Äî ‚Äî ‚Äî ‚Äî ‚Äî ‚Äî ‚Äî ‚Äî ‚Äî ‚Äî ‚Äî ‚Äî ‚Äî ‚Äî ‚Äî ‚Äî ‚Äî ‚Äú)print(‚ÄúThe model performance of testing set‚Äù)scores_(lin_model,X_test,Y_test)This data set fits properly with linear regression, but we find data sets which fits with high accuracy on train data set but when predicted with test data the accuracy is really low. This scenario is called overfitting and we will deal with it in this next blog References: An Introduction to Statistical Learning: With Applications in R Thank You!: "
    }, {
    "id": 34,
    "url": "http://localhost:4000/kowshikchills.github.io/The-American-Revolution/",
    "title": "The American Revolution",
    "body": "2020/02/15 - First Revolution Against Imperial Enterprise: Introduction: Events like military coups, revolutions dominate the list of events that shaped the ideas, politics and even borders of nations we live in today. American revolution stands one of the earliest but not deadliest revolutions in this list of events. It is one of the most important episode in our modern age and it sparked the fire of revolution in many other countries like France, Spain and in countries in Latin America. Let‚Äôs understand the chain of events and nature of British rule which led to the outbreak of mother of all revolutions. Beginning: British emigration to America started in early 17th century and slowly managed to colonise the eastern lands of America by the mid of 18th century. The violent hostility towards the native Americans by British colonial masters caused the overwhelming decline of native population in America. British nationals dominate the colonialists in America but French, Spain and Portuguese were also involved in this imperial dominance. French colonialists occupied the north of America and present Canada. As the time progressed, one peculiar character which was observed amongst the British colonial population, was their lack of uncompromising allegiance towards mother land Britain. It must be noted that the American emigration from Britain didn‚Äôt happen for the sole reason of colonial leverage. Many migrated to America from Britain because of economic compulsions or to avoid punishments. What led to creation of typical American identity among these emigrants was British‚Äôs cold eye to its diaspora in American lands, who had to work really hard for generations to fight the native people, coercive mother nature and start agriculture activity. American colonies were under British rule and the nature of rule was less exploitive till America‚Äôs significant advance in industrial production, which was against the British‚Äôs economic policy of mercantilism. Thus Americans also accepted the British sovereignty till industrial revolution in late 18th century. There was also one more dimension to British‚Äôs nominal rule in America, British doesn‚Äôt want to impose rigid colonial rules in America as it can invoke French hostility from its colonies in Canada and further weaken already confrontational Anglo-French relations. Outbreak: By middle of 18th century the level of development in American colonies is no less than England. The plantation industry is flourishing, trade and commerce fueled by advanced iron and steel industry were competing with Britain and other powerful European nations. America truly embraced the Industrial revolution and not dependent on any European super powers. Politico-administrative and legal institutions were also established in America with British representation at top level .  All 13 colonies in America have their own legislatures. A different cultural thinking and institutions were developed, a typical progressive and liberal culture had evolved in America. This can be seen as a direct consequence of ideas of enlightenment. Thinkers and writers like Benjamin Franklin propounded these ideas amongst American populace. British nominal rule and flexible execution of navigation acts (which are enacted to control the trade) were no longer agreeable to the British Industrialists and seen as a compromise of British‚Äôs mercantile economic policy. Hence British‚Äôs nominal rule hitherto, now transformed into absolute rule. In 1750, the British government imposed a number of restrictions on American industries and trade to avoid direct competition with British goods. Till 1763, Americans though obnoxious to this rigid governance, they choose not to confront British as they are apprehensive of French invasion from north. This rigid rule from 1750 by British invited an obvious war from the French Canada. This war is called war of seven years(1756‚Äì1763). British emerged victorious, Paris treaty was signed and sovereignty over Canada is transformed to British. The change in character of British rule after the French defeat lit the fuel of American discontent and led to the outbreak of American Revolution. The cost of this war is very high and British imposed the burden of war on American colonies. British enacted a number of new laws and slapped exorbitant taxes on American populace. Sugar act, Currency act, Stamp act were some of the laws which are of exploitative nature. These rigid and exploitive laws stirred the American‚Äôs and invited their hostility. Absence of fear of French invasion from the north also created an opportunity for the Americans to follow the path of mutiny. The voice against the rigid colonial and imperial enterprise was getting stronger. Revolution seems more imminent than ever before.  No Taxation without Representation ‚Äî James OtisThe enlightened Americans questioned the British taxations on American lands as Americans were not represented in British legislature. In this political turmoil, British parliament enacted ‚ÄúTea Policy‚Äù to help the financially troubled East India Company. This policy allows dumping of tea products into the America and this law is against the American interests. When first fleet of ships exporting tea reached Boston port, a group of Americans dumped the tea bags into the river. This incident is known as Boston Tea Party, this further intensified the American hostility towards imperial regime.  This mutual hostility continued till 1776, when large number British soldiers started landing on American soil. The alarmed Americans, proclaimed independence on 4th July, 1776 in the second American continental congress and made this immigrant war official. French immediately declared its support for Americans, French trained American soldiers and also spent a fortune in this war. Largely because of the French support, the American continental army led by general George Washington emerged victorious in the war. in 1783, peace treaty was signed in Paris to facilitate the transfer of sovereignty from British to America and America was declared independent. Aftermath: This revolution resulted in the independence of American colonies. All 13 colonies came together to form American Union or USA as an independent sovereign state. The first written constitution came into existence, the rights of citizens became more articulated than ever before when the American parliament adopted the bill of rights in 1789. A liberal and progressive political systems came into existence which is based on the philosophy of capitalism. The American revolution can be seen not just a war against Imperial enterprise, it can be seen as a war against mercantilism. This revolution popularised the ideas of liberty, equality and fraternity through out the world, this lit the fire of revolution in many other countries like France, Spain and in countries in Latin America. Thinkers, writers around the world started fearlessly propounding the need for democratic institutions and practices. Decolonisation can also be seen as one of the important consequence of American revolution. It is thus rightly called the *MOTHER OF ALL REVOLUTIONS *because it had triggered a number of revolutions including French revolution, Spanish Revolutions, revolutions in Latin America and many around the wold.  "
    }];

var idx = lunr(function () {
    this.ref('id')
    this.field('title')
    this.field('body')

    documents.forEach(function (doc) {
        this.add(doc)
    }, this)
});


    
function lunr_search(term) {
    $('#lunrsearchresults').show( 1000 );
    $( "body" ).addClass( "modal-open" );
    
    document.getElementById('lunrsearchresults').innerHTML = '<div id="resultsmodal" class="modal fade show d-block"  tabindex="-1" role="dialog" aria-labelledby="resultsmodal"> <div class="modal-dialog shadow-lg" role="document"> <div class="modal-content"> <div class="modal-header" id="modtit"> <button type="button" class="close" id="btnx" data-dismiss="modal" aria-label="Close"> &times; </button> </div> <div class="modal-body"> <ul class="mb-0"> </ul>    </div> <div class="modal-footer"><button id="btnx" type="button" class="btn btn-secondary btn-sm" data-dismiss="modal">Close</button></div></div> </div></div>';
    if(term) {
        document.getElementById('modtit').innerHTML = "<h5 class='modal-title'>Search results for '" + term + "'</h5>" + document.getElementById('modtit').innerHTML;
        //put results on the screen.
        var results = idx.search(term);
        if(results.length>0){
            //console.log(idx.search(term));
            //if results
            for (var i = 0; i < results.length; i++) {
                // more statements
                var ref = results[i]['ref'];
                var url = documents[ref]['url'];
                var title = documents[ref]['title'];
                var body = documents[ref]['body'].substring(0,160)+'...';
                document.querySelectorAll('#lunrsearchresults ul')[0].innerHTML = document.querySelectorAll('#lunrsearchresults ul')[0].innerHTML + "<li class='lunrsearchresult'><a href='" + url + "'><span class='title'>" + title + "</span><br /><small><span class='body'>"+ body +"</span><br /><span class='url'>"+ url +"</span></small></a></li>";
            }
        } else {
            document.querySelectorAll('#lunrsearchresults ul')[0].innerHTML = "<li class='lunrsearchresult'>Sorry, no results found. Close & try a different search!</li>";
        }
    }
    return false;
}
</script>
<style>
    .lunrsearchresult .title {color: #d9230f;}
    .lunrsearchresult .url {color: silver;}
    .lunrsearchresult a {display: block; color: #777;}
    .lunrsearchresult a:hover, .lunrsearchresult a:focus {text-decoration: none;}
    .lunrsearchresult a:hover .title {text-decoration: underline;}
</style>




<form class="bd-search hidden-sm-down" onSubmit="return lunr_search(document.getElementById('lunrsearch').value);">
<input type="text" class="form-control text-small"  id="lunrsearch" name="q" value="" placeholder="Type keyword and enter..."> 
</form>
            </ul>
        </div>
    </div>
    </nav>

    <!-- Search Results -->
    <div id="lunrsearchresults">
        <ul class="mb-0"></ul>
    </div>

    <!-- Content -->
    <main role="main" class="site-content">
        
<div class="container">
<div class="jumbotron jumbotron-fluid mb-3 pl-0 pt-0 pb-0 bg-white position-relative">
		<div class="h-100 tofront">
			<div class="row  justify-content-center ">
				<div class=" col-md-8  pr-0 pr-md-4 pt-4 pb-4 align-self-center">
					<p class="text-uppercase font-weight-bold">
                        <span class="catlist">
						
                        </span>
					</p>
					<h1 class="display-4 mb-4 article-headline">About</h1>
					<div class="d-flex align-items-center">
                        
						<small class="ml-3">  <span><a target="_blank" href="" class="btn btn-outline-success btn-sm btn-round ml-1">Follow</a></span>
                            <span class="text-muted d-block mt-1"> ¬∑ <span class="reading-time">
  
  
    2 mins read
  
</span>
    </span>
						</small>
					</div>
				</div>
                
			</div>
		</div>
	</div>
</div>





<div class="container-lg pt-4 pb-4">
	<div class="row justify-content-center">
        
        
        <!-- Share -->
		<div class="col-lg-2 pr-4 mb-4 col-md-12">
			<div class="sticky-top sticky-top-offset text-center">
				<div class="text-muted">
					Share this
				</div>
				<div class="share d-inline-block">
					<!-- AddToAny BEGIN -->
					<div class="a2a_kit a2a_kit_size_32 a2a_default_style">
						<a class="a2a_dd" href="https://www.addtoany.com/share"></a>
						<a class="a2a_button_facebook"></a>
						<a class="a2a_button_twitter"></a>
					</div>
					<script async src="https://static.addtoany.com/menu/page.js"></script>
					<!-- AddToAny END -->
				</div>
			</div>
		</div>
        
        
		<div class="col-md-12 col-lg-8">
            
            <!-- Article -->
			<article class="article-post">                
			<p><img src="/kowshikchills.github.io/assets/images/abt.jpg" alt="centre" /></p>

<h3 id="our-mission">Our Mission</h3>
<p>Being avid readers of blogs and books, we wanted to come up with blogs that gives readers a great perspective with real world applications and get the gist that sticks to mind. Our intention is to untangle intricate subjects to sow enduring knowledge in the readers mind. Not only technical we also bring in various topics like history, new innovations, finance etc., We really respect readers interests and whatever we write is solely our perspective. Let‚Äôs all try to improve the knowledge and make the world a better place to learn. Thank You!</p>

<h3 id="kowshik-chilamkurthy">Kowshik Chilamkurthy</h3>
<p><img src="/kowshikchills.github.io/assets/images/kowshik_abt.jpg" alt="centre" /></p>

<p>Hailed from IIT Madras. Data science has been my core interest since 5 years with 3 years of industry experience. I currently work in Algorithmic trading. In my experience I understood that with the soaring competition in data science day by day, one has to put an extensive effort to get accustomed with emerging technologies like Reinforcement learning, game theory etc., These interesting and new topics are mostly in the form of research papers or journals. I found there is a need to bring the subject and its applications in a more approchable way. My quench for knowledge includes learning history and other intriguing topics. This inquisitiveness has led me to comprise them all in a lucid way. Hope you learn a new thing from every blog here. Don‚Äôt forget to share your feedback üòä</p>

<h3 id="dharani-jonnagadda">Dharani Jonnagadda</h3>

<p><img src="/kowshikchills.github.io/assets/images/dharani_abt.jpg" alt="centre" /></p>

<p>Graduated from DAIICT, I have joined an analytics firm. I have 2+ experience in data analytics. I believe any data set has a lot of stories to unravel. In my experience when working with real word data sets, not all problems end with deploying a Machine learning model, a complete analysis is expected by any business. So I have sharpend my skills in ML, DL and Visualisation to facilitate the industry need. When I started learning ML and DL, I found existing blogs explain a topic on an over all level and I had to follow some or the other book to get to know the derivations and Math behind it later, which obviously consumed more time. I started to put my learnings in blogs explaining methodically with mathematical equations, derivations and little bit of code that comprehends a topic well. I also keep drawn to learn interesting things happening around the world and if I feel it‚Äôs something one needs to know, I keep adding them here. I hope every blog here gives you a good learning point.</p>
                
			</article>
			
			<!-- Tags -->
			<div class="mb-4">
				<span class="taglist">
				
				</span>
			</div>
 
            <!-- Mailchimp Subscribe Form -->
            
			<div class="border p-5 bg-lightblue">
				<div class="row justify-content-between">
					<div class="col-md-6 mb-2 mb-md-0">
						<h5 class="font-weight-bold">Join Newsletter</h5>
						 Get the latest news right in your inbox. We never spam!
					</div>
					<div class="col-md-6">
						<div class="row">
                            <form action="https://wowthemes.us11.list-manage.com/subscribe/post?u=8aeb20a530e124561927d3bd8&amp;id=8c3d2d214b" method="post" name="mc-embedded-subscribe-form" class="wj-contact-form validate w-100" target="_blank" novalidate>
                            <div class="mc-field-group">
							
								<input type="email" placeholder="Enter e-mail address" name="EMAIL" class="required email form-control w-100" id="mce-EMAIL" autocomplete="on" required>
							
							
								<button type="submit" value="Subscribe" name="subscribe" class="heart btn btn-success btn-block w-100 mt-2">Subscribe</button>
							
                            </div>
                            </form>
						</div>
					</div>
				</div>
			</div>
            
            
            
             <!-- Author Box -->
                
            
            <!-- Comments -->
            
                <!--  Don't edit anything here. Set your disqus id in _config.yml -->

<div id="comments" class="mt-5">
    <div id="disqus_thread">
    </div>
    <script type="text/javascript">
        var disqus_shortname = ''; 
        var disqus_developer = 0;
        (function() {
            var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
            dsq.src = window.location.protocol + '//' + disqus_shortname + '.disqus.com/embed.js';
            (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
        })();
    </script>
    <noscript>
    Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a>
    </noscript>
</div>
            
            
		</div>
        
        
	</div>
</div>


<!-- Aletbar Prev/Next -->
<div class="alertbar">
    <div class="container">
        <div class="row prevnextlinks small font-weight-bold">
          
          
        </div>
    </div>
</div>

    </main>


    <!-- Scripts: popper, bootstrap, theme, lunr -->
    <script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js" integrity="sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut" crossorigin="anonymous"></script>

    <script src="https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js" integrity="sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k" crossorigin="anonymous"></script>

    <script src="/kowshikchills.github.io/assets/js/theme.js"></script>


    <!-- Footer -->
    <footer class="bg-white border-top p-3 text-muted small">
        <div class="container">
        <div class="row align-items-center justify-content-between">
            <div>
            </div>
            <div>
            </div>
        </div>
        </div>
    </footer>

    <!-- All this area goes before </body> closing tag --> 


</body>

</html>
